\section{Ensemble Methods}

\begin{frame}{Ensemble Methods}
	\begin{block}{Ensemble Method}
		An \textit{ensemble method} creates a composite model that consists of several models to form one model.
	\end{block}
	\begin{itemize}
		\item \textbf{Basic Idea:}
		      \begin{itemize}
			      \item Use multiple models to improve classification accuracy.
			      \item The final prediction is made by combining the predictions of all models.
		      \end{itemize}
		\item \textbf{Popular methods:}
		      \begin{itemize}
			      \item Bagging
			      \item Boosting
			      \item Random Forest
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Bagging}
	\vspace*{-1em}

	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Basic Idea:}
				      \begin{itemize}
					      \item Multiple models classify the same tuple.
					      \item The bagged classifier collects results.
					      \item The class with the \textbf{most votes} is returned as the final prediction.
				      \end{itemize}
			\end{itemize}
		\end{column}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\visible<9->{
				\item \textbf{Continuous-valued Attributes:}
				      \begin{itemize}
					      \item Multiple models predict the same tuple.
					      \item The bagged regressor collects results.
					      \item The final prediction is the \textbf{average} of all predictions.
				      \end{itemize}
				      }
			\end{itemize}
		\end{column}
	\end{columns}

	\vspace*{1em}

	\begin{center}
		\scalebox{0.75}{
			\visible<2->{
				\begin{tikzpicture}
					\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, dotted] at (0,0) (tuple) {Tuple $T$};

					\visible<3->{
						\draw[draw=none,fill=faugray!25] (2,2) rectangle (8.5,-2);
						\draw[draw=none,fill=faugray!50] (2,2) rectangle (8.5,2.5);

						\node at (5.25,2.25) {\color{faubluedark}\textbf{Bagged Classifier}};


						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,1.2) (classifier1) {Classifier 1};
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,0) (classifier2) {Classifier 2};
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,-1.2) (classifier3) {Classifier 3};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier1);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier2);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier3);
					}

					\visible<4->{
						\node[right=-1mm of classifier1.east, draw, dashed, fill=faugreen!15, thick] (classifier1decision) {\tiny Yes};
						\node[right=-1mm of classifier2.east, draw, dashed, fill=faured!15, thick] (classifier2decision) {\tiny No};
						\node[right=-1mm of classifier3.east, draw, dashed, fill=faugreen!15, thick] (classifier3decision) {\tiny Yes};
					}

					\visible<5->{
						\node[draw, thick, rectangle, minimum width=1cm,  fill=faugray!10, minimum height=2.5cm] at (7,0) (vote) {Vote};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier1decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier2decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier3decision) -- (vote);
					}

					\visible<6->{
						\node[below=3mm of vote.center, text width=0.5cm] (votecontent) {\tiny \centering 2$\times$Yes\\1$\times$No\\};
					}

					\visible<7->{
						\node[above right=3mm and -1mm of vote.east, draw, dashed, fill=faugreen!15, thick] (votedecision) {\tiny Yes};
					}

					\visible<8->{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, fill=faugreen!15, dashed] at (10.5,0) (class) {Yes};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (vote) -- (class);
					}
				\end{tikzpicture}
			}
		}
	\end{center}

\end{frame}


\begin{frame}{Boosting}
	\vspace*{-1em}


	\begin{itemize}
		\item \textbf{Basic Idea:}
		      \begin{itemize}
			      \item Multiple models classify the same tuple.
			      \item The final prediction is made by combining the predictions of all models.
		      \end{itemize}
	\end{itemize}



	\vspace*{1em}

	\begin{center}
		\scalebox{0.75}{
			\visible<2->{
				% TODO
			}
		}
	\end{center}

\end{frame}


\begin{frame}{Boosting}
	\begin{itemize}
		\item \textbf{Analogy:}
		      \begin{itemize}
			      \item Consult several doctors, based on a combination of weighted diagnoses -- weight assigned based on the previous diagnosis accuracy
		      \end{itemize}
		\item \textbf{How boosting works:}
		      \begin{itemize}
			      \item Weights are assigned to each training tuple.
			      \item A series of $k$ classifiers is iteratively learned.
			      \item After a classifier $M_i$ is learned, the weights are updated to allow the subsequent classifier, $M_{i+1}$ to pay more attention to the training tuples that were misclassified by $M_i$.
			      \item The final $M^*$ combines the votes of each individual classifier, where the weight of each classifier's vote is a function of its accuracy.
		      \end{itemize}
		\item \textbf{Classification:}
		      \begin{itemize}
			      \item Each classifier $M_i$ returns its class prediction.
			      \item The bagged classifier $M^*$ counts the votes and assigns the class with the most votes to $X$.
		      \end{itemize}
		\item \textbf{Boosting algorithm can be extended for numeric prediction.}
	\end{itemize}
\end{frame}


\begin{frame}{AdaBoost ("Adaptive Boosting" \footfullcite[Algorithm AdaBoost.M1 on p. 131]{freund1997}): Training}
	\vspace*{-1em}
	\begin{itemize}
		\item \textbf{Given a data set $D$ of $d$ class-labeled tuples: $(x_1 , y_1), \ldots, (x_d, y_d)$} with $y_d \in Y = \{1, \dots, c\}$.
		\item \textbf{Initialize empty lists to hold information per classifier:} $\textbf{w}, \boldsymbol{\beta}, \textbf{M} \leftarrow $ empty list.
		\item \textbf{Initialize weights for first classifier to hold same probability for each tuple:} $w_j^1 \leftarrow \frac{1}{d}$
		\item \textbf{Generate $K$ classifiers in $K$ iterations. At iteration $k$,}
		      \begin{enumerate}
			      \item Calculate ``normalized'' weights: $\textbf{p}^k = \frac{\textbf{w}^k}{\sum_{j=1}^d w_j^i}$
			      \item Sample dataset with replacement according to $\textbf{p}^k$ to form training set $D_k$.
			      \item Derive classification model $M_k$ from $D_k$.
			      \item Calculate error $\varepsilon_k$ by using $D_k$ as a test set as follows: $\varepsilon_k = \sum_{j=1}^{d} p^k_j \cdot \text{err}(M_k, x_j, y_j)$,\\
			            where the \textit{misclassification error} $\text{err}(M_k, x_j, y_j)$ returns 1 if $M_k(x_j) \neq y_j$, otherwise it returns $0$.
			      \item If $\text{error}(M_k)>0.5$: Abandon this classifier and go back to step 1.
			      \item Calculate $\beta_k = \frac{\varepsilon_k}{1 - \varepsilon_k}$.
			      \item Update weights for the next iteration: $w^{k+1}_j=w^k_j \beta^{1-err(M_k, x_j, y_j)}_k$. \textit{If a tuple is misclassified, its weight remains the same, otherwise it is decreased.} Misclassified tuple weights are increased relatively.
			      \item Add $\textbf{w}^{k+1}$, $M_k$, and $\beta_k$ to their respective lists.
		      \end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{AdaBoost ("Adaptive Boosting" \footfullcite[Algorithm AdaBoost.M1 on p. 131]{freund1997}): Prediction}
	\begin{itemize}
		\item \textbf{Initialize weight of each class to zero.}
		\item \textbf{For each classifier $i$ in $k$ classifiers:}
		      \begin{enumerate}
			      \item Calculate the weight of this classifier's vote: $w_i = \log(\frac{1}{\beta_i})$.
			      \item Get class prediction $c$ for (single) tuple $x$ from current weak classifier $M_i$: $c = M_i(x)$.
			      \item Add $w_i$ to weight for class $c$.
		      \end{enumerate}
		\item \textbf{Return predicted class with the largest weight.}
		\item Mathematically, this can be formulated as:
		      \begin{align*}
			      \textstyle M(x)= \arg \max_{y\in Y} \sum_{i=1}^k (\log \frac{1}{\beta_i})M_i(x)
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Random Forest\footfullcite{breiman2001}}
	\begin{itemize}
		\item Ensemble method consisting only of decision trees where each tree has been generated using random selection of attributes at each node.
		\item Classification: Each tree votes and the most popular class is returned.
		\item \textbf{Two methods to construct random forests:} (each builds $k$ trees)
		      \begin{enumerate}
			      \item \underline{Forest-RI} (random input selection):
			            \begin{itemize}
				            \item Random sampling with replacement to obtain training data from $D$.
				            \item Set $F$ as the number of attributes to determine split at each node. $F$ is smaller than the number of available attributes.
				            \item Construct decision tree $M_i$ by randomly select candidates at each node. Use CART to grow tree to maximum size without pruning.
			            \end{itemize}
			      \item \underline{Forest-RC}: Similar to Forest-RI but new attributes (features) are generated by linear combinations of existing attributes to reduce correlation between individual classifiers. At each node, attributes are randomly selected.

		      \end{enumerate}
		\item \textbf{Comparable in accuracy to AdaBoost, but more robust to errors and outliers.}
		\item \textbf{Insensitive to the number of attributes selected for consideration at each split, and faster than bagging or boosting.}
	\end{itemize}
\end{frame}

\begin{frame}{Classification of Class-imbalanced Data Sets}
	\vspace*{-0.5em}
	\begin{block}{Class-Imbalanced Data}
		\textit{Class-Imbalanced Data} refers to data where the main class of interest (positive labeled) is only represented by a small number of tuples. E.g., medical diagnosis and fraud detection.
	\end{block}
	\begin{itemize}
		\item Problem because traditional methods assume \textit{equality between
			      classes},\\ i. e. a balanced distribution of classes and equal error
		      costs.
		\item \textbf{Typical methods for imbalanced data in binary classification:}
		      \begin{enumerate}
			      \item \textbf{\color{airforceblue}Undersampling/Oversampling:} Changes distribution of tuples in training data.
			            \begin{itemize}
				            \item \textit{Undersampling:} Randomly eliminate tuples from negative class.
				            \item \textit{Oversampling:} Re-samples data from positive class.\\ For instance, method SMOTE generates synthetic data that is similar to existing data using nearest neighbor.
			            \end{itemize}
			      \item \textbf{\color{airforceblue}Threshold-moving:} Moves the decision threshold, $t$, so that the rare-class tuples are easier to classify, and hence, less chance of costly false-negative errors. Works when class returns a probability.
			      \item \textbf{\color{airforceblue}Ensemble techniques}.
		      \end{enumerate}
		      Threshold-moving and ensemble methods work well on extremely imbalanced data.
		\item \textbf{Still difficult on multi-class tasks.}
	\end{itemize}
\end{frame}
