\section{Ensemble Methods}

\begin{frame}{Ensemble Methods}
	\begin{block}{Ensemble Method}
		An \textit{ensemble method} creates a composite model that consists of several models to form one model.
	\end{block}
	\begin{itemize}
		\item \textbf{Basic Idea:}
		      \begin{itemize}
			      \item Use multiple models to improve classification accuracy.
			      \item The final prediction is made by combining the predictions of all models.
		      \end{itemize}
		\item \textbf{Popular methods:}
		      \begin{itemize}
			      \item Bagging
			      \item Boosting
			            \begin{itemize}
				            \item Popular algorithm: AdaBoost \\
				                  $\Rightarrow$ More on that in the appendix. \footnote{Not part of the exam.}
			            \end{itemize}
			      \item Random Forest
			            \begin{itemize}
				            \item Algorithm specialized on decision trees \\
				                  $\Rightarrow$ More on that in the appendix. \footnote{Not part of the exam.}
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Bagging}
	\vspace*{-1em}

	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Basic Idea:}
				      \begin{itemize}
					      \item Multiple models classify the same tuple.
					      \item The bagged classifier collects results.
					      \item The class with the \textbf{most votes} is returned as the final prediction.
				      \end{itemize}
			\end{itemize}
		\end{column}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\visible<9->{
				\item \textbf{Continuous-valued Attributes:}
				      \begin{itemize}
					      \item Multiple models predict the same tuple.
					      \item The bagged regressor collects results.
					      \item The final prediction is the \textbf{average} of all predictions.
				      \end{itemize}
				      }
			\end{itemize}
		\end{column}
	\end{columns}

	\vspace*{1em}

	\begin{center}
		\scalebox{0.75}{
			\visible<2->{
				\begin{tikzpicture}
					\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, dotted] at (0,0) (tuple) {Tuple $T$};

					\visible<3->{
						\draw[draw=none,fill=faugray!25] (2,2) rectangle (8.5,-2);
						\draw[draw=none,fill=faugray!50] (2,2) rectangle (8.5,2.5);

						\node at (5.25,2.25) {\color{faubluedark}\textbf{Bagged Classifier}};


						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,1.2) (classifier1) {Classifier 1};
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,0) (classifier2) {Classifier 2};
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,-1.2) (classifier3) {Classifier 3};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier1);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier2);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier3);
					}

					\visible<4->{
						\node[right=-1mm of classifier1.east, draw, dashed, fill=faugreen!15, thick] (classifier1decision) {\tiny Yes};
						\node[right=-1mm of classifier2.east, draw, dashed, fill=faured!15, thick] (classifier2decision) {\tiny No};
						\node[right=-1mm of classifier3.east, draw, dashed, fill=faugreen!15, thick] (classifier3decision) {\tiny Yes};
					}

					\visible<5->{
						\node[draw, thick, rectangle, minimum width=1cm,  fill=faugray!10, minimum height=2.5cm] at (7,0) (vote) {Vote};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier1decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier2decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier3decision) -- (vote);
					}

					\visible<6->{
						\node[below=3mm of vote.center, text width=0.5cm] (votecontent) {\tiny \centering 2$\times$Yes\\1$\times$No\\};
					}

					\visible<7->{
						\node[above right=3mm and -1mm of vote.east, draw, dashed, fill=faugreen!15, thick] (votedecision) {\tiny Yes};
					}

					\visible<8->{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, fill=faugreen!15, dashed] at (10.5,0) (class) {Yes};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (vote) -- (class);
					}
				\end{tikzpicture}
			}
		}
	\end{center}

\end{frame}


\begin{frame}{Boosting}
	\vspace*{-1em}

	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Training:}
				      \begin{itemize}
					      \item Classifiers are iteratively learned.
					      \item After a classifier is learned, the weights of the training tuples are updated. \\
					            $\Rightarrow$ The next classifier pays more attention to the misclassified tuples.
				      \end{itemize}
			\end{itemize}
		\end{column}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\visible<9->{
				\item \textbf{Predictions:}
				      \begin{itemize}
					      \item All models classify the same tuple.
					      \item The boosted classifier collects results.
					      \item The weight of each classifier's vote is a function of its accuracy.
				      \end{itemize}
				      }
			\end{itemize}
		\end{column}
	\end{columns}

	\vspace*{1em}

	\begin{center}
		\scalebox{0.75}{
			\visible<2->{
				\begin{tikzpicture}
					% Background
					\visible<8-9>{
						\draw[draw=none,fill=faugray!25] (2,2) rectangle (6,-2);
						\draw[draw=none,fill=faugray!50] (2,2) rectangle (6,2.5);

						\node at (4,2.25) {\color{faubluedark}\textbf{Boosted Classifier}};
					}
					\visible<10->{
						\draw[draw=none,fill=faugray!25] (2,2) rectangle (8.5,-2);
						\draw[draw=none,fill=faugray!50] (2,2) rectangle (8.5,2.5);

						\node at (5.25,2.25) {\color{faubluedark}\textbf{Boosted Classifier}};
					}

					% 1. Training
					\visible<-8>{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=3cm, dotted] at (0,1.2) (original-data) {Original Data};
					}
					\visible<3->{
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,1.2) (classifier1) {Classifier 1};
						\visible<-8>{
							\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (original-data) -- (classifier1);
						}
					}

					% 2. Training
					\visible<4-8>{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=3cm, dotted] at (0,0) (weighted-data-1) {Weighted Data};
						\draw[->, thick, dashed, shorten <=1mm, shorten >=1mm, >=stealth] (classifier1) -- (weighted-data-1);
					}
					\visible<5->{
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,0) (classifier2) {Classifier 2};
						\visible<-8>{
							\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (weighted-data-1) -- (classifier2);
						}
					}

					% 3. Training
					\visible<6-8>{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=3cm, dotted] at (0,-1.2) (weighted-data-2) {Weighted Data};
						\draw[->, thick, dashed, shorten <=1mm, shorten >=1mm, >=stealth] (classifier2) -- (weighted-data-2);
					}
					\visible<7->{
						\node[draw, thick, rectangle, minimum height=0.8cm, fill=faugray!10, minimum width=2cm] at (4,-1.2) (classifier3) {Classifier 3};
						\visible<-8>{
							\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (weighted-data-2) -- (classifier3);
						}
					}


					% Predictions
					\visible<10->{
						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, dotted] at (0,0) (tuple) {Tuple $T$};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier1);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier2);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (tuple) -- (classifier3);

						\node[right=-1mm of classifier1.east, draw, dashed, fill=faugreen!15, thick] (classifier1decision) {\tiny Yes};
						\node[right=-1mm of classifier2.east, draw, dashed, fill=faured!15, thick] (classifier2decision) {\tiny No};
						\node[right=-1mm of classifier3.east, draw, dashed, fill=faugreen!15, thick] (classifier3decision) {\tiny Yes};

						\node[draw, thick, rectangle, minimum width=1cm,  fill=faugray!10, minimum height=2.5cm] at (7,0) (vote) {Vote};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier1decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier2decision) -- (vote);
						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (classifier3decision) -- (vote);

						\node[below=3mm of vote.center, text width=0.9cm] (votecontent) {\tiny \centering 0.15$\times$Yes\\0.75$\times$No\\0.10$\times$Yes\\};

						\node[above right=3mm and -1mm of vote.east, draw, dashed, fill=faured!15, thick] (votedecision) {\tiny No};

						\node[draw, thick, rectangle, minimum height=0.8cm, minimum width=2cm, fill=faured!15, dashed] at (10.5,0) (class) {No};

						\draw[->, thick, shorten <=1mm, shorten >=1mm, >=stealth] (vote) -- (class);
					}
				\end{tikzpicture}
			}
		}
	\end{center}

\end{frame}
