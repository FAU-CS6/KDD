\section{Ensemble Methods: Increasing the Accuracy}

\begin{frame}{Ensemble Methods: Increasing the Accuracy}
  \begin{columns}
    \begin{column}{0.4\textwidth}
      \centering
      \begin{itemize}
      \item \textbf{{\color{airforceblue}Ensemble} methods:}
        \begin{itemize}
        \item Use a combination of models to increase accuracy.
        \item Combine a series of $k$ learned models, $M_1$, $M_2$, $\ldots$, $M_k$, with the aim of creating an improved model $M^*$.
        \end{itemize}
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \begin{itemize}
      \item \textbf{Popular ensemble methods:}
        \begin{itemize}
        \item Bagging:
          \begin{itemize}
          \item Averaging the prediction over a collection of classifiers.
          \end{itemize}
        \item Boosting:
          \begin{itemize}
          \item Weighted vote with a collection of classifiers.
          \end{itemize}
        \item Ensemble:
          \begin{itemize}
          \item Combining a set of heterogeneous classifiers.
          \end{itemize}
        \end{itemize}
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Bagging: Boostrap Aggregation}
  \begin{itemize}
  \item \textbf{Analogy:}
    \begin{itemize}
    \item Diagnosis based on multiple doctors' majority vote.
    \end{itemize}
  \item \textbf{Training:}
    \begin{itemize}
    \item Given a set $D$ of d tuples, at each iteration $i$, a training set $D_i$ of $d$ tuples is sampled with replacement from $D$ (i.e., bootstrap).
    \item A classifier model $M_i$ is learned for each training set $D_i$.
    \end{itemize}
  \item \textbf{Classification: classify an unknown sample $X$.}
    \begin{itemize}
    \item Each classifier $M_i$ returns its class prediction.
    \item The bagged classifier $M^*$ counts the votes and assigns the class with the most votes to $X$.
    \end{itemize}
  \item \textbf{Prediction:}
    \begin{itemize}
    \item Can be applied to the prediction of continuous values by taking the average value of each prediction for a given test tuple.
    \end{itemize}
  \item \textbf{Accuracy:}
    \begin{itemize}
    \item Often significantly better than a single classifier derived from $D$.
    \item For noisy data: not considerably worse, more robust.
    \item Proved improved accuracy in prediction.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Boosting}
  \begin{itemize}
  \item \textbf{Analogy:}
    \begin{itemize}
    \item Consult several doctors, based on a combination of weighted diagnoses -- weight assigned based on the previous diagnosis accuracy
    \end{itemize}
  \item \textbf{How boosting works:}
    \begin{itemize}
    \item Weights are assigned to each training tuple.
    \item A series of $k$ classifiers is iteratively learned.
    \item After a classifier $M_i$ is learned, the weights are updated to allow the subsequent classifier, $M_{i+1}$ to pay more attention to the training tuples that were misclassified by $M_i$.
    \item The final $M^*$ combines the votes of each individual classifier, where the weight of each classifier's vote is a function of its accuracy.
    \end{itemize}
  \item \textbf{Boosting algorithm can be extended for numeric prediction.}
    \begin{itemize}
    \item Each classifier $M_i$ returns its class prediction.
    \item The bagged classifier $M^*$ counts the votes and assigns the class with the most votes to $X$.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{AdaBoost ("Adaptive Boosting" (Freund and Schapire, 1997))}
  \begin{itemize}
  \item \textbf{Given a set of d class-labeled tuples: $(x_1 , y_1), \ldots, (x_d, y_d)$.}
  \item \textbf{Initially, all the weights of tuples are set the same: $\frac{1}{d}$.}
  \item \textbf{Generate $k$ classifiers in $k$ rounds. At round $i$,}
    \begin{itemize}
    \item Tuples from $D$ are sampled (with replacement) to form a training set $D_i$ of the same size.
    \item Each tuple's chance of being selected is based on its weight.
    \item A classification model $M_i$ is derived from $D_i$.
    \item Its error rate is calculated using $D_i$ as a test set.
    \item If a tuple is misclassified, its weight is increased, otherwise it is decreased.
    \end{itemize}
  \item \textbf{Error rate: $\text{err}(x_j)$ is the misclassification error of tuple $x_j$. Classifier $M_i$ error rate is the sum of the weights of the misclassified tuples:}
    \begin{align}
      \text{error}(M_i) = \sum_{j=1}^{d} w_j \cdot \text{err}(x_j).
    \end{align}
    \textbf{The weight of classifier $M_i$'s vote is: $\log \frac{1-\text{error}(M_i)}{\text{error}(M_i)}$.}
  \end{itemize}
\end{frame}

\begin{frame}{Random Forest (Breiman, 2001)}
  \begin{itemize}
  \item \textbf{Random forest:}
    \begin{itemize}
    \item Each classifier in the ensemble is a decision-tree classifier and is generated using a random selection of attributes at each node to determine the split.
    \item During classification, each tree votes and the most popular class is returned.
    \end{itemize}
  \item \textbf{Two methods to construct random forests:}
    \begin{itemize}
    \item Forest-RI (random input selection):
      \begin{itemize}
      \item Randomly select, at each node, F attributes as candidates for the split at the node. The CART methodology is used to grow the trees to maximum size.
      \end{itemize}
    \item Creates new attributes (or features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers).
    \end{itemize}
  \item \textbf{Comparable in accuracy to AdaBoost, but more robust to errors and outliers.}
  \item \textbf{Insensitive to the number of attributes selected for consideration at each split, and faster than bagging or boosting.}
  \end{itemize}
\end{frame}

\begin{frame}{Classification of Class-imbalanced Data Sets}
  \begin{itemize}
  \item \textbf{Class-imbalance problem:}
    \begin{itemize}
    \item Rare positive example but numerous negative ones.
      \begin{itemize}
      \item E.g., medical diagnosis, fraud, oil-spill, fault, etc.
      \end{itemize}
    \item Traditional methods assume a balanced distribution of classes and equal error costs: not suitable for class-imbalanced data.
    \end{itemize}
  \item \textbf{Typical methods for imbalanced data in 2-class classification:}
    \begin{itemize}
    \item \textbf{\color{airforceblue}Oversampling:}
      \begin{itemize}
      \item Re-sampling of data from positive class.
      \end{itemize}
    \item \textbf{\color{airforceblue}Undersampling:}
      \begin{itemize}
      \item Randomly eliminate tuples from negative class.
      \end{itemize}
    \item \textbf{\color{airforceblue}Threshold-moving:}
      \begin{itemize}
      \item Moves the decision threshold, $t$, so that the rare-class tuples are easier to classify, and hence, less chance of costly false-negative errors
      \end{itemize}
    \item \textbf{\color{airforceblue}Ensemble techniques:}
      \begin{itemize}
      \item Ensemble multiple classifiers introduced above.
      \end{itemize}
    \end{itemize}
  \item \textbf{Still difficult on multi-class tasks.}
  \end{itemize}
\end{frame}
