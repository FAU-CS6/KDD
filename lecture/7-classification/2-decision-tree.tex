\section{Decision Tree Induction}

\begin{frame}{Decision Tree: An Example}
	\vspace*{0.5cm}
	\begin{columns}
		\begin{column}{0.35\textwidth}
			\vspace{0cm}
			\begin{itemize}
				\item \textbf{Training dataset:} buys\_computer.
				\item \textbf{Resulting tree:}\\[0.1cm]
			\end{itemize}
			\centering
			\scalebox{0.9}{
				\input{7-classification/tree-example.tex}
			}
		\end{column}
		\begin{column}{0.55\textwidth}
			\vspace*{-1cm}
			\begin{center}
				\scalebox{0.8}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}{Decision Tree}
	\vspace*{-0.6em}
	\begin{block}{Decision Tree Induction}
		\textit{Decision tree induction} refers to the learning of a decision-tree based on labeled training data.
	\end{block}

	\begin{block}{Decision Tree}
		A \textit{decision tree} is a flowchart-like structure consisting of interconnected internal and leaf nodes.
	\end{block}

	\begin{columns}[t]
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{figure}[t]
				\centering
				\input{7-classification/tree-example.tex}
			\end{figure}

		\end{column}
		\begin{column}{0.55\textwidth}
			\textbf{Components of a Decision Tree}
			\begin{itemize}
				\item \tikzmark{root} \textbf{Root}: topmost node.
				\item \tikzmark{internal} \textbf{Internal node}: test on an attribute.
				\item \tikzmark{leaf-node} \textbf{Leaf node}: holds a class label, also called \textit{terminal node}.
				\item \tikzmark{branch} \textbf{Branch}: outcome of a leaf node's test coupled with a text. In this example: \texttt{excellent}.
			\end{itemize}
		\end{column}
	\end{columns}

	\begin{tikzpicture}[remember picture,overlay]
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]root) to[out=170,in=0] (age.east);
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]internal) to[out=180,in=10] (credit-rating.east);
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]leaf-node) to[out=190,in=30] (credit-yes.east);
		\draw[faucyan,thick,->] ([xshift=-3mm]branch) to[out=-120,in=-90] ([xshift=1mm]$(credit-no.north east) + (.1em,.7em)$);
	\end{tikzpicture}
\end{frame}

\begin{frame}{Algorithm for Decision Tree Induction (I)}
	\vspace*{-0em}
	\begin{columns}
		\begin{column}{0.7\textwidth}
			\textbf{Construction} in a \textit{top-down recursive} and \textit{divide-and-conquer} manner.\\\medskip
			\textbf{Input:} data partition $D$, \texttt{attribute\_list}, \texttt{attribute\_selection\_method}.\\\medskip

			\textbf{Algorithm Sketch \texttt{build\_decision\_tree}:}
			\footnotesize
			\begin{enumerate}
				\item Create node $N$.
				\item Determine splitting attribute $A$ with \texttt{attribute\_selection\_method}.
				\item Label $N$ with splitting criterion.
				\item If the splitting attribute has been fully utilized, remove it from \texttt{attribute\_list}.
				\item For each outcome of splitting criterion:
				      \begin{itemize}
					      \footnotesize
					      \item Partition $D$ according to outcome of splitting criterion.
					      \item Grow branches on $N$ for each partition.
				      \end{itemize}
				\item Return node $N$
			\end{enumerate}


		\end{column}
		\begin{column}{0.2\textwidth}
			\vspace*{-2em}
			\centering

			Attribute Types:
			\small

			Discrete:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-discrete.tex}
			\end{figure}
			~ \\\bigskip
			Discrete \& Binary Tree:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-discrete-binary.tex}
			\end{figure}

			~ \\\bigskip
			Continuous:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-continuous.tex}
			\end{figure}

		\end{column}
	\end{columns}

	\note[itemize]{
		\item data partition $D$ = initially whole training dataset
		\item \texttt{attribute\_list} = list of all attributes
		\item \texttt{attribute\_selection\_method} = heuristic to find best split
		\item Node $N$ could be root or internal node.
	}

\end{frame}

\begin{frame}{Algorithm for Decision Tree Induction (II)}
	\textbf{Stopping criteria:}
	\begin{itemize}
		\item All samples in $D$ belong to the same class: \\
		      $\Rightarrow$ $N$ becomes a leaf.
		\item Samples in $D$ belong to multiple classes, but \texttt{attribute\_list} is empty: \\
		      $\Rightarrow$ Create leaf with majority class.
		\item Partition $D$ is empty: \\
		      $\Rightarrow$ Create leaf with majority class.
	\end{itemize}
	\vfill
	\begin{alertblock}{Decision Tree Algorithm}
		A detailed algorithm to construct a decision tree is covered in the appendix and in our reference book\footfullcite[pp. 332 -- 335]{han2011}.
	\end{alertblock}
\end{frame}


\begin{frame}{Attribute Selection Methods}
	\begin{block}{Attribute Selection Methods}
		An \textit{attribute selection method} is a heuristic to determine the ``best'' splitting criterion to partition data.
	\end{block}

	\begin{itemize}
		\item Also known as \textit{splitting rules}.
		\item Provides ranking for each attribute.
		\item Partition data based on attribute with best score.
		\item Tree node is labeled with splitting criterion (attribute).
	\end{itemize} \medskip

	We will discuss \textbf{three} popular attribute selection methods\footnote{Since this it not even close to an exhaustive list, we list some other popular methods in the appendix.}:
	\begin{enumerate}
		\item Information Gain
		\item Gain Ratio
		\item Gini Index
	\end{enumerate}
\end{frame}

\subsection{Information Gain (ID3)}

\begin{frame}{Information Gain (ID3) (I)}
	\begin{itemize}
		\item Partitions reflect least randomness, i. e. impurity.
		\item \textbf{Expected information} (entropy) needed to classify a tuple in $D$:
		      \begin{align*}
			      \text{Info}(D) = -\sum_{i=1}^{m}p_i \log_2(p_i)
		      \end{align*}

		      \begin{itemize}
			      \item $p_i$ is the probability that tuple in $D$ belongs to class $C_i$, estimated by $\frac{|C_i|}{|D|}$.
			      \item $\log_2$ because information is encoded in bits.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Information Gain (ID3) (II)}
	Calculate information for every attribute in \texttt{attribute\_list} and data partition $D$:
	\vspace*{-1.5em}
	\begin{columns}
		\visible<2->{
			\begin{column}{0.5\textwidth}
				\begin{center}
					\textbf{Discrete-valued Attribute}
				\end{center}
				\vspace*{-1em}
				\begin{itemize}
					\item Attribute $A$ with $v$ distinct values.
					\item Expected information required to classify tuple in $D$ based on partitioning by $A$.
					\item $D_A$: dataset $D$ partitioned by $A$, \\
					      $D_{A,j}$: dataset $D$ partitioned by $A$ with distinct value $j$.
				\end{itemize}
			\end{column}
		}
		\visible<3->{
			\begin{column}{0.5\textwidth}
				\begin{center}
					\textbf{Continuous-valued Attribute}
				\end{center}
				\vspace*{-1em}

				\begin{itemize}
					\item Attribute $A$ with $v$ distinct values.
					\item Order values in increasing order.
					\item Calculate midpoint of every neighbouring value.
					\item $v-1$ possible split points $s_i=\frac{a_i + a_{i+1}}{2}$.
					\item Evaluate $\text{Info}_A(D)$ for every possible binary splitting ($A\leq s_i$ and  $A>s_i$).
				\end{itemize}

			\end{column}
		}
	\end{columns}

	\vspace*{0.25cm}

	\begin{columns}
		\visible<2->{
			\begin{column}{0.5\textwidth}
				{
					\footnotesize
					\begin{align*}
						\text{Info}_A(D) = \sum_{j=1}^v \frac{|D_{A,j}|}{|D_A|} \text{Info}(D_{A,j})
					\end{align*}
				}
			\end{column}
		}
		\visible<3->{
			\begin{column}{0.5\textwidth}
				{
					\footnotesize
					\begin{align*}
						\text{Info}_A(D) = \frac{|D_{A\leq s_i}|}{|D_A|} \text{Info}(D_{A\leq s_i}) + \frac{|D_{A>s_i}|}{|D_A|} \text{Info}(D_{A>s_i})
					\end{align*}
				}
			\end{column}
		}
	\end{columns}

\end{frame}

\begin{frame}{Information Gain (ID3) (III)}
	Given $\text{Info}(D)$ and $\text{Info}_A(D)$, Information Gain of each attribute $A$ is calculated as:
	\begin{align*}
		\text{Gain}(A) = \text{Info}(D) - \text{Info}_A(D)
	\end{align*}

	\vspace*{0.75cm}

	\begin{block}{Information Gain}
		The attribute with the \textbf{highest} Information Gain is selected as the splitting attribute.
	\end{block}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (I)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Target attribute:} buys\_computer \medskip
				\item \textbf{Expected Information}:
			\end{itemize}
			{
			\footnotesize
			\begin{align*}
				\text{Info}(D) & = I(9,5)                                                               \\
				               & = - \frac{9}{14}\log_2(\frac{9}{14})-\frac{5}{14} \log_2(\frac{5}{14}) \\
				               & = 0.94                                                                 \\
			\end{align*}
			}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (II)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{itemize}
				\item \textbf{Attribute:} Age \medskip
				\item \textbf{Value distribution:}
				      \vspace*{0.1cm}
				      \begin{center}
					      \scalebox{0.8}{
						      \begin{tabular}{|c|c|c|c|}
							      \hline
							      \cellcolor{faugray!62}Age     & \cellcolor{faugray!62}Yes & \cellcolor{faugray!62}No & \cellcolor{faugray!62}$I(\text{Yes},\text{No})$ \\\hline
							      \cellcolor{white}$\leq 30$    & 2                         & 3                        & 0.971                                           \\\hline
							      \cellcolor{white}$31\ldots40$ & 4                         & 0                        & 0                                               \\\hline
							      \cellcolor{white}$>40$        & 3                         & 2                        & 0.971                                           \\\hline
						      \end{tabular}
					      }\medskip
				      \end{center}
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Info}_{\texttt{age}}(D) & = \frac{5}{14}I(2,3) + \frac{4}{14} I(4,0) + \frac{5}{14} I(3,2) \\
						                                    & = 0.694                                                          \\
						      \text{Gain}(\texttt{age})     & = \text{Info}(D)-\text{Info}_{\texttt{age}}(D)                   \\
						                                    & = 0.94 - 0.694                                                   \\
						                                    & = 0.246
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\only<1>{
				\vspace*{-0.5cm}
				\begin{center}
					\scalebox{0.7}{
						\input{7-classification/table-example.tex}
					}
				\end{center}
			}
			\only<2->{
				\vspace*{-0.5cm}
				\begin{itemize}
					\item \textbf{Gain of other attributes:}
					      \begin{itemize}
						      \item $\text{Gain}(\texttt{income}) = 0.029$,
						      \item $\text{Gain}(\texttt{student}) = 0.151$,
						      \item $\text{Gain}(\texttt{credit\_rating}) = 0.048$.
					      \end{itemize} \medskip
					      \visible<3->{
					\item \textbf{Splitting attribute:} \\
					      Age  with $\text{Gain}(\texttt{age}) = 0.246$.
					      }
				\end{itemize}
			}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (III)}
	\centering

	\begin{tikzpicture}[
			overlay,
			remember picture,
			>=latex,
			thick,
			node/.style={
					draw=faugray,
					rounded corners=.25em,
					fill=faugray!62,
					text depth=0.2em
				},
			leaf/.style={
					draw,
					rounded corners=.7em,
					text depth=0.2em
				},
			branch/.style={
					fill=white,
					font=\ttfamily\scriptsize,
					rounded corners=.7em,
					text depth=0.2em
				}
		]
		\node[node] at (0,0) (age) {age?};

		\node[draw=none,below left=3em and 1.5em of age,text width=3.8em] (student) {};
		\node[draw=none,below right=3em and .6em of age,text width=6em] (credit-rating) {};

		\draw[rounded corners=5pt]
		(age.south) -- ($(age.south) + (0,-1em)$) --
		($(student.north) + (0,2em)$) -- (student.north);
		\node[branch,above=2em of student.north] {$\leq$30};

		\path[draw,rounded corners=5pt]
		(age.south) -- ($(age.south) + (0,-1em)$) --
		($(credit-rating.north) + (0,2em)$) -- (credit-rating.north);
		\node[branch,above=2em of credit-rating.north] {>40};
		\node at (-4,-2.4) (t1) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faured}no}                 \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faured}no}                 \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
				\end{tabular}
			}
		};
		\node[below=10.5em of age] (t3) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}high        & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
				\end{tabular}
			}
		};
		\draw (age.south) -- (t3.north);
		\node[branch,below=3em of age.north] {31\dots 40};
		\node at (4,-2.4) (t2) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
					\cellcolor{white}medium      & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
				\end{tabular}
			}
		};
	\end{tikzpicture}
\end{frame}

\subsection{Gain Ratio (C4.5)}

\begin{frame}{Gain Ratio (C4.5)}
	\begin{itemize}
		\item \textbf{Problem of Information Gain:}
		      \begin{itemize}
			      \item Tends to prefer attributes with large number of distinct values.
			            \begin{itemize}
				            \item E. g. attribute \texttt{degree\_program} with 278 values vs. \texttt{student\_status} with 2 values.
				                  % 278 is the number of all degree programs at FAU in WS2024/25
			            \end{itemize}
		      \end{itemize}
		      \visible<2->{
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Normalize the Information Gain to get the \textbf{Gain Ratio (C4.5)}:
			            \begin{align*}
				            \text{SplitInfo}_A(D) & = - \sum_{j=1}^{v} \frac{|D_j|}{|D|} \log_2\left( \frac{|D_j|}{|D|} \right) \\
				            \text{GainRatio}(A)   & = \frac{\text{Gain}(A)}{\text{SplitInfo}_A(D)}
			            \end{align*}
			      \item \textbf{Disadvantage:} Becomes unstable as $\text{SplitInfo}_A(D)$ approaches zero.

			            \begin{block}{Gain Ratio}
				            The attribute with the \textbf{highest} Gain Ratio is selected as the splitting attribute.
			            \end{block}

		      \end{itemize}
		      }
	\end{itemize}
\end{frame}

\begin{frame}{Gain Ratio (C4.5) - Example}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5em}
			\begin{itemize}
				\item \textbf{Attribute:} Age \medskip
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Gain}(\texttt{age})  =     & 0.246                                                                                        \\
						      \text{SplitInfo}_{age}(D)  =     & -\frac{5}{14} \log_2\left(\frac{5}{14}\right) - \frac{4}{14} \log_2\left(\frac{4}{14}\right) \\
						                                       & - \frac{5}{14} \log_2\left(\frac{5}{14}\right)                                               \\
						      =                                & 1.577                                                                                        \\
						      \text{GainRatio}(\texttt{age}) = & \frac{0.246}{1.577}                                                                          \\
						      =                                & 0.156
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\subsection{Gini Index (CART)}

\begin{frame}{Gini Index (CART) (I)}
	\begin{itemize}
		\item \textbf{Problem of Information Gain and Gain Ratio:}
		      \begin{itemize}
			      \item Use of logarithm is computationally expensive.
		      \end{itemize}
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Use the \textbf{Gini Index (CART)} as an alternative to Information Gain and Gain Ratio.
			      \item Measures the \textbf{statistical dispersion} of a dataset.
		      \end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}{Gini Index (CART) (II)}
	\begin{itemize}
		\item \textbf{Measured impurity} of partition $D$ is defined as the sum over $n$ classes:
		      \vspace*{-1em}
		      \begin{align*}
			      \text{Gini}(D) = 1-\sum_{j=1}^{n} p_j^2
		      \end{align*}
		      \vspace*{-1em}
		      \begin{itemize}
			      \item $p_j$ is the non-zero probability that sample in $D$ belongs to class $C_j$ as estimated by $\frac{|C_{j,D}|}{|D|}$
		      \end{itemize}
	\end{itemize}

	\vspace*{-1em}

	\visible<2->{
		\begin{columns}
			\visible<3->{
				\begin{column}{0.45\textwidth}
					\begin{center}
						\textbf{Discrete-valued Attribute}
					\end{center}
					\vspace*{-1em}
					\begin{itemize}
						\item Attribute $A$ with $v$ distinct values.
						\item Compute all possible subsets of values.
						\item Compute weighted sum of each partition tuple ($D_1$ and $D_2$) as follows:
					\end{itemize}
				\end{column}
			}
			\visible<4->{
				\begin{column}{0.45\textwidth}
					\begin{center}
						\textbf{Continuous-valued Attribute}
					\end{center}
					\vspace*{-1em}

					\begin{itemize}
						\item Order values in increasing order.
						\item Split the dataset at every midpoint.
						\item Evaluate $\text{Gini}_A(D)$ for every possible binary splitting:
					\end{itemize}

				\end{column}
			}
		\end{columns}

		\vspace*{0.25cm}

		\only<-4>{
			\begin{columns}
				\visible<3->{
					\begin{column}{0.5\textwidth}
						{
							\footnotesize
							\begin{align*}
								\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
							\end{align*}
						}
					\end{column}
				}
				\visible<4->{
					\begin{column}{0.5\textwidth}
						{
							\footnotesize
							\begin{align*}
								\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
							\end{align*}
						}
					\end{column}
				}
			\end{columns}
		}
		% EXTRA ANIMATION STEP:
		% Might be used to show that the formula is the same for both cases. But for now, we decided to keep it 
		% simple.
		%
		%\only<5->{
		%	{
		%			\footnotesize
		%			\begin{align*}
		%				\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
		%			\end{align*}
		%		}
		%}
	}
\end{frame}

\begin{frame}{Gini Index (CART) (III)}
	\begin{itemize}
		\item The \textbf{reduction in impurity} is defined as
		      follows:
		      \begin{align*}
			      \Delta\text{Gini}_A(D) = \text{Gini}(D)-\text{Gini}_A(D).
		      \end{align*}
	\end{itemize}

	\vspace*{4em}

	\begin{block}{Gini Index}
		The attribute (and partitioning) with the \textbf{highest} reduction in impurity is selected as the splitting attribute.
	\end{block}

\end{frame}

\begin{frame}{Gini Index (CART) - Example (I)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Target attribute:} buys\_computer \medskip
				\item \textbf{Measured impurity}:
			\end{itemize}
			{
			\footnotesize
			\begin{align*}
				\text{Gini}(D) & = 1 - \left( \frac{9}{14} \right)^2 - \left( \frac{5}{14} \right)^2 \\
				               & = 0.459
			\end{align*}
			}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Gini Index (CART) - Example (II)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{itemize}
				\item \textbf{Attribute:} Income \medskip
				\item \textbf{Subsets:}
				      {
					      \footnotesize
					      \begin{align*}
						      D_1: & \{\texttt{low,medium}\} \\
						      D_2: & \{\texttt{high}\}
					      \end{align*}
				      }
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Gini}(D\vert_{\texttt{i}=\{high\}}) = & \text{Gini}(D\vert_{\texttt{i}=\{medium,low\}})                                             \\
						      =                                           & \frac{10}{14} \text{Gini}(D_1) + \frac{4}{14} \text{Gini}(D_2)                              \\
						      =                                           & \frac{10}{14} \left(1-\left( \frac{7}{10} \right)^2 - \left( \frac{3}{10} \right)^2 \right) \\
						      +                                           & \frac{4}{14} \left( 1-\left( \frac{2}{4} \right)^2 - \left( \frac{2}{4} \right)^2 \right)   \\
						      =                                           & 0.443
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\only<1>{
				\vspace*{-0.5cm}
				\begin{center}
					\scalebox{0.7}{
						\input{7-classification/table-example.tex}
					}
				\end{center}
			}
			\only<2->{
				\vspace*{-0.5cm}
				\begin{itemize}
					\item \textbf{Reduction in impurity:}
					      {
						      \footnotesize
						      \begin{align*}
							      \Delta\text{Gini}_{\texttt{i}=\{high\}}(D) = & \text{Gini}(D)-\text{Gini}(D\vert_{\texttt{i}=\{high\}}) \\
							      =                                            & 0.459 - 0.443                                            \\
							      =                                            & 0.016
						      \end{align*}
					      }
					\item \textbf{Other subsets:}
					      \begin{itemize}
						      \item $\Delta\text{Gini}_{\texttt{i}=\{medium\}}(D) = 0.001$
						      \item $\Delta\text{Gini}_{\texttt{i}=\{low\}}(D) = 0.009$
					      \end{itemize} \medskip
					      \visible<3->{
					\item \textbf{Splitting subset:} \\
					      \begin{itemize}
						      \item If income has the overall highest reduction of impurity, then the split is on \{"low","medium"\} and \{"high"\}.
					      \end{itemize}
					      }
				\end{itemize}
			}
		\end{column}
	\end{columns}
\end{frame}

\subsection*{Overview}

\begin{frame}{Attribute Selection Methods Overview}
	\textbf{The three methods, in general, return good results, but}
	\begin{itemize}
		\item \textbf{\color{airforceblue}Information Gain:}
		      \begin{itemize}
			      \item Biased towards multi-valued attributes.
		      \end{itemize}
		\item \textbf{\color{airforceblue}Gain Ratio:}
		      \begin{itemize}
			      \item Tends to prefer unbalanced splits in which one partition is much smaller than the others.
		      \end{itemize}
		\item \textbf{\color{airforceblue}Gini Index:}
		      \begin{itemize}
			      \item Biased to multi-valued attributes.
			      \item Has difficulty when number of classes is large.
			      \item Tends to favor tests that result in equal-sized partitions and purity in both partitions.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Decision Tree Induction - Overfitting}
	\begin{itemize}
		\item \textbf{Problem:}
		      \begin{itemize}
			      \item Many branches may reflect anomalies due to noise or outliers.
			      \item Overall poor accuracy for unseen samples.
		      \end{itemize}
		      $\Rightarrow$ \textbf{Overfitting:} An induced tree may overfit the training data.
		      \visible<2->{
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item \textbf{\color{airforceblue}Pruning:} Avoid branches that have little importance.
			      \item \textbf{Two} types of pruning:
			            \begin{enumerate}
				            \item \textbf{\color{airforceblue}Prepruning:} Stop growing the tree early.
				            \item \textbf{\color{airforceblue}Postpruning:} Remove branches from a "fully grown" tree.
			            \end{enumerate}
		      \end{itemize}
		      }
	\end{itemize}
\end{frame}

\begin{frame}{Decision Tree Induction - Enhancements}
	\begin{itemize}
		\item A lot of research has been done to improve basic \textbf{decision tree induction} algorithms. E.g.:
		      \begin{itemize}
			      \item \textbf{Allow for} \textbf{\color{airforceblue}continuous-valued attributes.}
			            \begin{itemize}
				            \item Dynamically define new discrete-valued attributes that partition the values of continuous-valued attributes into a discrete set of intervals.
			            \end{itemize}
			      \item \textbf{Handle} \textbf{\color{airforceblue}missing attribute values.}
			            \begin{itemize}
				            \item Assign the most common value of the attribute.
				            \item Assign probability to each of the possible values.
			            \end{itemize}
			      \item \textbf{\color{airforceblue}Attribute construction.}
			            \begin{itemize}
				            \item Create new attributes based on existing ones that are sparsely represented.
				            \item This reduces fragmentation, repetition, and replication.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Decision Tree Induction - Scalability}
	\begin{itemize}
		\item \textbf{Problem:}
		      \begin{itemize}
			      \item Basic decision tree algorithms (ID3, C4.5, and CART) are not scalable \\
			            (They require the entire dataset to be in memory).
			      \item They are not designed to handle large datasets.
		      \end{itemize}
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Extend the basic algorithms to handle large datasets.
			      \item We will take a look at \textbf{two} modifications/methods:
			            \begin{enumerate}
				            \item RainForest
				            \item BOAT
			            \end{enumerate}
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Scalability: RainForest}
	\begin{itemize}
		\item \textbf{Basic Idea:}
		      \begin{itemize}
			      \item Extract all data required for the attribute selection methods \\
			            $\Rightarrow$ Store it in a \textbf{compact} data structure.
			      \item Apply the original algorithm to the compact data structure(s).
		      \end{itemize}
		\item \textbf{Data structure(s):}
		      \begin{itemize}
			      \item \textbf{\color{airforceblue}AVC-list:}
			            \begin{itemize}
				            \item \textbf{\color{airforceblue}A}ttribute
				            \item \textbf{\color{airforceblue}V}alue
				            \item \textbf{\color{airforceblue}C}lass label
			            \end{itemize}
			      \item \textbf{\color{airforceblue}AVC-set} \textbf{(of an attribute X):}
			            \begin{itemize}
				            \item Aggregated projection of training dataset onto the attribute $X$ with counts of each class label.
			            \end{itemize}
			      \item \textbf{\color{airforceblue}AVC-group} \textbf{(of a node n):}
			            \begin{itemize}
				            \item Set of AVC-sets of all predictor attributes at the node $n$.
			            \end{itemize}
		      \end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}{Scalability: RainForest - AVC-set Example}
	\vspace*{-0.25cm}
	\begin{center}
		\scalebox{0.7}{
			\input{7-classification/table-example.tex}
		}
	\end{center}
	\vspace*{0.25cm}
	\begin{center}
		\begin{columns}
			\begin{column}{0.225\textwidth}
				\visible<2->{
					\footnotesize
					\centering
					AVC-set on age:\\
					\begin{tabular}{|c|c|c|}
						\hline
						\rowcolor{faugray!62}\textbf{age} & \textbf{yes} & \textbf{no} \\\hline
						$\leq 30$                         & 2            & 3           \\\hline
						$31\ldots40$                      & 4            & 0           \\\hline
						$>40$                             & 3            & 2           \\\hline
					\end{tabular}
				}
			\end{column}
			\begin{column}{0.225\textwidth}
				\visible<3->{
					\footnotesize
					\centering
					AVC-set on income:\\
					\begin{tabular}{|c|c|c|}
						\hline
						\rowcolor{faugray!62}\textbf{income} & \textbf{yes} & \textbf{no} \\\hline
						high                                 & 2            & 2           \\\hline
						medium                               & 4            & 2           \\\hline
						low                                  & 3            & 1           \\\hline
					\end{tabular}
				}
			\end{column}
			\begin{column}{0.225\textwidth}
				\visible<4->{
					\footnotesize
					\centering
					AVC-set on student:\\
					\begin{tabular}{|c|c|c|}
						\hline
						\rowcolor{faugray!62}\textbf{student} & \textbf{yes} & \textbf{no} \\\hline
						yes                                   & 6            & 1           \\\hline
						no                                    & 3            & 4           \\\hline
					\end{tabular}
				}
			\end{column}
			\begin{column}{0.225\textwidth}
				\visible<5->{
					\footnotesize
					\centering
					AVC-set on credit\_rating:\\
					\begin{tabular}{|c|c|c|}
						\hline
						\rowcolor{faugray!62}\textbf{credit\_rating} & \textbf{yes} & \textbf{no} \\\hline
						fair                                         & 6            & 2           \\\hline
						excellent                                    & 3            & 3           \\\hline
					\end{tabular}
				}
			\end{column}
		\end{columns}
	\end{center}
\end{frame}


\begin{frame}{Scalability: BOAT}
	\begin{itemize}
		\item \textbf{Basic Idea:}
		      \begin{itemize}
			      \item Use a statistical technique to create several smaller samples (subsets).
			      \item Every sample fits in the memory and is used to induce a decision tree.
			      \item All trees are combined to form a single tree T'.
		      \end{itemize}
		\item \textbf{Advantages:}
		      \begin{itemize}
			      \item Resulting tree often very close to the tree induced from the entire dataset.
			      \item Requires only two scans of the DB.
			      \item An incremental algorithm:
			            \begin{itemize}
				            \item Take insertions and deletions of training data and update the decision tree.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}

	\vspace*{0.5cm}

	\begin{block}{BOAT}
		The full title of BOAT is \textit{Bootstrapped Optimistic Algorithm for Tree Construction}, which refers to the underlying statistical technique used: \textbf{Bootstrapping}.
	\end{block}
\end{frame}
