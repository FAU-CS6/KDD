\section{Decision Tree Induction}

\begin{frame}{Decision Tree: An Example}
	\vspace*{0.5cm}
	\begin{columns}
		\begin{column}{0.35\textwidth}
			\vspace{0cm}
			\begin{itemize}
				\item \textbf{Training dataset:} buys\_computer.
				\item \textbf{Resulting tree:}\\[0.1cm]
			\end{itemize}
			\centering
			\scalebox{0.9}{
				\input{7-classification/tree-example.tex}
			}
		\end{column}
		\begin{column}{0.55\textwidth}
			\vspace*{-1cm}
			\begin{center}
				\scalebox{0.8}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}{Decision Tree}
	\vspace*{-0.6em}
	\begin{block}{Decision Tree Induction}
		\textit{Decision tree induction} refers to the learning of a decision-tree based on labeled training data.
	\end{block}

	\begin{block}{Decision Tree}
		A \textit{decision tree} is a flowchart-like structure consisting of interconnected internal and leaf nodes.
	\end{block}

	\begin{columns}[t]
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{figure}[t]
				\centering
				\input{7-classification/tree-example.tex}
			\end{figure}

		\end{column}
		\begin{column}{0.55\textwidth}
			\textbf{Components of a Decision Tree}
			\begin{itemize}
				\item \tikzmark{root} \textbf{Root}: topmost node.
				\item \tikzmark{internal} \textbf{Internal node}: test on an attribute.
				\item \tikzmark{leaf-node} \textbf{Leaf node}: holds a class label, also called \textit{terminal node}.
				\item \tikzmark{branch} \textbf{Branch}: outcome of a leaf node's test coupled with a text. In this example: \texttt{excellent}.
			\end{itemize}
		\end{column}
	\end{columns}

	\begin{tikzpicture}[remember picture,overlay]
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]root) to[out=170,in=0] (age.east);
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]internal) to[out=180,in=10] (credit-rating.east);
		\draw[faucyan,thick,->] ([yshift=1.5mm,xshift=-4mm]leaf-node) to[out=190,in=30] (credit-yes.east);
		\draw[faucyan,thick,->] ([xshift=-3mm]branch) to[out=-120,in=-90] ([xshift=1mm]$(credit-no.north east) + (.1em,.7em)$);
	\end{tikzpicture}
\end{frame}

\begin{frame}{Algorithm for Decision Tree Induction (I)}
	\vspace*{-0em}
	\begin{columns}
		\begin{column}{0.7\textwidth}
			\textbf{Construction} in a \textit{top-down recursive} and \textit{divide-and-conquer} manner.\\\medskip
			\textbf{Input:} data partition $D$, \texttt{attribute\_list}, \texttt{attribute\_selection\_method}.\\\medskip

			\textbf{Algorithm Sketch \texttt{build\_decision\_tree}:}
			\footnotesize
			\begin{enumerate}
				\item Create node $N$.
				\item Determine splitting attribute $A$ with \texttt{attribute\_selection\_method}.
				\item Label $N$ with splitting criterion.
				\item If the splitting attribute has been fully utilized, remove it from \texttt{attribute\_list}.
				\item For each outcome of splitting criterion:
				      \begin{itemize}
					      \footnotesize
					      \item Partition $D$ according to outcome of splitting criterion.
					      \item Grow branches on $N$ for each partition.
				      \end{itemize}
				\item Return node $N$
			\end{enumerate}


		\end{column}
		\begin{column}{0.2\textwidth}
			\vspace*{-2em}
			\centering

			Attribute Types:
			\small

			Discrete:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-discrete.tex}
			\end{figure}
			~ \\\bigskip
			Discrete \& Binary Tree:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-discrete-binary.tex}
			\end{figure}

			~ \\\bigskip
			Continuous:
			\begin{figure}[t]
				\centering
				\input{7-classification/decision-tree-split-continuous.tex}
			\end{figure}

		\end{column}
	\end{columns}

	\note[itemize]{
		\item data partition $D$ = initially whole training dataset
		\item \texttt{attribute\_list} = list of all attributes
		\item \texttt{attribute\_selection\_method} = heuristic to find best split
		\item Node $N$ could be root or internal node.
	}

\end{frame}

\begin{frame}{Algorithm for Decision Tree Induction (II)}
	\textbf{Stopping criteria:}
	\begin{itemize}
		\item All samples in $D$ belong to the same class: \\
		      $\Rightarrow$ $N$ becomes a leaf.
		\item Samples in $D$ belong to multiple classes, but \texttt{attribute\_list} is empty: \\
		      $\Rightarrow$ Create leaf with majority class.
		\item Partition $D$ is empty: \\
		      $\Rightarrow$ Create leaf with majority class.
	\end{itemize}
	\vfill
	\begin{alertblock}{Decision Tree Algorithm}
		A detailed algorithm to construct a decision tree is covered in the appendix and in our reference book\footfullcite[pp. 332 -- 335]{han2011}.
	\end{alertblock}
\end{frame}


\begin{frame}{Attribute Selection Methods}
	\begin{block}{Attribute Selection Methods}
		An \textit{attribute selection method} is a heuristic to determine the ``best'' splitting criterion to partition data.
	\end{block}

	\begin{itemize}
		\item Also known as \textit{splitting rules}.
		\item Provides ranking for each attribute.
		\item Partition data based on attribute with best score.
		\item Tree node is labeled with splitting criterion (attribute).
	\end{itemize} \medskip

	We will discuss \textbf{three} popular attribute selection methods\footnote{Since this it not even close to an exhaustive list, we list some other popular methods in the appendix.}:
	\begin{enumerate}
		\item Information Gain
		\item Gain Ratio
		\item Gini Index
	\end{enumerate}
\end{frame}

\subsection{Information Gain (ID3)}

\begin{frame}{Information Gain (ID3) (I)}
	\begin{itemize}
		\item Partitions reflect least randomness, i. e. impurity.
		\item \textbf{Expected information} (entropy) needed to classify a tuple in $D$:
		      \begin{align*}
			      \text{Info}(D) = -\sum_{i=1}^{m}p_i \log_2(p_i)
		      \end{align*}

		      \begin{itemize}
			      \item $p_i$ is the probability that tuple in $D$ belongs to class $C_i$, estimated by $\frac{|C_i|}{|D|}$.
			      \item $\log_2$ because information is encoded in bits.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Information Gain (ID3) (II)}
	Calculate information for every attribute in \texttt{attribute\_list} and data partition $D$:
	\vspace*{-1.5em}
	\begin{columns}
		\visible<2->{
			\begin{column}{0.5\textwidth}
				\begin{center}
					\textbf{Discrete-valued Attribute}
				\end{center}
				\vspace*{-1em}
				\begin{itemize}
					\item Attribute $A$ with $v$ distinct values.
					\item Expected information required to classify tuple in $D$ based on partitioning by $A$.
					\item $D_A$: dataset $D$ partitioned by $A$, \\
					      $D_{A,j}$: dataset $D$ partitioned by $A$ with distinct value $j$.
				\end{itemize}
			\end{column}
		}
		\visible<3->{
			\begin{column}{0.5\textwidth}
				\begin{center}
					\textbf{Continuous-valued Attribute}
				\end{center}
				\vspace*{-1em}

				\begin{itemize}
					\item Attribute $A$ with $v$ distinct values.
					\item Order values in increasing order.
					\item Calculate midpoint of every neighbouring value.
					\item $v-1$ possible split points $s_i=\frac{a_i + a_{i+1}}{2}$.
					\item Evaluate $\text{Info}_A(D)$ for every possible binary splitting ($A\leq s_i$ and  $A>s_i$).
				\end{itemize}

			\end{column}
		}
	\end{columns}

	\vspace*{0.25cm}

	\begin{columns}
		\visible<2->{
			\begin{column}{0.5\textwidth}
				{
					\footnotesize
					\begin{align*}
						\text{Info}_A(D) = \sum_{j=1}^v \frac{|D_{A,j}|}{|D_A|} \text{Info}(D_{A,j})
					\end{align*}
				}
			\end{column}
		}
		\visible<3->{
			\begin{column}{0.5\textwidth}
				{
					\footnotesize
					\begin{align*}
						\text{Info}_A(D) = \frac{|D_{A\leq s_i}|}{|D_A|} \text{Info}(D_{A\leq s_i}) + \frac{|D_{A>s_i}|}{|D_A|} \text{Info}(D_{A>s_i})
					\end{align*}
				}
			\end{column}
		}
	\end{columns}

\end{frame}

\begin{frame}{Information Gain (ID3) (III)}
	Given $\text{Info}(D)$ and $\text{Info}_A(D)$, Information Gain of each attribute $A$ is calculated as:
	\begin{align*}
		\text{Gain}(A) = \text{Info}(D) - \text{Info}_A(D)
	\end{align*}

	\vspace*{0.75cm}

	\begin{block}{Information Gain}
		The attribute with the \textbf{highest} Information Gain is selected as the splitting attribute.
	\end{block}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (I)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Target attribute:} buys\_computer \medskip
				\item \textbf{Expected Information}:
			\end{itemize}
			{
			\footnotesize
			\begin{align*}
				\text{Info}(D) & = I(9,5)                                                               \\
				               & = - \frac{9}{14}\log_2(\frac{9}{14})-\frac{5}{14} \log_2(\frac{5}{14}) \\
				               & = 0.94                                                                 \\
			\end{align*}
			}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (II)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{itemize}
				\item \textbf{Attribute:} Age \medskip
				\item \textbf{Value distribution:}
				      \vspace*{0.1cm}
				      \begin{center}
					      \scalebox{0.8}{
						      \begin{tabular}{|c|c|c|c|}
							      \hline
							      \cellcolor{faugray!62}Age     & \cellcolor{faugray!62}Yes & \cellcolor{faugray!62}No & \cellcolor{faugray!62}$I(\text{Yes},\text{No})$ \\\hline
							      \cellcolor{white}$\leq 30$    & 2                         & 3                        & 0.971                                           \\\hline
							      \cellcolor{white}$31\ldots40$ & 4                         & 0                        & 0                                               \\\hline
							      \cellcolor{white}$>40$        & 3                         & 2                        & 0.971                                           \\\hline
						      \end{tabular}
					      }\medskip
				      \end{center}
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Info}_{\texttt{age}}(D) & = \frac{5}{14}I(2,3) + \frac{4}{14} I(4,0) + \frac{5}{14} I(3,2) \\
						                                    & = 0.694                                                          \\
						      \text{Gain}(\texttt{age})     & = \text{Info}(D)-\text{Info}_{\texttt{age}}(D)                   \\
						                                    & = 0.94 - 0.694                                                   \\
						                                    & = 0.246
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\only<1>{
				\vspace*{-0.5cm}
				\begin{center}
					\scalebox{0.7}{
						\input{7-classification/table-example.tex}
					}
				\end{center}
			}
			\only<2->{
				\vspace*{-0.5cm}
				\begin{itemize}
					\item \textbf{Gain of other attributes:}
					      \begin{itemize}
						      \item $\text{Gain}(\texttt{income}) = 0.029$,
						      \item $\text{Gain}(\texttt{student}) = 0.151$,
						      \item $\text{Gain}(\texttt{credit\_rating}) = 0.048$.
					      \end{itemize} \medskip
					      \visible<3->{
					\item \textbf{Splitting attribute:} \\
					      Age  with $\text{Gain}(\texttt{age}) = 0.246$.
					      }
				\end{itemize}
			}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Information Gain (ID3) - Example (III)}
	\centering

	\begin{tikzpicture}[
			overlay,
			remember picture,
			>=latex,
			thick,
			node/.style={
					draw=faugray,
					rounded corners=.25em,
					fill=faugray!62,
					text depth=0.2em
				},
			leaf/.style={
					draw,
					rounded corners=.7em,
					text depth=0.2em
				},
			branch/.style={
					fill=white,
					font=\ttfamily\scriptsize,
					rounded corners=.7em,
					text depth=0.2em
				}
		]
		\node[node] at (0,0) (age) {age?};

		\node[draw=none,below left=3em and 1.5em of age,text width=3.8em] (student) {};
		\node[draw=none,below right=3em and .6em of age,text width=6em] (credit-rating) {};

		\draw[rounded corners=5pt]
		(age.south) -- ($(age.south) + (0,-1em)$) --
		($(student.north) + (0,2em)$) -- (student.north);
		\node[branch,above=2em of student.north] {$\leq$30};

		\path[draw,rounded corners=5pt]
		(age.south) -- ($(age.south) + (0,-1em)$) --
		($(credit-rating.north) + (0,2em)$) -- (credit-rating.north);
		\node[branch,above=2em of credit-rating.north] {>40};
		\node at (-4,-2.4) (t1) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faured}no}                 \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faured}no}                 \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
				\end{tabular}
			}
		};
		\node[below=10.5em of age] (t3) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}high        & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}high        & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
				\end{tabular}
			}
		};
		\draw (age.south) -- (t3.north);
		\node[branch,below=3em of age.north] {31\dots 40};
		\node at (4,-2.4) (t2) {
			\resizebox{6.5cm}{!}{%
				\begin{tabular}{|l|l|l|l|}
					\hline
					\cellcolor{faugray!62}income & \cellcolor{faugray!62}student & \cellcolor{faugray!62}credit\_rating & \cellcolor{brown!20}buys\_computer \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}low         & \cellcolor{white}yes          & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
					\cellcolor{white}medium      & \cellcolor{white}yes          & \cellcolor{white}fair                & {\color{faugreen}yes}              \\\hline
					\cellcolor{white}medium      & \cellcolor{white}no           & \cellcolor{white}excellent           & {\color{faured}no}                 \\\hline
				\end{tabular}
			}
		};
	\end{tikzpicture}
\end{frame}

\subsection{Gain Ratio (C4.5)}

\begin{frame}{Gain Ratio (C4.5)}
	\begin{itemize}
		\item \textbf{Problem of Information Gain:}
		      \begin{itemize}
			      \item Tends to prefer attributes with large number of distinct values.
			            \begin{itemize}
				            \item E. g. attribute \texttt{degree\_program} with 278 values vs. \texttt{student\_status} with 2 values.
				                  % 278 is the number of all degree programs at FAU in WS2024/25
			            \end{itemize}
		      \end{itemize}
		      \visible<2->{
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Normalize the Information Gain to get the \textbf{Gain Ratio (C4.5)}:
			            \begin{align*}
				            \text{SplitInfo}_A(D) & = - \sum_{j=1}^{v} \frac{|D_j|}{|D|} \log_2\left( \frac{|D_j|}{|D|} \right) \\
				            \text{GainRatio}(A)   & = \frac{\text{Gain}(A)}{\text{SplitInfo}_A(D)}
			            \end{align*}
			      \item \textbf{Disadvantage:} Becomes unstable as $\text{SplitInfo}_A(D)$ approaches zero.

			            \begin{block}{Gain Ratio}
				            The attribute with the \textbf{highest} Gain Ratio is selected as the splitting attribute.
			            \end{block}

		      \end{itemize}
		      }
	\end{itemize}
\end{frame}

\begin{frame}{Gain Ratio (C4.5) - Example}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5em}
			\begin{itemize}
				\item \textbf{Attribute:} Age \medskip
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Gain}(\texttt{age})  =     & 0.246                                                                                        \\
						      \text{SplitInfo}_{age}(D)  =     & -\frac{5}{14} \log_2\left(\frac{5}{14}\right) - \frac{4}{14} \log_2\left(\frac{4}{14}\right) \\
						                                       & - \frac{5}{14} \log_2\left(\frac{5}{14}\right)                                               \\
						      =                                & 1.577                                                                                        \\
						      \text{GainRatio}(\texttt{age}) = & \frac{0.246}{1.577}                                                                          \\
						      =                                & 0.156
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\subsection{Gini Index (CART)}

\begin{frame}{Gini Index (CART) (I)}
	\begin{itemize}
		\item \textbf{Problem of Information Gain and Gain Ratio:}
		      \begin{itemize}
			      \item Use of logarithm is computationally expensive.
		      \end{itemize}
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Use the \textbf{Gini Index (CART)} as an alternative to Information Gain and Gain Ratio.
			      \item Measures the \textbf{statistical dispersion} of a dataset.
		      \end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}{Gini Index (CART) (II)}
	\begin{itemize}
		\item \textbf{Measured impurity} of partition $D$ is defined as the sum over $n$ classes:
		      \vspace*{-1em}
		      \begin{align*}
			      \text{Gini}(D) = 1-\sum_{j=1}^{n} p_j^2
		      \end{align*}
		      \vspace*{-1em}
		      \begin{itemize}
			      \item $p_j$ is the non-zero probability that sample in $D$ belongs to class $C_j$ as estimated by $\frac{|C_{j,D}|}{|D|}$
		      \end{itemize}
	\end{itemize}

	\vspace*{-1em}

	\visible<2->{
		\begin{columns}
			\visible<3->{
				\begin{column}{0.45\textwidth}
					\begin{center}
						\textbf{Discrete-valued Attribute}
					\end{center}
					\vspace*{-1em}
					\begin{itemize}
						\item Attribute $A$ with $v$ distinct values.
						\item Compute all possible subsets of values.
						\item Compute weighted sum of each partition tuple ($D_1$ and $D_2$) as follows:
					\end{itemize}
				\end{column}
			}
			\visible<4->{
				\begin{column}{0.45\textwidth}
					\begin{center}
						\textbf{Continuous-valued Attribute}
					\end{center}
					\vspace*{-1em}

					\begin{itemize}
						\item Order values in increasing order.
						\item Split the dataset at every midpoint.
						\item Evaluate $\text{Gini}_A(D)$ for every possible binary splitting:
					\end{itemize}

				\end{column}
			}
		\end{columns}

		\vspace*{0.25cm}

		\only<-4>{
			\begin{columns}
				\visible<3->{
					\begin{column}{0.5\textwidth}
						{
							\footnotesize
							\begin{align*}
								\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
							\end{align*}
						}
					\end{column}
				}
				\visible<4->{
					\begin{column}{0.5\textwidth}
						{
							\footnotesize
							\begin{align*}
								\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
							\end{align*}
						}
					\end{column}
				}
			\end{columns}
		}
		% EXTRA ANIMATION STEP:
		% Might be used to show that the formula is the same for both cases. But for now, we decided to keep it 
		% simple.
		%
		%\only<5->{
		%	{
		%			\footnotesize
		%			\begin{align*}
		%				\text{Gini}_A(D) = \frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
		%			\end{align*}
		%		}
		%}
	}
\end{frame}

\begin{frame}{Gini Index (CART) (III)}
	\begin{itemize}
		\item The \textbf{reduction in impurity} (the actual \textbf{Gini Index}) is defined as
		      follows:
		      \begin{align*}
			      \Delta\text{Gini}_A(D) = \text{Gini}(D)-\text{Gini}_A(D).
		      \end{align*}
	\end{itemize}

	\vspace*{4em}

	\begin{block}{Gini Index}
		The attribute (and partitioning) with the \textbf{highest} Gini Index is selected as the splitting attribute.
	\end{block}

\end{frame}

\begin{frame}{Gini Index (CART) - Example (I)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{itemize}
				\item \textbf{Target attribute:} buys\_computer \medskip
				\item \textbf{Measured impurity}:
			\end{itemize}
			{
			\footnotesize
			\begin{align*}
				\text{Gini}(D) & = 1 - \left( \frac{9}{14} \right)^2 - \left( \frac{5}{14} \right)^2 \\
				               & = 0.459
			\end{align*}
			}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-0.5cm}
			\begin{center}
				\scalebox{0.7}{
					\input{7-classification/table-example.tex}
				}
			\end{center}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Gini Index (CART) - Example (II)}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\vspace*{-2em}
			\begin{itemize}
				\item \textbf{Attribute:} Income \medskip
				\item \textbf{Subsets:}
				      {
					      \footnotesize
					      \begin{align*}
						      D_1: & \{\texttt{low,medium}\} \\
						      D_2: & \{\texttt{high}\}
					      \end{align*}
				      }
				\item \textbf{Calculation:}
				      {
					      \footnotesize
					      \begin{align*}
						      \text{Gini}(D\vert_{\texttt{i}=\{high\}}) = & \text{Gini}(D\vert_{\texttt{i}=\{medium,low\}})                                             \\
						      =                                           & \frac{10}{14} \text{Gini}(D_1) + \frac{4}{14} \text{Gini}(D_2)                              \\
						      =                                           & \frac{10}{14} \left(1-\left( \frac{7}{10} \right)^2 - \left( \frac{3}{10} \right)^2 \right) \\
						      +                                           & \frac{4}{14} \left( 1-\left( \frac{2}{4} \right)^2 - \left( \frac{2}{4} \right)^2 \right)   \\
						      =                                           & 0.443
					      \end{align*}
				      }
			\end{itemize}


		\end{column}
		\begin{column}{0.45\textwidth}
			\only<1>{
				\vspace*{-0.5cm}
				\begin{center}
					\scalebox{0.7}{
						\input{7-classification/table-example.tex}
					}
				\end{center}
			}
			\only<2->{
				\vspace*{-0.5cm}
				\begin{itemize}
					\item \textbf{Reduction in impurity:}
					      {
						      \footnotesize
						      \begin{align*}
							      \Delta\text{Gini}_{\texttt{i}=\{high\}}(D) = & \text{Gini}(D)-\text{Gini}(D\vert_{\texttt{i}=\{high\}}) \\
							      =                                            & 0.459 - 0.443                                            \\
							      =                                            & 0.016
						      \end{align*}
					      }
					\item \textbf{Gini Index of other subsets:}
					      \begin{itemize}
						      \item $\Delta\text{Gini}_{\texttt{i}=\{medium\}}(D) = 0.001$
						      \item $\Delta\text{Gini}_{\texttt{i}=\{low\}}(D) = 0.009$
					      \end{itemize} \medskip
					      \visible<3->{
					\item \textbf{Splitting subset:} \\
					      \begin{itemize}
						      \item If income has the overall highest reduction of impurity, then the split is on \{"low","medium"\} and \{"high"\}.
					      \end{itemize}
					      }
				\end{itemize}
			}
		\end{column}
	\end{columns}
\end{frame}

\subsection*{Overview}

\begin{frame}{Attribute Selection Methods Overview}
	\textbf{The three methods, in general, return good results, but}
	\begin{itemize}
		\item \textbf{\color{airforceblue}Information gain:}
		      \begin{itemize}
			      \item Biased towards multi-valued attributes.
		      \end{itemize}
		\item \textbf{\color{airforceblue}Gain ratio:}
		      \begin{itemize}
			      \item Tends to prefer unbalanced splits in which one partition is much smaller than the others.
		      \end{itemize}
		\item \textbf{\color{airforceblue}Gini index:}
		      \begin{itemize}
			      \item Biased to multi-valued attributes.
			      \item Has difficulty when number of classes is large.
			      \item Tends to favor tests that result in equal-sized partitions and purity in both partitions.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Overfitting and Tree Pruning}
	\begin{itemize}
		\item \textbf{Overfitting: An induced tree may overfit the training data.}
		      \begin{itemize}
			      \item Too many branches, some may reflect anomalies due to noise or outliers.
			      \item Poor accuracy for unseen samples.
		      \end{itemize}
		\item Pruned trees are typically smaller, less complex, easier to
		      comprehend, faster and better at classifying unseen data.
		\item \textbf{Two approaches to avoid overfitting:}
		      \begin{enumerate}
			      \item \textbf{\color{airforceblue}Prepruning:}
			            \begin{itemize}
				            \item Halt tree construction early.\\
				                  Do not split a node, if this would result in the goodness measure falling below a threshold.
				            \item Difficult to choose an appropriate threshold.
			            \end{itemize}
			      \item \textbf{\color{airforceblue}Postpruning:}
			            \begin{itemize}
				            \item Remove branches from a "fully grown" tree.\\
				                  Get a sequence of progressively pruned trees.
				            \item Use a set of data different from the training data to decide which is the "best pruned tree."
			            \end{itemize}
		      \end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Enhancements to Basic Decision Tree Induction}
	\begin{itemize}
		\item \textbf{Allow for} \textbf{\color{airforceblue}continuous-valued attributes.}
		      \begin{itemize}
			      \item Dynamically define new discrete-valued attributes that partition the values of continuous-valued attributes into a discrete set of intervals.
		      \end{itemize}
		\item \textbf{Handle} \textbf{\color{airforceblue}missing attribute values.}
		      \begin{itemize}
			      \item Assign the most common value of the attribute.
			      \item Assign probability to each of the possible values.
		      \end{itemize}
		\item \textbf{\color{airforceblue}Attribute construction.}
		      \begin{itemize}
			      \item Create new attributes based on existing ones that are sparsely represented.
			      \item This reduces fragmentation, repetition, and replication.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Classification in Large Databases}
	\begin{itemize}
		\item ID3, C4.5, and CART have been developed with the assumption that data fits into memory.\\With Big Data that's not possible anymore.
		\item \textbf{Scalability:}
		      \begin{itemize}
			      \item Classifying datasets with millions of examples and \\ hundreds of attributes with reasonable speed.
		      \end{itemize}
		\item \textbf{Why is decision tree induction popular?}
		      \begin{itemize}
			      \item Relatively fast learning speed (compared to other classification methods).
			      \item Convertible to simple and easy-to-understand classification rules.
			      \item Can use SQL queries for accessing databases.
			      \item Classification accuracy comparable with other methods.
		      \end{itemize}
		\item Two scalable methods, among others:
		      \begin{enumerate}
			      \item RainForest
			      \item BOAT
		      \end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Scalable Decision Tree: RainForest}
	\begin{itemize}
		\item Applicable to any decision tree algorithm.
		\item \textbf{Separates the scalability aspects from the criteria that determine the quality of the tree.}
		\item \textbf{Builds an} \textbf{\color{airforceblue}AVC-list:} (Attribute, Value, Class\_label).
		\item \textbf{\color{airforceblue}AVC-set} \textbf{(of an attribute X):}
		      \begin{itemize}
			      \item Projection of training dataset onto the attribute $X$ and class label where counts of individual class label are aggregated.
		      \end{itemize}
		\item \textbf{\color{airforceblue}AVC-group} \textbf{(of a node n):}
		      \begin{itemize}
			      \item Set of AVC-sets of all predictor attributes at the node $n$.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{RainForest: Training Set and its AVC-sets}
	\begin{columns}
		\begin{column}{0.6\textwidth}
			\small
			\input{7-classification/table-example.tex}
		\end{column}
		\begin{column}{0.3\textwidth}
			\vspace{-3cm}

			\centering
			AVC-set on age:\\
			\begin{tabular}{|c|c|c|}
				\hline
				age          & yes & no \\\hline
				$\leq 30$    & 2   & 3  \\\hline
				$31\ldots40$ & 4   & 0  \\\hline
				$>40$        & 3   & 2  \\\hline
			\end{tabular}\\[1cm]
			AVC-set on income:\\
			\begin{tabular}{|c|c|c|}
				\hline
				income & yes & no \\\hline
				high   & 2   & 2  \\\hline
				medium & 4   & 2  \\\hline
				low    & 3   & 1  \\\hline
			\end{tabular}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{RainForest: Training Set and its AVC-sets (II)}
	\begin{columns}
		\begin{column}{0.6\textwidth}
			\small
			\input{7-classification/table-example.tex}
		\end{column}
		\begin{column}{0.3\textwidth}
			\vspace{-3cm}

			\centering
			AVC-set on student:\\
			\begin{tabular}{|c|c|c|}
				\hline
				student & yes & no \\\hline
				yes     & 6   & 1  \\\hline
				no      & 3   & 4  \\\hline
			\end{tabular}\\[1cm]
			AVC-set on credit\_rating:\\
			\begin{tabular}{|c|c|c|}
				\hline
				credit\_rating & yes & no \\\hline
				fair           & 6   & 2  \\\hline
				excellent      & 3   & 3  \\\hline
			\end{tabular}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Scalable Decision Tree: BOAT}
	\begin{itemize}
		\item BOAT = Bootstrapped Optimistic Algorithm for Tree Construction
		\item \textbf{Use a statistical technique called bootstrapping to create several smaller samples (subsets), each fitting in memory.}
		      \begin{itemize}
			      \item See on the subsequent slides.
		      \end{itemize}
		\item \textbf{Each subset is used to create a tree, resulting in several trees.}
		\item \textbf{These trees are examined and used to construct a new tree T'.}
		      \begin{itemize}
			      \item It turns out that T' is very close to the tree that would be generated \\
			            using the whole data set together.
		      \end{itemize}
		\item \textbf{Advantages:}
		      \begin{itemize}
			      \item Requires only two scans of DB.
			      \item An incremental algorithm:
			            \begin{itemize}
				            \item Take insertions and deletions of training data and update the decision tree.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}
