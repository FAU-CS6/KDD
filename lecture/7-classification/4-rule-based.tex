\section{Rule-Based Classification}

\begin{frame}{Using \uppercase{if-then} Rules for Classification}
	\begin{itemize}
		\item \textbf{Represent the knowledge in the form of {\color{airforceblue}IF-THEN rules}.}
		      \begin{itemize}
			      \item E.g., if \texttt{age} $\leq 30$ AND \texttt{student} = "yes" THEN buys\_computer = "yes".
			      \item Readable.
		      \end{itemize}
		\item \textbf{Rule {\color{airforceblue}antecedent/precondition} vs. rule {\color{airforceblue}consequent}}.
		\item \textbf{Assessment of a rule R: coverage and accuracy.}
		      \begin{itemize}
			      \item $n_{\text{covers}} = \#$ of tuples covered by $R$ (antecedent if true).
			      \item $n_{\text{correct}} = \#$ of tuples correctly classified by $R$.
			      \item $\text{coverage}(R) = \frac{n_{\text{covers}}}{|D|}$ with $D$ training data set.
			      \item $\text{accuracy}(R) = \frac{n_{\text{correct}}}{n_{\text{covers}}}$.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[c]{Potential Problems of Rule-Based Classification}
	\centering
	\huge
	\begin{enumerate}
		\item More than one rule is triggered.
		\item No rule is triggered.
	\end{enumerate}
\end{frame}

\begin{frame}{Potential Solutions}
	\begin{enumerate}
		\item \textbf{More than one rule is triggered: {\color{airforceblue}conflict resolution}.}
		      \begin{itemize}
			      \item \textbf{\color{airforceblue}Size ordering:}
			            \begin{itemize}
				            \item Assign the highest priority to the triggered rule that has the "toughest" requirement \\ (i.e., rule with most used attribute in condition).
			            \end{itemize}
			      \item \textbf{\color{airforceblue}Class-based ordering:}
			            \begin{itemize}
				            \item Decreasing order of prevalence or misclassification cost per class.
				            \item No order of rules within class $\rightarrow$ disjunction (logical \texttt{OR}) between rules.
			            \end{itemize}
			      \item \textbf{\color{airforceblue}Rule-based ordering} (decision list):
			            \begin{itemize}
				            \item Rules are organized into one long priority list,\\
				                  according to some measure of rule quality, or by experts.
				            \item Rules \underline{must} be applied in this particular order to avoid conflict.
			            \end{itemize}
		      \end{itemize}
		\item \textbf{No rule is triggered.}
		      \begin{itemize}
			      \item Use a fallback/default rule.
			      \item Always evaluated as the last rule, if and only if other rules are not covered by some tuple, i. e. no rules have been triggered.
		      \end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}{Rule Extraction from a Decision Tree}
	\begin{itemize}
		\item \textbf{Rules are {\color{airforceblue}easier to understand} than large trees.}
		\item \textbf{Rule can be created for {\color{airforceblue}each path from the root to a leaf.}}
		      \begin{itemize}
			      \item The leaf holds the class prediction.
		      \end{itemize}
		\item \textbf{Each attribute-value pair along the path forms a conjunction:}
	\end{itemize}
	\vspace*{1em}
	\begin{columns}
		\begin{column}{0.55\textwidth}
			\textbf{Example:}
			\begin{enumerate}
				\item IF \texttt{age} $\leq$ 30 AND \texttt{student} = "no" \\
				      THEN \texttt{buys\_computer} = "no".
				\item IF \texttt{age} $\leq$ 30 AND \texttt{student} = "yes" \\
				      THEN \texttt{buys\_computer} = "yes".
				\item IF \texttt{age}== $31\ldots40$ THEN \texttt{buys\_computer} = "yes".
				\item \dots
			\end{enumerate}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace*{-1em}
			\begin{figure}[h]
				\centering
				\input{7-classification/tree-example.tex}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Rule Induction: Sequential Covering Method}
	\begin{itemize}
		\item \textbf{Sequential covering algorithm:}
		      \begin{itemize}
			      \item Extracts rules directly from training data.
		      \end{itemize}
		\item \textbf{Typical sequential covering algorithms:}
		      \begin{itemize}
			      \item FOIL, AQ, CN2, RIPPER.
		      \end{itemize}
		\item \textbf{Rules are learned {\color{airforceblue}sequentially}.}
		      \begin{itemize}
			      \item Each rule for a given class $C_i$ will cover many tuples of $C_i$, but none (or few) of the tuples of other classes.
		      \end{itemize}
		\item \textbf{Algorithm sketch:}
		      \begin{itemize}
			      \item Rules are learned one at a time.
			      \item Each time a rule is learned, the tuples covered by the rule are removed.
			      \item The process repeats on the remaining tuples unless termination condition, e.g., when no more training examples left or when the quality of a rule returned is below a user-specified threshold.
		      \end{itemize}
		\item \textbf{Compare with decision-tree induction:}
		      \begin{itemize}
			      \item That was learning a set of rules simultaneously.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Sequential Covering Algorithm (I)}
	\begin{algorithm}[H]
		\SetAlgoVlined
		\KwData{
			\begin{itemize}
				\item Training dataset $D$ containing tuples with their associated class labels;
				\item \texttt{attribute\_values}, the set of all attributes and their possible values;
			\end{itemize}
		}
		\KwResult{A rule set.}
		\BlankLine
		\texttt{rule\_set} $\leftarrow \{\}$ \tcp*[l]{Initial set of rules learned is empty}
		\BlankLine
		\ForEach{class $c$ of $D$}{
			\Repeat{terminating condition}{
				\texttt{rule} $\leftarrow$ \texttt{learn\_one\_rule}($D$, \texttt{attribute\_values, $c$})\;
				remove tuples covered by \texttt{rule} from $D$\;
				\BlankLine
				\texttt{rule\_set} $\leftarrow$ \texttt{rule\_set} + \texttt{rule} \tcp*[l]{add new rule to rule set}
			}
		}
		\BlankLine
		\KwRet{
			\texttt{rule\_set}\;
		}
	\end{algorithm}
\end{frame}

\begin{frame}{Sequential Covering Algorithm (II)}
	\begin{itemize}
		\item Rules are learned in a \textit{general-to-specific} manner
		\item Start with the most general rule possible: a rule with an empty condition
		      \begin{center}
			      IF THEN \texttt{buys\_computer} = "yes"
		      \end{center}
		\item Then: Consider each possible attribute (\texttt{attribute\_values})\newline
		      For instance: attribute-value pair (\texttt{att}, \texttt{val}); consider the following attribute tests:\newline
		      \texttt{att} = \texttt{val}, \texttt{att} $<$ \texttt{val}, \texttt{att} $\leq$ \texttt{val}, \texttt{att} > \texttt{val}, \texttt{att} $\geq$ \texttt{val}
		\item \textbf{Curse of dimensionality:} testing each attribute-value pair is computationally explosive
		\item Solution: \textbf{greedy depth-first strategy of \texttt{learn\_one\_rule}}
		      \begin{itemize}
			      \item Add new attribute test that improves the rule quality the most.
			      \item Each time a new attribute test is added, the rule should cover more "accept" tuples (\texttt{buys\_computer} = "yes").
			      \item Repeat until a certain acceptable quality level is reached (\textit{terminating condition}).
		      \end{itemize}
		\item \textbf{What if we added a poor choice?} Greedy search does not allow for backtracking.
		      \begin{itemize}
			      \item Retain the best $k$ attribute candidates at each step, rather than a single best candidate.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Sequential Covering Algorithm (III)}
	\vspace*{-1em}
	\begin{figure}[h]
		\centering
		\includegraphics[height=0.8\textheight]{img/7-classification_rule_space.png}
	\end{figure}
\end{frame}

\begin{frame}{Sequential Covering Algorithm (IV)}
	\begin{itemize}
		\item \texttt{learn\_one\_rule} requires a measure of rule quality.
		\item Accuracy and coverage seems obvious choices on their own, but
		      individually not enough.
		\item \textbf{FOIL} (First-Order Inductive Learner): based on information gain
		      \begin{itemize}
			      \item Suppose we have two rules:
			            \begin{align*}
				            \label{}
				            R  & : \text{IF condition THEN class} = c  \\
				            R' & : \text{IF condition' THEN class} = c
			            \end{align*}
			      \item $pos/neg$ are $\#$ of positive/negative tuples covered by $R$, $pos'/neg'$ respectively for $R'$.
			      \item FOIL assesses the information gained by extending \textit{condition'} as
			            \begin{align*}
				            \text{FOIL\_Gain} = \text{pos}' \left( \log_2 \frac{\text{pos}'}{\text{pos}' + \text{neg}'} - \log_2 \frac{\text{pos}}{\text{pos}+\text{neg}} \right).
			            \end{align*}
			      \item FOIL favors rules that have high accuracy and cover many positive tuples.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Rule Pruning}
	\begin{itemize}
		\item \textbf{Danger of {\color{airforceblue}overfitting}.}
		\item \textbf{Removing a conjunct (attribute test),}
		      \begin{itemize}
			      \item if pruned version of rule has greater quality,\\
			            assessed on an independent set of test tuples (called "pruning set").
		      \end{itemize}
		\item \textbf{FOIL uses:}
		      \begin{align*}
			      \text{FOIL\_Prune}(R) = \frac{\text{pos}-\text{neg}}{\text{pos}+\text{neg}}.
		      \end{align*}
		\item If $\text{FOIL\_Prune}$ is higher for the pruned version of $R$, prune $R$.
	\end{itemize}
\end{frame}
