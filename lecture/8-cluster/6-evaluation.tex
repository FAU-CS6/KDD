\section{Evaluation of clustering}

\begin{frame}{Assessing Clustering Tendency}
	\begin{itemize}
		\item \textbf{Assess {\color{airforceblue}if non-random structure}
			      exists in the data by measuring the probability that the data is
			      generated by a uniform data distribution.}
		\item \textbf{Data with random structure:}
		      \begin{itemize}
			      \item Points uniformly distributed in data space.
		      \end{itemize}
		\item \textbf{Clustering may return clusters, but:}
		      \begin{itemize}
			      \item Artificial partitioning.
			      \item Meaningless.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Hopkins Statistic (I)}
	\begin{itemize}
		\item \textbf{Data set:}
		      \begin{itemize}
			      \item Let $X = \{x_i \; \vert i=1,\ldots,n\}$ be a collection of n
			            patterns in a $d$-dimensional space such that $x_i = (x_{i1},
				            x_{i2}, \ldots, x_{id})$.
		      \end{itemize}
		\item \textbf{Random sample of data space:}
		      \begin{itemize}
			      \item Let $Y = \{y_j \; \vert \; j=1, \ldots, m\}$ be $m$ sampling
			            points placed at random in the $d$-dimensional space, with $m \ll
				            n$.
		      \end{itemize}
		\item \textbf{Two types of distances defined:}
		      \begin{itemize}
			      \item $u_j$ as minimum distance from $y_j$ to its nearest pattern
			            in $X$ and
			      \item $w_j$ as minimum distance from a randomly selected pattern in
			            $X$ to its nearest neighbor in $X$.
		      \end{itemize}
		\item \textbf{The {\color{airforceblue}Hopkins} statistic in d
			      dimensions is defined as:}
		      \begin{align*}
			      H = \frac{\sum_{j=1}^{m} u_j}{\sum_{j=1}^{m}u_j + \sum_{j=1}^{m}
				      w_j}.
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Hopkins Statistic (II)}
	\begin{itemize}
		\item \textbf{Compares nearest-neighbor distribution of randomly
			      selected locations (points) to that for randomly selected patterns.}
		\item \textbf{Under the null hypothesis, $H_0$, of uniform
			      distribution:}
		      \begin{itemize}
			      \item Distances from sampling points to nearest patterns should, on
			            the average,\\ \textbf{\color{airforceblue}be the same} as the
			            interpattern nearest-neighbor distances, implying randomness.
			      \item $H$ should be about $0.5$.
		      \end{itemize}
		\item \textbf{When patterns are aggregated or clustered:}
		      \begin{itemize}
			      \item Distances from sampling points to nearest patterns should, on
			            the average,\\ \textbf{\color{airforceblue}larger} as the
			            interpattern nearest-neighbor distances.
			      \item $H$ should be larger than $0.5$.
			      \item Almost equal to 1.0 for very well clustered data.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Determine the Number of Clusters}
	\begin{itemize}
		\item \textbf{Empirical method:}
		      \begin{itemize}
			      \item $\#$ of clusters $\approx \sqrt{\frac{n}{2}}$ for a dataset
			            of $n$ points.
		      \end{itemize}
		\item \textbf{Elbow method:}
		      \begin{itemize}
			      \item Use the turning point in the curve of sum of within-cluster
			            variance w.r.t. the $\#$ of clusters.
		      \end{itemize}
		\item \textbf{Cross-validation method:}
		      \begin{itemize}
			      \item Divide a given data set into $m$ parts.
			      \item Use $m-1$ parts to obtain a clustering model.
			      \item Use the remaining part to test the quality of the clustering.

			            E.g., for each point in the test set, find the closest centroid,
			            and use the sum of squared distances between all points in the
			            test set and the closest centroids to measure how well the model
			            fits the test set.
			      \item For any $k > 0$, repeat it $m$ times, compare the overall
			            quality measure w.r.t. different $k$'s, and find $\#$ of clusters
			            that fits the data the best.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Measuring Clustering Quality}
	\textbf{Two methods:}
	\begin{enumerate}
		\item \textbf{{\color{airforceblue}Extrinsic}: supervised, i.e.,
			      the ground truth is available.}
		      \begin{itemize}
			      \item Compare a clustering against the ground truth using
			            certain clustering quality measure.
			            \begin{itemize}
				            \item Ex. BCubed precision and recall metrics.
			            \end{itemize}
		      \end{itemize}
		\item \textbf{{\color{airforceblue}Intrinsic}: unsupervised, i.e.,
			      the ground truth is unavailable.}
		      \begin{itemize}
			      \item Evaluate the goodness of a clustering by considering how
			            well the clusters are separated, and how compact the clusters
			            are.
			            \begin{itemize}
				            \item Ex. silhouette coefficient.
			            \end{itemize}
		      \end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}{Measuring Clustering Quality: Extrinsic Methods}
	\begin{itemize}
		\item \textbf{Clustering-quality measure: $Q(C,C_*)$.}
		      \begin{itemize}
			      \item For a clustering $C$ given the ground truth $C_*$.
		      \end{itemize}
		\item \textbf{$Q$ is good, if it satisfies the following four essential
			      criteria:}
		      \begin{itemize}
			      \item Cluster homogeneity:
			            \begin{itemize}
				            \item The purer, the better.
			            \end{itemize}
			      \item Cluster completeness:
			            \begin{itemize}
				            \item Should assign objects that belong to the same category \\
				                  in the ground truth to the same cluster.
			            \end{itemize}
			      \item Rag bag:
			            \begin{itemize}
				            \item Putting a heterogeneous object into a pure cluster should
				                  be penalized more than putting it into a rag bag (i.e.,
				                  "miscellaneous" or "other" category).
			            \end{itemize}
			      \item Small cluster preservation:
			            \begin{itemize}
				            \item Splitting a small category into pieces is more harmful
				                  than \\
				                  splitting a large category into pieces.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}
