\section{Evaluation of clustering}

\begin{frame}{Assessing Clustering Tendency}
	\begin{itemize}
		\item \textbf{Assess {\color{airforceblue}if non-random structure}
		exists in the data by measuring the probability that the data is
		generated by a uniform data distribution.}
		\item \textbf{Data with random structure:}
		\begin{itemize}
			\item Points uniformly distributed in data space.
		\end{itemize}
		\item \textbf{Clustering may return clusters, but:}
		\begin{itemize}
			\item Artificial partitioning.
			\item Meaningless.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Hopkins Statistic (I)}
	\begin{itemize}
		\item \textbf{Data set:}
		\begin{itemize}
			\item Let $X = \{x_i \; \vert i=1,\ldots,n\}$ be a collection of n
			patterns in a $d$-dimensional space such that $x_i = (x_{i1},
			x_{i2}, \ldots, x_{id})$.
		\end{itemize}
		\item \textbf{Random sample of data space:}
		\begin{itemize}
			\item Let $Y = \{y_j \; \vert \; j=1, \ldots, m\}$ be $m$ sampling
			points placed at random in the $d$-dimensional space, with $m \ll
			n$.
		\end{itemize}
		\item \textbf{Two types of distances defined:}
		\begin{itemize}
			\item $u_j$ as minimum distance from $y_j$ to its nearest pattern
			in $X$ and
			\item $w_j$ as minimum distance from a randomly selected pattern in
			$X$ to its nearest neighbor in $X$.
		\end{itemize}
		\item \textbf{The {\color{airforceblue}Hopkins} statistic in d
		dimensions is defined as:}
		\begin{align}
			H = \frac{\sum_{j=1}^{m} u_j}{\sum_{j=1}^{m}u_j + \sum_{j=1}^{m}
			w_j}.
		\end{align}
	\end{itemize}
\end{frame}

\begin{frame}{Hopkins Statistic (II)}
	\begin{itemize}
		\item \textbf{Compares nearest-neighbor distribution of randomly
		selected locations (points) to that for randomly selected patterns.}
		\item \textbf{Under the null hypothesis, $H_0$, of uniform
		distribution:}
		\begin{itemize}
			\item Distances from sampling points to nearest patterns should, on
			the average,\\ \textbf{\color{airforceblue}be the same} as the
			interpattern nearest-neighbor distances, implying randomness.
			\item $H$ should be about $0.5$.
		\end{itemize}
		\item \textbf{When patterns are aggregated or clustered:}
		\begin{itemize}
			\item Distances from sampling points to nearest patterns should, on
			the average,\\ \textbf{\color{airforceblue}larger} as the
			interpattern nearest-neighbor distances.
			\item $H$ should be larger than $0.5$.
			\item Almost equal to 1.0 for very well clustered data.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Determine the Number of Clusters}
	\begin{itemize}
		\item \textbf{Empirical method:}
		\begin{itemize}
			\item $\#$ of clusters $\approx \sqrt{\frac{n}{2}}$ for a dataset
			of $n$ points.
		\end{itemize}
		\item \textbf{Elbow method:}
		\begin{itemize}
			\item Use the turning point in the curve of sum of within-cluster
			variance w.r.t. the $\#$ of clusters.
		\end{itemize}
		\item \textbf{Cross-validation method:}
		\begin{itemize}
			\item Divide a given data set into $m$ parts.
			\item Use $m-1$ parts to obtain a clustering model.
			\item Use the remaining part to test the quality of the clustering.

              E.g., for each point in the test set, find the closest centroid,
              and use the sum of squared distances between all points in the
              test set and the closest centroids to measure how well the model
              fits the test set.
			\item For any $k > 0$, repeat it $m$ times, compare the overall
			quality measure w.r.t. different $k$'s, and find $\#$ of clusters
			that fits the data the best.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Measuring Clustering Quality}
  \textbf{Two methods:}
  \begin{enumerate}
  \item \textbf{{\color{airforceblue}Extrinsic}: supervised, i.e.,
      the ground truth is available.}
    \begin{itemize}
    \item Compare a clustering against the ground truth using
      certain clustering quality measure.
      \begin{itemize}
      \item Ex. BCubed precision and recall metrics.
      \end{itemize}
    \end{itemize}
  \item \textbf{{\color{airforceblue}Intrinsic}: unsupervised, i.e.,
      the ground truth is unavailable.}
    \begin{itemize}
    \item Evaluate the goodness of a clustering by considering how
      well the clusters are separated, and how compact the clusters
      are.
      \begin{itemize}
      \item Ex. silhouette coefficient.
      \end{itemize}
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}{Measuring Clustering Quality: Extrinsic Methods}
	\begin{itemize}
		\item \textbf{Clustering-quality measure: $Q(C,C_*)$.}
		\begin{itemize}
			\item For a clustering $C$ given the ground truth $C_*$.
		\end{itemize}
		\item \textbf{$Q$ is good, if it satisfies the following four essential
		criteria:}
		\begin{itemize}
			\item Cluster homogeneity:
			\begin{itemize}
				\item The purer, the better.
			\end{itemize}
			\item Cluster completeness:
			\begin{itemize}
				\item Should assign objects that belong to the same category \\
				in the ground truth to the same cluster.
			\end{itemize}
			\item Rag bag:
			\begin{itemize}
				\item Putting a heterogeneous object into a pure cluster should
				be penalized more than putting it into a rag bag (i.e.,
				"miscellaneous" or "other" category).
			\end{itemize}
			\item Small cluster preservation:
			\begin{itemize}
				\item Splitting a small category into pieces is more harmful
				than \\
				splitting a large category into pieces.
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{frame}