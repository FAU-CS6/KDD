\section{Measuring Data Similarity and Dissimilarity}

\begin{frame}{Motivation}
	Similarity and Dissimilarity is inherently important for applications like
	\begin{itemize}
		\item \textbf{Nearest-Neighbor Classification} (c.f. lecture 7).

		      Assign class label to similar objects.

		      Example: spam classification or patient diagnosis.
		\item \textbf{Clustering} (c.f. lecture 8).

		      Cluster customers based on similar properties such as income, age, area of residence).

		      Useful for \textit{marketing} campaigns.

		\item \textbf{Outlier Analysis} (c.f. lecture 9).

		      Cluster data objects to identify outliers.
	\end{itemize}

	\begin{block}{Cluster}
		A \textit{cluster} subsumes data objects that are \underline{similar} to
		each other yet \underline{dissimilar} to data objects of other clusters.
	\end{block}

\end{frame}


\begin{frame}{Similarity and Dissimilarity}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\textbf{Similarity}
			\begin{itemize}
				\item Numerical measure of how alike two data objects are.
				\item Value is higher when objects are more alike.
				\item Often chosen within the range of $[0,1]$.
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\textbf{Dissimilarity}
			\begin{itemize}
				\item E.g. distance.
				\item Numerical measure of how different two data objects are.
				\item Lower when objects are more alike.
				\item Minimum dissimilarity is often $0$.
				\item Upper limit varies.
			\end{itemize}
		\end{column}
	\end{columns}

	\begin{alertblock}{Proximity}
		\textit{Proximity} can refer to similarity or dissimilarity.
	\end{alertblock}
\end{frame}

\begin{frame}{Data Matrices and Dissimilarity Matrices}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\textbf{Data Matrix}

			\begin{itemize}
				\item Stores data objects.
				\item Also called \textit{object-by-attribute structure}.
				\item Each data object is described by $m$ attributes.
				\item Having $n$ data objects we have a $n\times m$ data matrix.
			\end{itemize}
			\vspace*{1em}
			\begin{center}
				{$\begin{Bmatrix}
							x_{11} & x_{12} & \cdots & x_{1m} \\
							x_{21} & x_{22} & \cdots & x_{2m} \\
							\vdots & \vdots & \ddots & \vdots \\
							x_{n1} & x_{n2} & \cdots & x_{nm}
						\end{Bmatrix}$ }
			\end{center}
		\end{column}

		\begin{column}{0.5\textwidth}
			\textbf{Dissimilarity Matrix}

			\begin{itemize}
				\item Stores dissimilarity values of data object pairs.
				\item Also called \textit{object-by-object structure}.
			\end{itemize}
			\begin{center}
				{$\begin{Bmatrix}
							0              & 0              & 0              & \cdots & 0      \\
							d(x_{1},x_{2}) & 0              & 0              & \cdots & 0      \\
							d(x_{1},x_{3}) & d(x_{2},x_{3}) & 0              & \cdots & 0      \\
							\vdots         & \vdots         & \vdots         & \ddots & \vdots \\
							d(x_{1},x_{m}) & d(x_{2},x_{m}) & d(x_{3},x_{m}) & \cdots & 0
						\end{Bmatrix}$}
			\end{center}
			with $d(i, j)$ as the measured dissimilarity between two objects $i$ and $j$.

			\textit{Similarity} expressed with $\text{sim}(i,j) = 1 - d(i, j)$.
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Proximity Measures for Nominal Attributes}
	Recall: Nominal attributes can take two or more states and values do not follow an order.
	\begin{itemize}
		\item Values can be the same (distance of $0$) or different (distance of $1$).
		\item More options for sets of nominal attributes (variables).
		      \begin{enumerate}
			      \item \textbf{Simple Matching Coefficient} (SMC).
			            \begin{align*}
				            \text{SMC} = \frac{\# \text{of matching attributes}}{\# \text{number of attributes}}
			            \end{align*}
			      \item \textbf{One-Hot Encoding.}

			            Convert nominal attributes to binary attributes,\\ i.e. create one
			            binary attribute for every unique nominal value.
		      \end{enumerate}
	\end{itemize}

	\vspace*{1em}
	\begin{block}{Proximity Measure for Ordinal Attributes}
		Perform \textbf{Integer Encoding}, where every unique value is assigned an integer value.
	\end{block}
\end{frame}

\begin{frame}{Proximity Measure for Binary Attributes}
	\begin{itemize}
		\item Contingency table for binary data that counts matches.
		      \begin{center}
			      \vspace{-2em}
			      \[
				      \text{data object}~i\underbrace{
					      \left\{\begin{tabular}{ c | c c c }
						             & $1$   & $0$   & $\sum$    \\ \hline
						      $1$    & $q$   & $r$   & $q+r$     \\
						      $0$    & $s$   & $t$   & $s+t$     \\
						      $\sum$ & $q+s$ & $r+t$ & $q+r+s+t$
					      \end{tabular}\right.}_{\let\scriptstyle\textstyle\substack{\text{data object}~j}}
			      \]
		      \end{center}
		\item Distance measure for \textit{symmetrical} binary variables: $d(x,y) = \frac{r+s}{q+r+s+t}$
		      \note{Symmetrical binary values: each unique value is equally important.}
		\item Distance measure for \textit{asymmetrical} binary variables: $d(x,y) = \frac{r+s}{q+r+s}$
		      \note{Asymmetrical binary values: each unique value is not equally important. Example: Result of disease test.}
		\item Similarity for asymmetrical binary values is called \textit{Jaccard} coefficient and calculated as follows:
		      \begin{align*}
			      \text{sim}(i,j)=\text{jaccard}(i,j)=1-d(i,j)=\frac{q}{q+r+s}
		      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Dissimilarity Between Binary Variables}
	\begin{center}
		\begin{tabular}{c | c | c | c | c | c | c | c}
			\textbf{Name} & \textbf{Sex} & \textbf{Fever} & \textbf{Cough} & \textbf{Test-$1$} & \textbf{Test-$2$} & \textbf{Test-$3$} & \textbf{Test-$4$} \\\hline
			Bob           & M            & Y              & N              & P                 & N                 & N                 & N                 \\
			Alice         & F            & Y              & N              & P                 & N                 & P                 & N                 \\
			Charlie       & M            & Y              & P              & N                 & N                 & N                 & N                 \\
		\end{tabular}
	\end{center}

	\begin{alertblock}{Note}
		Attribute ``Sex'' is a symmetrical attribute (all values are of equal importance), whereas all remaining attributes are asymmetrical binary attributes.
	\end{alertblock}

	Let values $Y$ and $P$ be equal to $1$ and the value of $N$ be $0$, then we can compute:

	\vspace*{-2em}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{align*}
				d(\text{Bob}, \text{Alice}) = \frac{0+1}{2+0+1} \approx 0.33 \\
				d(\text{Bob}, \text{Charlie}) = \frac{1+1}{1+1+1} \approx 0.67
			\end{align*}
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{align*}
				d(\text{Charlie}, \text{Alice}) = \frac{1+2}{1+1+2} = 0.75
			\end{align*}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Standardizing Numerical Data}
	\begin{itemize}
		\item \textbf{$z$-Score:}
		      \begin{align*}
			      z = \frac{x-\mu}{\sigma}.
		      \end{align*}
		\item $x$ is the score to be standardized; $\mu$ is the population mean; $\sigma$ is the standard deviation.
		\item The distance between the raw score and the population mean in units of the standard deviation.
		\item Negative when the raw score is below the mean, positive else.

		      % Based on the timings in SS2024:
		      % There does not seem to be a lot of time time to go into detail about both the z-score and the MAD.
		      % formula. So I decided to focus on the z-score formula for now.
		      % 
		      %\item \textbf{An alternative way is to compute the mean absolute deviation (MAD):}
		      %      \begin{align*}
		      %	      \text{MAD}(X = \{x_1,x_2,\ldots,x_n\}) = \frac{1}{n} \sum_{i=1}^{n} \vert x_i - \bar{x} \vert, \\
		      %	      \text{where} \; \bar{x} = \frac{1}{n} \sum_{i=1}^{n}x_i, \; \text{thus} \; z_i = \frac{x_i-\bar{x}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2}}.
		      %      \end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Example: Data Matrix and Dissimilarity Matrix}
	\begin{itemize}
		\item Data matrix: \\[0.1cm]
		      \begin{center}
			      \begin{tabular}{| c | c | c |}
				      \hline
				      Point & Attribute $1$ & Attribute $2$ \\\hline
				      $x_1$ & $1$           & $2$           \\\hline
				      $x_2$ & $3$           & $5$           \\\hline
				      $x_3$ & $2$           & $0$           \\\hline
				      $x_4$ & $4$           & $5$           \\\hline
			      \end{tabular}\\[0.5cm]
		      \end{center}
		\item Dissimilarity matrix (with Euclidean distance): \\[0.1cm]
		      \begin{center}
			      \begin{tabular}{| c | c | c | c | c |}
				      \hline
				            & $x_1$  & $x_2$ & $x_3$  & $x_4$ \\\hline
				      $x_1$ & $0$    &       &        &       \\\hline
				      $x_2$ & $3,61$ & $0$   &        &       \\\hline
				      $x_3$ & $2,24$ & $5,1$ & $0$    &       \\\hline
				      $x_4$ & $4,24$ & $1$   & $5,39$ & $0$   \\\hline
			      \end{tabular}\\[0.2cm]
		      \end{center}
	\end{itemize}
\end{frame}

\begin{frame}{Distance on Numerical Data}
	\vspace{-1.5em}
	\begin{align*}
		d(x,y) = \sqrt[h]{\sum_{i=1}^{n} \vert x_i-y_i \vert^h}
	\end{align*}

	where $x = (x_1,x_2, \ldots, x_n)$ and $y = (y_1,y_2,\ldots,y_n)$ are two $n$-dimensional data objects.

	In fact, this distance induces a norm over real vector space, called $L_n$-norm.

	\begin{block}{Properties of a $L_n$-norm}
		\begin{itemize}
			\item $d(x,y) \geq 0$, positive definiteness.
			\item $d(x,y) = d(y,x)$, symmetry.
			\item $d(x,y) \leq d(x,z) + d(z,y)$, triangle inequality.
		\end{itemize}

		A distance satisfying these properties is called \textbf{metric}.
	\end{block}
\end{frame}

\begin{frame}{Special Cases of Minkowski Distance}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			$h=1$: \textbf{Manhattan Distance}
			\begin{itemize}
				\item Also known as city block, or $L_1$-norm.
				\item E.g. Hamming distance: number of bits that differ in two binary
				      vectors.
			\end{itemize}
			\vspace*{-1.5em}
			\begin{align*}
				d(x,y) = \sum_{i=1}^{n} \vert x_i - y_i \vert
			\end{align*}
		\end{column}

		\begin{column}{0.45\textwidth}
			$h=2$: \textbf{Euclidean}
			\begin{itemize}
				\item Also known as $L_2$-norm.
			\end{itemize}
			\vspace*{-1.5em}
			\begin{align*}
				d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2}.
			\end{align*}
		\end{column}

	\end{columns}

	\vspace*{0.1cm}


	$h \rightarrow \infty:$ \textbf{Supremum}
	\begin{itemize}
		\item Also known as $L_{\text{max}}$-norm, or $L_\infty$-norm.
		\item Maximum difference between any attribute of two vectors.
	\end{itemize}
	\vspace*{-1.5em}
	\begin{align*}
		d(x,y) = \lim_{h \rightarrow \infty} \left( \sum_{i=1}^{n} \vert x_i - y_i \vert^{h} \right)^{\frac{1}{h}} = \max_i \vert x_i-y_i \vert.
	\end{align*}

\end{frame}

% TODO: (Dis-)Similarity of data objects with mixed attribute types, c.f. chapter 2.4.6, page 75
% TODO: Cosine Similarity, c.f. chapter 2.4.7, page 77
