\section{Data integration}

\begin{frame}{Data Integration}
	\begin{itemize}
		\item \textbf{Data integration:}
		      \begin{itemize}
			      \item Combine data from multiple sources into a coherent store.
		      \end{itemize}
		\item \textbf{Schema integration:}
		      \begin{itemize}
			      \item E.g. \texttt{A.cust-id} $\equiv$ \texttt{B.cust-\#}.
			      \item Integrate metadata from different sources.
		      \end{itemize}
		\item \textbf{Entity-identification problem:}
		      \begin{itemize}
			      \item Identify the same real-world entities from multiple data
			            sources.
			      \item E.g. Bill Clinton = William Clinton.
		      \end{itemize}
		\item \textbf{Detecting and resolving {\color{airforceblue}data-value
					      conflicts}:}
		      \begin{itemize}
			      \item For the same real world entity, attribute values from
			            different sources are different.
			      \item Possible reasons:
			            \begin{itemize}
				            \item Different representations (coding).
				            \item Different scales, e.g. metric vs. British units.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Handling Redundancy in Data Integration}
	\begin{itemize}
		\item \textbf{Redundant data often occur when integrating multiple
			      databases.}
		      \begin{itemize}
			      \item \textbf{Object (entity) identification:} \\
			            The same attribute or object may have different names in different
			            databases.
			      \item \textbf{Derivable data:}\\
			            One attribute may be a "derived" attribute in another table. E.g.
			            annual revenue.
		      \end{itemize}
		\item \textbf{Redundant attributes:}
		      \begin{itemize}
			      \item Can be detected by \textbf{\color{airforceblue}correlation
				            analysis} and \textbf{\color{airforceblue}covariance analysis}.
		      \end{itemize}
		\item \textbf{Careful integration of the data from multiple sources:}
		      \begin{itemize}
			      \item Helps to reduce/avoid redundancies and inconsistencies and
			            improve mining speed and quality.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Correlation Analysis for Nominal Data (I)}
	\begin{itemize}
		\item \textbf{Given:}
		      \begin{itemize}
			      \item \textbf{Two categories:}
			            \begin{itemize}
				            \item $A$ has $n$ distinct values: $A := \{a_1, a_2, \ldots,
					                  a_n\}$, where $n \in \mathbb{N}_{>1}$.
				            \item $B$ has $m$ distinct values: $B := \{b_1, b_2, \ldots,
					                  b_m\}$
				                  , where $m \in \mathbb{N}_{>1}$.
			            \end{itemize}
			      \item \textbf{Data points:}
			            \begin{itemize}
				            \item $X$ is a multiset of data points: $X := \{(a, b)
					                  \; \vert \; a \in A \; \text{and} \; b \in B\}$.
			            \end{itemize}
		      \end{itemize}
		\item \textbf{Number of data points with value $(a_i,b_j)$:}
		      \begin{align}
			      c_{ij} = \#\{(a,b) \in X \mid a = a_i, b= b_i\}
		      \end{align}
		\item \textbf{Anticipated value (assuming equal distribution):}
		      \begin{align}
			      e_{ij} = \frac{\sum_{k=1}^{m} c_{ik} \cdot \sum_{l=1}^{n}c_{lj}}{\#X}
		      \end{align}
	\end{itemize}
\end{frame}

\begin{frame}{Correlation Analysis for Nominal Data (II)}
	\begin{itemize}
		\item \textbf{The values $c_{ij}$ and $e_{ij}$ are often presented in a
			      \color{airforceblue}contingency table:} \\
		      \centering
		      \vspace{5mm}
		      \begin{tabular}{l|c|c|c|c|}
			               & $a_1$                             & $\ldots$ & $a_n$             &
			      \\\hline
			      $b_1$    & $c_{11} (e_{11})$                 & $\ldots$ & $c_{n1} (e_{n1})$ &
			      $\sum_{i=1}^n e_{i1}$
			      \\\hline
			      $\ldots$ & $\ldots$                          & $\ldots$ & $\ldots$          & $\ldots$
			      \\\hline
			      $b_m$    & $c_{1m} (e_{1m})$                 & $\ldots$ & $c_{nm} (e_{nm})$ &
			      $\sum_{i=1}^n e_{im}$
			      \\\hline
			               & $\sum_{j=1}^m e_{1j}$             & $\ldots$ & $\sum_{j=1}^m
			      e_{nj}$  & $\sum_{i=1}^n\sum_{j=1}^m e_{ij}$
			      \\\hline
		      \end{tabular}
	\end{itemize}
\end{frame}

\begin{frame}{Correlation Analysis for Nominal Data (III)}
	\begin{itemize}
		\item \textbf{\color{airforceblue}$\chi^2$-test:}
		      \begin{align}
			      \chi^2 = \sum_{i=1}^{N}\sum_{j=1}^{M}
			      \frac{(c_{ij}-e_{ij})^2}{e_{ij}}.
		      \end{align}
		\item Summing over all cells of the contingency table.
		\item No correlation (i.e. independence of attributes) yields $\chi^2$
		      value of zero.
		\item The larger the $\chi^2$ value, the more likely the variables are
		      related.
		\item The cells that contribute the most to the $\chi^2$ value
		      are those whose actual count is very different from the expected count
		      $e_{ij}$.
	\end{itemize}
	\begin{itemize}
		\item \textbf{Correlation does not imply causality!}
		      \begin{itemize}
			      \item E.g. $\#$ of hospitals and $\#$ of car-thefts in a city are
			            correlated.
			      \item Both are causally linked to the third variable: population.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{$\chi^2$ Calculation: An Example}
	\centering
	\begin{tabular}{l|c|c|c|}
		                         & Play chess & Not play chess & Sum (row) \\\hline
		Like Science fiction     & $250 (90)$ & $200 (360)$    & $450$
		\\\hline
		Not like science fiction & $50 (210)$ & $1000 (840)$   & $1050$
		\\\hline
		Sum (column)             & $300$      & $1200$         & $1500$
		\\\hline
	\end{tabular}
	\begin{itemize}
		\item $\chi^2$ calculation:
		      \begin{align}
			      \chi^2 = \frac{(250-90)^2}{90} + \frac{(50-210)^2}{210} +
			      \frac{(200-360)^2}{360} + \frac{(1000-840)^2}{840} = 507.93.
		      \end{align}
		\item Degrees of freedom are $(n-1)\cdot(m-1) = (2-1)(2-1) = 1$.
		\item The $\chi^2$ value for a significance level of $0.001$ is
		      $10.828$.
		\item It shows that "like science fiction" and "play chess" are
		      correlated in the group.
	\end{itemize}
\end{frame}

\begin{frame}{Correlation Analysis of Numerical Data}
	\begin{itemize}
		\item \textbf{\color{airforceblue}Correlation coefficient :}
		      \begin{itemize}
			      \item Also called Pearson's product-moment coefficient:
			            \begin{align}
				            \text{Cor}(X) = \frac{\sum_{(a, b) \in X}
					            (a-\mu_{X_a})(b-\mu_{X_b})}{\#X\cdot\sigma_{X_a}\sigma_{X_b}} =
				            \frac{\sum_{(a, b) \in X}ab -\#X\cdot\mu_{X_a}\mu_{X_b}}{\#X\cdot
					            \sigma_{X_a}\sigma_{X_b}}.
			            \end{align}
			            where $X_a$ and $X_b$ are multisets containing all values of the
			            first
			            ($X_a$) and the second ($X_b$) component of the pairs in $X$.
			            $\mu_{X_a}$ and $\mu_{X_b}$ denote the mean values, while
			            $\sigma_{X_a}$ and $\sigma_{X_b}$ denote the standard
			            deviations of these multisets.
		      \end{itemize}
		\item If $\text{Cor}(X) > 0$: $X_a$ and $X_b$ are positively
		      correlated.
		\item If $\text{Cor}(X) = 0$: $X_a$ and $X_b$ are independent.
		\item If $\text{Cor}(X) < 0$: $X_a$ and $X_b$ are negatively
		      correlated.
	\end{itemize}
\end{frame}

\begin{frame}{Visually Evaluating Correlation}
	\begin{figure}[H]
		\centering
		\begin{minipage}{0.32\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
				\draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
				\foreach \x in {-1.7,-1.5,...,1.7}{
						\pgfmathsetmacro\xcoord{\x+rand/10}
						\pgfmathsetmacro\ycoord{\x+rand/2}
						\pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
						\pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
						\pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
						\pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
						\node[circle,draw,fill=black,scale=0.3] at
						(\xcoord,\ycoord) {};
					}
			\end{tikzpicture}
			\caption{a) Positive correlation.}
		\end{minipage}\hfill
		\begin{minipage}{0.32\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
				\draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
				\foreach \x in {-1.7,-1.5,...,1.7}{
						\pgfmathsetmacro\xcoord{\x+rand/10}
						\pgfmathsetmacro\ycoord{rand*2}
						\pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
						\pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
						\pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
						\pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
						\node[circle,draw,fill=black,scale=0.3] at
						(\xcoord,\ycoord) {};
					}
			\end{tikzpicture}
			\caption{b) Uncorrelated/no correlation.}
		\end{minipage}\hfill
		\begin{minipage}{0.32\textwidth}
			\centering
			\begin{tikzpicture}
				\draw[->, thick] (-1.7,0)--(1.7,0) node[right]{$x$};
				\draw[->, thick] (0,-1.7)--(0,1.7) node[above]{$y$};
				\foreach \x in {-1.7,-1.5,...,1.7}{
						\pgfmathsetmacro\xcoord{\x+rand/10}
						\pgfmathsetmacro\ycoord{-\x+rand/2}
						\pgfmathsetmacro\xcoord{\xcoord < -1.7 ? -1.7 : \xcoord}
						\pgfmathsetmacro\xcoord{\xcoord > 1.7 ? 1.7 : \xcoord}
						\pgfmathsetmacro\ycoord{\ycoord < -1.7 ? -1.7 : \ycoord}
						\pgfmathsetmacro\ycoord{\ycoord > 1.7 ? 1.7 : \ycoord}
						\node[circle,draw,fill=black,scale=0.3] at
						(\xcoord,\ycoord) {};
					}
			\end{tikzpicture}
			\caption{c) Negative correlation.}
		\end{minipage}\hfill
	\end{figure}
\end{frame}

\begin{frame}{Covariance of Numerical Data (I)}
	\begin{itemize}
		\item \textbf{\color{airforceblue}Covariance} \textbf{is similar to
			      correlation:}\\
		      \begin{align}
			      \text{Cov}(X) =
			      \frac{\sum_{i=1}^{n}(a_i-\mu_{X_a})(b_i-\mu_{X_b})}{\#X} =
			      \frac{\sum_{i=1}^{n}a_ib_i}{\#X}-\mu_{X_a}\mu_{X_b}
		      \end{align}
		\item \textbf{Computing the correlation based on the covariance:}\\
		      \begin{align}
			      \text{Cor}({X}) = \frac{\text{Cov}(X)}{\sigma_{X_a}\sigma_{X_b}}
		      \end{align}
	\end{itemize}
\end{frame}

\begin{frame}{Covariance of Numerical Data (II)}
	\begin{itemize}
		\item \textbf{Positive covariance:}\\
		      If $\text{Cov}(X) > 0$, then $A$ and $B$ tend to be either both
		      larger or both smaller than their expected values.
		\item \textbf{Negative covariance:}\\
		      If $\text{Cov}(X) < 0$, then if $A$ is larger than its expected
		      value, $B$ is likely to be smaller than its expected value and vice
		      versa.
		\item \textbf{Independence:}
		      \begin{itemize}
			      \item $\text{Cov}(X) = 0$.
			      \item \textbf{\color{airforceblue}But the converse is not true:}
			            Some pairs of random variables may have a covariance of $0$ but are
			            not independent. Only under some additional assumptions (e.g., the
			            data follow multivariate normal distributions) does a covariance of
			            $0$ imply independence.
		      \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Covariance: An Example}
	\begin{itemize}
		\item Suppose two stocks being part of $X$ have the following values:\\
		      $(2,5), (3,8), (5,10), (4,11), (6,14).$
		\item If the stocks are affected by the same industry trends, will
		      their prices rise or fall together?
		      \begin{align}
			      \text{Cov}(X) & = \frac{2\cdot5 + 3\cdot 8 + 5 \cdot 10 + 4 \cdot
				      11 + 6 \cdot 14}{5} - 4\cdot 9.6 = 4.
		      \end{align}
		\item Thus, $A$ and $B$ rise together since $\text{Cov}(X) > 0$.
	\end{itemize}
\end{frame}
