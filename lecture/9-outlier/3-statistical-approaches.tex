\section{Statistical Approaches}


\begin{frame}{Statistical Approaches}
	\begin{itemize}
		\item \textbf{Assumption:} Objects in a data set are \textbf{\color{airforceblue}generated by a stochastic process} (a generative model).
		\item \textbf{Idea:} Learn a generative model fitting the given data set, and then identify the objects in low-probability regions of the model as outliers.
		\item \textbf{Methods divided into two categories:}
	\end{itemize}
	\vspace*{-2em}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{center}
				\textbf{Parametric Methods}
			\end{center}
			\vspace*{-1.2em}
			\begin{itemize}
				\item Assumes that the normal data is generated by a parametric distribution with parameter $\theta$.
				\item The probability density function of the parametric distribution $f(x, \theta)$ gives the probability that object $x$ is generated by the distribution.
				\item Small values indicate potential outlier.
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\pause
			\begin{center}
				\textbf{Non-Parametric Methods}
			\end{center}
			\vspace*{-1.2em}
			\begin{itemize}
				\item Do not assume an a-priori statistical model and determine the model from the input data.
				\item Not completely parameter-free, but consider number and nature of the parameters to be flexible and not fixed in advance.
				\item \textbf{Examples:} \textbf{\color{airforceblue}histogram} and kernel-density estimation.
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}{Parametric Methods I: Detection of Univariate Outliers Based on Normal Distribution}
	% TODO: add plot for visual motivation
	\begin{itemize}
		\item \textbf{Univariate data:} A data set involving \textit{only one attribute} or variable.
		\item \textbf{Assumption:} Data are generated from a normal distribution.
		\item \textbf{Learn the parameters from the input data, and identify the points with low probability as outliers.}
		      \begin{itemize}
			      \item \textbf{\color{faugray}Recall}: A normal distribution is characterized by two parameters: mean $\mu$, and standard deviation $\sigma$, often also written as $\mathcal{N}(\mu, \sigma)$. Its probability density function is: $f(x)=\frac{1}{\sqrt{2\pi \sigma^2}} {\rm e}^{-\frac{(x-\mu)^2}{2\sigma^2}}$.
			      \item Use \textbf{maximum likelihood method} to estimate $\mu$ and $\sigma$.
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Maximum Likelihood Estimation}
	\begin{block}{Maximum Likelihood Estimation}
		\textit{Maximum Likelihood Estimation} (MLE) estimates \textit{parameters} of an assumed probability distribution such that the \textit{distribution fits the data as closely as possible}.
	\end{block}
	\begin{itemize}
		\item \textbf{Likelihood function} $\mathcal{L}(\theta|X)$ with
		      \begin{itemize}
			      \item parameter space $\theta$ and
			      \item observed data $X=\{x_1, x_2, \dots, x_n\}$
		      \end{itemize}
		      describes the joint probability of observed data as a function of the parameters of the assumed distribution.
		      \begin{itemize}
			      \item Frequently assumed distribution: Gaussian (normal) distribution $\mathcal{N}(\mu, \sigma)$.
			      \item Thus, the likelihood function $\mathcal{L}(\theta|X)$ with parameter space $\theta=\{\mu, \sigma\}$ is the Gaussian process:
			            \begin{equation*}
				            \mathcal{L}(\theta|X)=\mathcal{L}(\mu, \sigma|x_1, \dots, x_n) = \prod\nolimits_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} {\rm e}^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
			            \end{equation*}
		      \end{itemize}
		\item Find good estimates for $\theta$ such that $\widehat{\theta} = \argmax_{\theta} \mathcal{L}(\theta|X)$.
	\end{itemize}
\end{frame}


\begin{frame}{Maximum Likelihood Estimation}
	\begin{itemize}
		\item In the case of a Gaussian distribution: find estimates for $\mu$ and $\sigma$ such that
		      \begin{align*}
			      \widehat{\mu}    & = \textstyle\argmax_{\mu} \mathcal{L}(\theta|X),   \\
			      \widehat{\sigma} & = \textstyle\argmax_{\sigma} \mathcal{L}(\theta|X)
		      \end{align*}
		\item \textbf{General procedure:}
		      \begin{enumerate}
			      \item Generate two derivatives, with respect to $\mu$ and $\sigma$, respectively.
			      \item Solve each equation by setting them equal to zero.
		      \end{enumerate}
		\item Instead of taking the derivative directly, we take the log of the likelihood function as this makes it easier to take derivatives.
		      \begin{equation*}
			      \ln{\mathcal{L}(\theta|x_1, \dots, x_n)}=\ln{\mathcal{L}(\mu, \sigma|x_1, \dots, x_n)}= \ln{\bigg({\textstyle\prod\nolimits_{i=1}^n} \frac{1}{\sqrt{2\pi \sigma^2}} {\rm e}^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\bigg)}
		      \end{equation*}
		      \begin{itemize}
			      \item Logarithm is monotonically increasing, thus satisfying $\argmax_{\theta} \; \ln(\mathcal{L}(\theta|X)) = \text{argmax}_{\theta} \; \mathcal{L}(\theta|X)$.
			      \item Logarithm turns multiplication $\prod$ into addition $\sum$ making derivatives easier to compute.
		      \end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}{Maximum Likelihood Estimation: General Process}
	\begin{enumerate}
		\item Transform equation:
		      \vspace*{-1em}
		      \begin{equation*}
			      \ln \mathcal{L}(\mu, \sigma | x_1, \dots, x_n) = \ln{\bigg({\textstyle\prod\nolimits_{i=1}^n} \frac{1}{\sqrt{2\pi \sigma^2}} {\rm e}^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\bigg)}= -\frac{n}{2}\ln{2\pi}-\frac{n}{2}\ln{\sigma^2}-\frac{1}{2\sigma^2}\textstyle\sum\nolimits_{i=1}^n (x_i - \mu)^2
		      \end{equation*}
		\item To estimate parameters, take the partial derivative with respect to $\mu$ and $\sigma$:
		      \vspace*{-1em}
		      \begin{align*}
			      \textstyle\frac{\partial}{\partial \mu}\ln{(\mathcal{L}(\mu, \sigma|x_1, \dots, x_n))}    & = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i -\mu)                            \\
			      \textstyle\frac{\partial}{\partial \sigma}\ln{(\mathcal{L}(\mu, \sigma|x_1, \dots, x_n))} & =-\textstyle\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum_{i=1}^n (x_i-\mu)^2
		      \end{align*}
		\item Then, by setting these quation equals zero, we derive the following likelihood estimates:
		      \begin{center}
			      mean $\widehat{\mu}&=\frac{1}{n}\sum\nolimits_{i=1}^n x_i$ \hspace{3em} standard deviation $\widehat{\sigma}&=\sqrt{\frac{1}{n}\sum\nolimits_{i=1}^n (x_i-\mu)^2}$
		      \end{center}
		      % TODO: proof in appendix or discuss in lecture?
		      {\color{gray}Refer to appendix for proof.}
	\end{enumerate}
\end{frame}

\begin{frame}{Parametric Methods I: \\ Detection of Univariate Outliers Based on Normal Distribution (2)}
	\textbf{Example:}
	\begin{itemize}
		\item Average yearly temperatures: $\{24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4\}$.
		      % TODO: plot data?
		\item For these data with $n = 10$, we have
		      \begin{align}
			      \widehat{\mu}=28.61, \quad \widehat{\sigma}=\sqrt{2.29}=1.51.
		      \end{align}
		\item Then the most deviating value $24.0$ is $4.61$ away form the estimated mean.
		\item $\mu \pm 3\sigma $ contains $99.7\%$ of the data under the assumption of normal distribution.
		\item Because $\frac{4.61}{1.51}  =  3.04  >  3$, \\
		      the probability that $24.0$ is generated by a normal distribution is less than $0.15\%$.
		      \begin{itemize}
			      \item Each \emph{tail} to the left and to the right of the $99.7\%$ has $0.15\%$.
		      \end{itemize}
		\item Hence, $24.0$ identified as an outlier.
	\end{itemize}
\end{frame}


\begin{frame}{Parametric Methods I: The Grubb's Test}
	\begin{itemize}
		\item Univariate outlier detection: The Grubb's test (maximum normed residual test).
		      \begin{itemize}
			      \item Another statistical method under normal distribution
			      \item For each object $x$ in a data set, compute its $z$-score:  $z = \frac{\vert x - \overline{x}\vert}{s}$,
			            \begin{itemize}
				            \item where $\overline{x}$ is the mean and $s$ the standard deviation of the input data.
			            \end{itemize}
			      \item $x$ is an outlier, if
			            \begin{align}
				            z \geq \frac{N-1}{\sqrt{N}} \sqrt{\frac{t^2_{\frac{\alpha}{2N},N-2}}{N-2 + t^2_{\frac{\alpha}{2N},N-2}}},
			            \end{align}
		      \end{itemize}
		      \begin{itemize}
			      \item where $t^2_{\frac{\alpha}{2N},N-2}$ is the value taken by a $t$-distribution \\
			            at a significance level of $\frac{\alpha}{2N}$, and $N$ is the number of objects in the data set.
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Parametric Methods II: Detection of Multivariate Outliers}
	\begin{itemize}
		\item \textbf{Multivariate data}:
		      \begin{itemize}
			      \item A data set involving \textbf{\color{airforceblue}two or more attributes} or variables.
		      \end{itemize}
		\item \textbf{Transform the multivariate outlier-detection task into a univariate outlier-detection problem.}
		\item \textbf{Method 1: Compute Mahalanobis distance.}
		      \begin{itemize}
			      \item Let $\mathbf{\overline{o}}$ be the mean vector for a multivariate data set. Mahalanobis distance for an object $\mathbf{o}$ to $\mathbf{\overline{o}}$ is $\Delta(\mathbf{o}, \mathbf{\overline{o}}) = (\mathbf{o} - \mathbf{\overline{o}})^T \mathbf{S}^{-1} (\mathbf{o} - \mathbf{\overline{o}})$ where $\mathbf{S}$ is the covariance matrix.
			      \item Use the Grubb's test on this measure to detect outliers.
		      \end{itemize}
		\item \textbf{Method 2: Use $\chi^2$ statistic.}
		      \vspace*{-1em}
		      \begin{align*}
			      \chi^2 = \sum^n_{i=1} \frac{(\mathbf{o}_i-E_i)^2}{E_i},
		      \end{align*}
		      \vspace*{-1em}
		      \begin{itemize}
			      \item where $E_i$ is the mean of the $i$-dimension among all objects, and $n$ is the dimensionality.
			      \item If $\chi^2$ statistic is large, then object $\mathbf{o}_i$ is an outlier.
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Parametric Methods III: Using Mixture of Parametric Distributions}
	\begin{itemize}
		\item \textbf{Assuming that data are generated by a normal distribution \\ could sometimes be overly simplified.}
		\item \textbf{Example (right figure):}
		      % TODO: insert figure!
		      \begin{itemize}
			      \item The objects between the two clusters cannot be captured \\
			            as outliers since they are close to the estimated mean.
		      \end{itemize}
		\item \textbf{Assume the normal data is generated by {\color{airforceblue}two normal distributions.}}
		      \begin{itemize}
			      \item For any object $\mathbf{o}$ in the data set, the probability that $\mathbf{o}$ is generated by the mixture of the two distributions is given by
			            \begin{align}
				            \text{P}(\mathbf{o} \; \vert \; \theta_1, \theta_2) = f(\mathbf{o} \; \vert \; \theta_1) + f(\mathbf{o} \; \vert \; \theta_2),
			            \end{align}
			            where $f_{\theta 1}$ and $f_{\theta 2}$ are the probability density functions of $\theta_1$ and $\theta_2$.
			      \item Then use expectation-maximization (EM) algorithm \\
			            to learn the parameters $\mu_1, \sigma_1, \mu_2, \sigma_2$ from the data.
			      \item An object $\mathbf{o}$ is an outlier if it does not belong to any cluster.
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Non-Parametric Methods:Detection Using Histogram}
	\begin{itemize}
		\item \textbf{The model of normal data is learned from the input data without any apriori structure.}
		      \begin{itemize}
			      \item Often makes fewer assumptions about the data, and thus can be applicable in more scenarios.
		      \end{itemize}
		\item \textbf{Outlier detection using histograms:}
		      \begin{itemize}
			      \item Figure shows the histogram of purchase amounts in transactions.
			      \item A transaction with the amount of $\$7,500$ is an outlier, since only $0.2\%$ \\ of the transactions have an amount higher than $\$5,000$.
		      \end{itemize}
	\end{itemize}
	\centering
	\vspace{0.2cm}
	\includegraphics[width=0.25\textwidth]{img/histogram8.png}
\end{frame}


\begin{frame}{Non-Parametric Methods: Detection Using Histogram (2)}
	\begin{itemize}
		\item \textbf{Problem:}
		      \begin{itemize}
			      \item Hard to \textbf{\color{airforceblue}choose an appropriate bin size} for histogram.
			            \begin{itemize}
				            \item Too small bin size $\rightarrow$ normal objects in empty/rare bins, false positive.
				            \item Too big bin size $\rightarrow$ outliers in some frequent bins, false negative.
			            \end{itemize}
		      \end{itemize}
		\item \textbf{Solution:}
		      \begin{itemize}
			      \item Adopt kernel-density estimation to estimate the probability-density distribution of the data.
			            \begin{itemize}
				            \item If the estimated density function is high, the object is likely normal.
				            \item Otherwise, it is likely an outlier.
			            \end{itemize}
		      \end{itemize}
	\end{itemize}
\end{frame}
