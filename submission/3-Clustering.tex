\documentclass[
english,
smallborders
]{i6prcsht}
\usepackage{i6common}
\usepackage{i6lecture}

\usepackage{todonotes}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{pdfpages}
\usepackage{csquotes}
\usepackage{awesomebox}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{array}
\usepackage{enumitem}

\usepackage{listings}
\usepackage{multicol}
\usetikzlibrary{calc}
\lstset
{
    language=python,
	showtabs=true,
	tab=,
	tabsize=2,
	basicstyle=\ttfamily\scriptsize,
	backgroundcolor=\color{lightgray!20},
	breakindent=.5\textwidth,
	frame=single,
	breaklines=true,
	numbers=left,
	stepnumber=1,
	deletekeywords=[2]{abs,max}
}

\usepackage{tcolorbox}
\tcbuselibrary{listings,skins,breakable}

\newtcolorbox{functionbox}[1]{
    colback=gray!5!white,
    colframe=gray!75!black,
    fonttitle=\bfseries,
    title=#1,
	arc=0mm,
	outer arc=0mm,
    breakable,
    enhanced,
	boxrule=1pt,
    attach boxed title to top left={yshift=-3mm,xshift=5mm},
    boxed title style={colback=gray!85!black,arc=1mm},
    before upper={\parindent=0pt}
}

\newcommand{\points}[1]{\hfill \color{red}(#1 Points)\color{black}}

\newcommand{\Point}[2]{\texttt{Point(x=#1, y=#2)}}

\setlength{\columnsep}{-25pt}
% First parameter: Points (gray)
% Second parameter: Lowest X coordinate
% Third parameter: Highest X coordinate
% Fourth parameter: Lowest Y coordinate
% Fifth parameter: Highest Y coordinate
% Sixth parameter: Legend text width
% Seventh parameter: Columns of legend
\newcommand{\CoordinateSystem}[7]{
    \begin{center}
		\vspace*{0.1cm}
        \newcommand*{\xMin}{#2}%
        \newcommand*{\xMax}{#3}%
        \newcommand*{\yMin}{#4}%
        \newcommand*{\yMax}{#5}%
        \begin{tikzpicture}[scale=0.375]
            % Draw grid
            \foreach \i in {\xMin,...,\xMax} {
                \draw [very thin] (\i,\yMin) -- (\i,\yMax)  node [below] at
                (\i,\yMin) {\tiny$\i$};
            }
            \foreach \i in {\yMin,...,\yMax} {
                \draw [very thin] (\xMin,\i) -- (\xMax,\i) node [left] at
                (\xMin,\i) {\tiny$\i$};
            }

            % Draw points (gray)
            \foreach \point in {#1} {
                \node[circle,draw,fill=gray,scale=0.5] at \point {};
            }

			% Draw legend
			% Check if either cluster has points
            \ifstrempty{#1}{}{\def\hasGray{true}}

			% Legend
			\node[very thin,right] at (\xMax + 0.5, \yMax - 0.5) (l) {\scriptsize Points:};
			\node[very thin,right, below=-2mm of l] (p) {};

			% If there are gray points
			\ifdefined\hasGray
				\node[circle,draw,fill=gray,scale=0.5] (l) at ($(l |- p.south) - (0, 0.5)$) {};
				\node[very thin,above right=7.4mm and -7mm of l, anchor=north west,text width=#6] (p) {\scriptsize 
				\begin{multicols}{#7}
					\noindent
					\begin{itemize}
						\foreach \point in {#1} {
							\item[] \point
						}
					\end{itemize}
					
				\end{multicols}
				};
			\fi
        \end{tikzpicture}
		
		\vspace*{0.2cm}
    \end{center}
}

\hyphenation{Stud-On}

\begin{document}

\title{Submission 3: \\ Clustering}
\maketitle
\vspace*{-2cm}

\section*{About this Assignment}

In this assignment, your task is to implement the algorithms for \hyperref[sec:task-one]{K-means} and \hyperref[sec:task-two]{DBSCAN}. For this purpose, you have access to a basic code skeleton, some helper classes, and several test cases.

\subsection*{Key Data}

\begin{itemize}
	\item \textbf{Max. Group Size:} 3
	\item \textbf{Max. Points:} 30
	\item \textbf{Estimated Workload:} 3 - 4 hours
\end{itemize}

\subsection*{How to Work on the Assignment}

To start working on the assignment, you'll need to accept the assignment via GitHub Classroom by clicking the provided link. This will set up a new GitHub repository for your group, packed with all the necessary files for the assignment. If you're joining an existing group, it'll add you to that group's repository.\footnote{Each student must join individually. You can join groups while accepting an assignment.}

Once that's done, you have two main options for working on your assignment. You can clone the repository\footnote{If you're unfamiliar with Git or GitHub, check out this helpful guide: \url{https://github.com/git-guides/}} to your local machine by navigating to \texttt{Code $\rightarrow$ Local}, which allows you to work directly from your computer. Alternatively, you might prefer using GitHub Codespaces by selecting \texttt{Code $\rightarrow$ Codespaces} for a virtual online environment, complete with the ability to run Python through the \texttt{Terminal} provided.

Whichever method you choose, it's crucial to commit and push your changes back to the repository to submit your solution\footnotemark[\value{footnote}]. After your submission, GitHub Actions takes over to automatically grade your solution and provide feedback. You'll find this feedback in the \texttt{Actions} tab of your repository. If you didn't receive full points, you can improve your solution and push the changes back to the repository to trigger a reevaluation.

\subsection*{How to Prepare the Transfer the Points to StudOn}

In addition to joining the GitHub Classroom, you also need to register your GitHub username on StudOn. This is necessary to transfer the points you've earned on GitHub to StudOn. To do this, enter your GitHub username in \texttt{Submission 3 - GitHub Username}. Make sure to enter your username correctly, as otherwise, the points cannot be transferred.

After submission deadline, the points you've earned on GitHub will be transferred to StudOn. This process is not immediate and may take a few days. If you have any questions or issues, please contact us via the StudOn forum.

\subsection*{Restrictions}

Within the scope of your implementation, you are not permitted to modify the helper classes, the test cases, or the provided GitHub Actions.

This will be checked on a random basis, and any attempt to do so will result in zero points for the involved group, similar to the consequences of plagiarism.

\newpage

\section*{Task 1: K-means}
\label{sec:task-one}

K-means is a simple and widely used clustering algorithm. It partitions a dataset into \textit{k} clusters by iteratively assigning each data point to the cluster with the nearest centroid and updating the centroids based on the mean of the data points in the cluster.

\subsection*{Task 1.1 \points{3}}

The first step of the K-means algorithm is to distribute the data points to the k partitions. This partition can be done randomly or by using a more sophisticated method. All partitions have to be non-empty after the initialization and each data point has to be assigned to exactly one partition.

Open \texttt{kmeans.py} and implement \texttt{\_initialize\_partitions}, which initializes the partitions for the K-means algorithm:

\vspace*{0.3cm}

\begin{functionbox}{\_initialize\_partitions}
	\begin{lstlisting}[numbers=none]
def _initialize_partitions(
    self, 
    points: List[Point]
):
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Initializes the partitions (self.partitions) by assigning each point to a cluster/partition. All clusters/partitions are non empty after this method is called.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{points} (List[Point]): The points to cluster.
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None - The method stores the partitions in \texttt{self.partitions}
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method expects a list of \texttt{Point}s and does not return anything. The method should put each point into one of the partitions available at \texttt{self.partitions}. The number of partitions is given by the variable \texttt{self.k}.

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/kmeans/test_initialize_partitions.py
\end{lstlisting}

\newpage

\subsection*{Task 1.2 \points{3}}

A second important part of the K-means algorithm is to update the centroids of the partitions. The centroid of a partition is the mean of all points in the partition.

Open \texttt{kmeans.py} and implement \texttt{\_update\_centroids}, which updates the centroids of the partitions:

\vspace*{0.3cm}

\begin{functionbox}{\_update\_centroids}
	\begin{lstlisting}[numbers=none]
def _update_centroids(self):
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Updates the centroids of the partitions and writes the new centroids into self.centroids.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None - The method updates \texttt{self.centroids}
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method does not expect any parameters and does not return anything. The method should update the \texttt{self.centroids} based on the mean of all points in the partition. The i-th centroid in \texttt{self.centroids} should refer to the i-th partition in \texttt{self.partitions}.

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/kmeans/test_update_centroids.py
\end{lstlisting}

\newpage

\subsection*{Task 1.3 \points{5}}

The last step of the K-means algorithm is to assign each data point to the cluster with the nearest centroid. 

Open \texttt{kmeans.py} and implement \texttt{\_reassign\_points}, which assigns each data point to the partition with the nearest centroid:

\vspace*{0.3cm}

\begin{functionbox}{\_reassign\_points}
	\begin{lstlisting}[numbers=none]
def _reassign_points(self) -> bool:
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Reassigns each point to the partition with the closest centroid. Ensures that each partition is non-empty after reassigning the points, by randomly reassigning a single point from a random non-empty partition with more than one element into each empty partition.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{bool}: True if the reassignment changed the partitions, False otherwise
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method does not expect any parameters and returns a boolean. The method should remove all points from their previous partition and add them to the partition with the nearest centroid if there is a closer centroid. If the reassignment changed the partitions, the method should return \texttt{True}, otherwise \texttt{False}.

If there are empty partitions after the reassignment, the method should randomly reassign a single point from a random non-empty partition with more than one element into each empty partition. This is necessary to avoid empty partitions, which would lead to K-means not producing $k$ clusters, but $k-n$ clusters ($n$ being the number of empty partitions).

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/kmeans/test_reassign_points.py
\end{lstlisting}

\newpage

\subsection*{Task 1.4 \points{3}}

The K-means algorithm is iterative. The algorithm stops if the partitions do not change anymore or if a maximum number of iterations is reached.

Open \texttt{kmeans.py} and implement \texttt{fit}, which combines the previous steps to implement the K-means algorithm:

\vspace*{0.3cm}

\begin{functionbox}{fit}
	\begin{lstlisting}[numbers=none]
def fit(
    self, 
    points: List[Point]
):
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Fit the K-Means clustering instance to the given points.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{points} (List[Point]): The points to cluster.
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method expects a list of \texttt{Point}s and does not return anything. The method should implement the K-means algorithm by calling the methods \texttt{\_initialize\_partitions}, \texttt{\_update\_centroids}, and \texttt{\_reassign\_points}. 

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/kmeans/test_fit.py
\end{lstlisting}

\newpage

\section*{Task 2: DBSCAN}
\label{sec:task-two}

DBSCAN is a density-based clustering algorithm that groups together points that are closely packed together. It is based on two parameters: $\varepsilon$ and $MinPts$. 

\subsection*{Task 2.1 \points{2}}

A core part of the DBSCAN algorithm is to find all points that are within a distance of $\varepsilon$ of a given point, the so-called $\varepsilon$-Neighborhood.

Open \texttt{dbscan.py} and implement \texttt{\_get\_neighborhood}, which returns all points that are within a distance of $\varepsilon$ of a given point:

\vspace*{0.3cm}

\begin{functionbox}{\_get\_neighborhood}
	\begin{lstlisting}[numbers=none]
def _get_neighborhood(
    self, 
    point: Point, 
    points: List[Point]
) -> List[Point]:
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Get the neighborhood of a point.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{point} (Point): The point to get the neighborhood of.
		\item \texttt{points} (List[Point]): The points to consider.
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{List[Point]}: The points in the neighborhood.
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method expects the \texttt{Point} for which the neighborhood should be determined and a list of all \texttt{Point}s that should be considered. The method should return all points that are within a distance of $\varepsilon$ of the given point as a list of \texttt{Point}s. $\varepsilon$ is given by the variable \texttt{self.epsilon}.

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/dbscan/test_epsilon_neighborhood.py
\end{lstlisting}

\newpage

\subsection*{Task 2.2 \points{14}}

The function \texttt{\_get\_neighborhood} can be used to implement the DBSCAN algorithm. The algorithm can be implemented either iteratively or recursively.

Open \texttt{dbscan.py} and implement \texttt{fit}, which implements the DBSCAN algorithm:

\vspace*{0.3cm}

\begin{functionbox}{fit}
	\begin{lstlisting}[numbers=none]
def fit(
    self, 
    points: List[Point]
) -> None:
\end{lstlisting}
	
	\textbf{Description:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item Fit the DBSCAN clustering instance to the given points.
	\end{itemize}
	
	\textbf{Parameters:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item \texttt{points} (List[Point]): The points to cluster.
	\end{itemize}
	
	\textbf{Returns:}
	\begin{itemize}[leftmargin=*,topsep=0pt]
		\item None - The method stores the found clusters in \texttt{self.clusters} and the noise points in \texttt{self.noise}
	\end{itemize}
\end{functionbox}

\vspace*{0.5cm}

The method expects a list of \texttt{Point}s and does store the found clusters in the variable \texttt{self.clusters} and the noise points in the variable \texttt{self.noise}. 

The method can be implemented either iteratively or recursively. Additional helper methods can be implemented if necessary, but they will not be tested and therefore award no points.

You can test whether your implementation is correct by executing the following command in the console:

\vspace*{0.3cm}

\begin{lstlisting}
pytest tests/dbscan/test_fit.py
\end{lstlisting}

\newpage

\section*{Appendices}

In \hyperref[sec:task-one]{Task 1} and \hyperref[sec:task-two]{Task 2} test cases are provided and used to grade the submission.

\subsection*{Dataset(s)}

The most test cases are based on the following data sets:

\subsubsection*{Small Point Dataset}

All test cases starting with the prefix \texttt{test\_with\_small\_point\_dataset} are based on the small dataset of 2D Points known from Exercise Sheet 5 - Task 1/Task 2.

The dataset is structured as follows:

\CoordinateSystem{(1,1), (1,2), (1,4), (2,1), (2,3), (3,2), (3,4), (4,1), (4,3), (4,4)}{0}{5}{0}{5}{5cm}{5}


\subsubsection*{Bigger Point Dataset}

All test cases starting with the prefix \texttt{test\_with\_bigger\_point\_dataset} are based on a bigger dataset of one-hundred 2D Points.

The dataset is structured as follows:

\CoordinateSystem{
	(-10,-10), (-10,-9), (-10,-7), (-10,10), 
	(-9,-10), (-9,-9), (-9,-8), (-9,-6), (-9,9), 
	(-8,-9), (-8,-8), (-8,-7), (-8,-5), (-8,8), 
	(-7,-8), (-7,-7), (-7,-6), (-7,-4), (-7,7),
	(-6,-7), (-6,-6), (-6,-5), (-6,-3), (-6,6),
	(-5,-6), (-5,-5), (-5,-4), (-5,-2), (-4,-5),
	(-4,-4), (-4,-3), (-4,-1), (-4,4), (-3,-4),
	(-3,-3), (-3,-2), (-3,0), (-3,3), (-2,-3),   
	(-2,-2), (-2,-1), (-2,1), (-2,2), (-1,-2),
	(-1,-1), (-1,0), (-1,1), (-1,2), (0,-3),
	(0,-1), (0,0), (0,1), (0,3), (1,-4), (1,-2),
	(1,1), (1,2), (1,4), (2,-5), (2,-3), (2,2),
	(2,3), (2,5), (3,-6), (3,-4), (3,3), (3,4),
	(3,6), (4,-7), (4,-5), (4,4), (4,5), (4,7),
	(5,-8), (5,-6), (5,5), (5,5), (5,6), (5,8),
	(6,-9), (6,-7), (6,6), (6,7), (6,9), (7,-10),
	(7,-8), (7,7), (7,8), (7,10), (8,-10), (8,-9),
	(8,8), (8,9), (9,-10), (9,-9), (9,9), (9,10),
	(10,-10), (10,-8), (10,10)
}{-11}{11}{-13}{13}{7cm}{5}


\subsection*{Helper Classes}

The following helper classes are provided in the \texttt{classes/} folder to support your implementation. Each class serves a specific purpose in the clustering algorithms.

\vspace*{0.5cm}

\subsubsection*{Basic Data Structures}

\paragraph{Point} (\texttt{classes/point.py})

Represents a single point in 2D space with additional flags for DBSCAN.

\begin{itemize}
	\item \textbf{Attributes:}
	      \begin{itemize}
		      \item \texttt{x} (float) - The x-coordinate of the point
		      \item \texttt{y} (float) - The y-coordinate of the point
		      \item \texttt{visited} (bool) - Flag for DBSCAN to indicate if the point has been visited (default: False)
		      \item \texttt{clustered} (bool) - Flag for DBSCAN to indicate if the point is part of a cluster (default: False)
	      \end{itemize}
	\item \textbf{Key Methods:}
	      \begin{itemize}
		      \item \texttt{get\_x()} - Returns the x-coordinate
		      \item \texttt{get\_y()} - Returns the y-coordinate
		      \item \texttt{get\_distance(other)} - Calculates Euclidean distance to another point
		      \item \texttt{is\_visited()} / \texttt{set\_visited(visited)} - Check/set visited flag
		      \item \texttt{is\_clustered()} / \texttt{set\_clustered(clustered)} - Check/set clustered flag
	      \end{itemize}
	\item \textbf{Usage:} Create points like \texttt{Point(1.0, 2.0)} or \texttt{Point(3.5, 4.2, visited=True)}
	\item \textbf{Example:}
	      \begin{lstlisting}
point1 = Point(1.0, 2.0)
point2 = Point(4.0, 6.0)
distance = point1.get_distance(point2)  # Calculate distance
point1.set_visited(True)  # Mark as visited for DBSCAN
print(f"Point: {point1}")  # Output: (1.0, 2.0)
    \end{lstlisting}
\end{itemize}

\vspace*{0.3cm}

\paragraph{Cluster} (\texttt{classes/cluster.py})

Represents a cluster (or partition) of points. Can be used for both K-means partitions and DBSCAN clusters.

\begin{itemize}
	\item \textbf{Attributes:}
	      \begin{itemize}
		      \item \texttt{points} (List[Point]) - List of points in the cluster
	      \end{itemize}
	\item \textbf{Key Methods:}
	      \begin{itemize}
		      \item \texttt{add\_point(point)} - Add a point to the cluster
		      \item \texttt{remove\_point(point)} - Remove a point from the cluster
		      \item \texttt{get\_points()} - Returns all points as a list
		      \item \texttt{\_\_contains\_\_(point)} - Check if point is in cluster (enables \texttt{point in cluster})
		      \item \texttt{\_\_len\_\_()} - Get number of points in cluster (enables \texttt{len(cluster)})
		      \item \texttt{\_\_iter\_\_()} - Iterate over points (enables \texttt{for point in cluster})
	      \end{itemize}
	\item \textbf{Usage:} Essential for storing and managing groups of points in both algorithms
	\item \textbf{Example:}
	      \begin{lstlisting}
cluster = Cluster()
cluster.add_point(Point(1.0, 2.0))
cluster.add_point(Point(3.0, 4.0))

print(f"Cluster size: {len(cluster)}")  # Output: 2
for point in cluster:
    print(f"Point: {point}")

if Point(1.0, 2.0) in cluster:
    print("Point found in cluster")
    \end{lstlisting}
\end{itemize}

\vspace*{0.3cm}

\vspace*{0.5cm}

\subsubsection*{Practical Tips}

\begin{itemize}
	\item \textbf{Distance Calculation:} \\ Use \texttt{point1.get\_distance(point2)} for Euclidean distance between points
	\item \textbf{DBSCAN Flags:} \\ The \texttt{visited} and \texttt{clustered} flags in Point are specifically designed for DBSCAN implementation
	\item \textbf{K-means Partitions:} \\ Use Cluster objects to represent the k partitions in K-means
	\item \textbf{DBSCAN Clusters:} \\ Use Cluster objects to store the final clusters found by DBSCAN
	\item \textbf{Iteration:} \\ Both Point and Cluster classes support Python's standard iteration patterns
	\item \textbf{Point Equality:} \\ Points are considered equal if they have the same x and y coordinates
	\item \textbf{Debugging:} \\ Both classes have \texttt{\_\_str\_\_} methods for easy printing and debugging
	\item \textbf{Neighborhood Search:} \\ For DBSCAN, use \texttt{get\_distance()} to find points within epsilon distance
\end{itemize}

\vspace*{0.3cm}


\end{document}
