{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 2. Data analysis & Preprocessing\n",
    "\n",
    "In this exercise you will get to know the basics from the lectures \"3. Getting to Know Your Data\" and \"4. Preprocessing\" in their practical use and apply them yourself.\n",
    "\n",
    "Since this practice sheet is designed to be used in three sessions, it is roughly divided into three parts:\n",
    "\n",
    "- Part One: Getting to Know Your Data\n",
    "- Part Two: Preprocessing - Data cleaning & Data integration\n",
    "- Part Three: Preprocessing - Data reduction, data transformation & data discretization\n",
    "\n",
    "Of course, depending on how quickly an exercise group progresses in the actual exercise, one of these parts may not be discussed entirely in the affected exercise, or parts of the subsequent part may already be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part Three: Preprocessing - Data reduction, data transformation & data discretization\n",
    "\n",
    "In this part you will apply the theoretical knowledge gained in the second part of the lecture \"Preprocessing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    dataset_folder + \"/adventure-works.db\",\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(dataset_folder + \"/adventure-works.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the clean dataframe(s)\n",
    "# Order dataframe\n",
    "order_dataframe = pd.read_sql_query(\n",
    "    \"SELECT p.ProductID,p.Name,p.ProductNumber,p.MakeFlag,p.FinishedGoodsFlag,p.Color,p.SafetyStockLevel,\"\n",
    "    \"p.ReorderPoint,p.StandardCost,p.ListPrice,p.Size,p.SizeUnitMeasureCode,p.WeightUnitMeasureCode,p.Weight,\"\n",
    "    \"p.DaysToManufacture,p.ProductLine,p.Class,p.Style,p.ProductSubcategoryID,p.ProductModelID,p.SellStartDate,\"\n",
    "    \"p.SellEndDate,p.DiscontinuedDate,d.PurchaseOrderID,d.PurchaseOrderDetailID,d.DueDate,d.OrderQty,d.ProductID,\"\n",
    "    \"d.UnitPrice,d.ReceivedQty,d.RejectedQty,h.RevisionNumber,h.Status,h.EmployeeID,h.VendorID,h.ShipMethodID,\"\n",
    "    \"h.OrderDate,h.ShipDate,h.SubTotal,h.TaxAmt,h.Freight,h.TotalDue,e.NationalIDNumber,e.LoginID,e.OrganizationNode,\"\n",
    "    \"e.JobTitle,e.BirthDate,e.MaritalStatus,e.Gender,e.HireDate,e.SalariedFlag,e.VacationHours,e.SickLeaveHours,\"\n",
    "    \"e.CurrentFlag,r.PersonType,r.NameStyle,r.Title,r.FirstName,r.MiddleName,r.LastName,r.Suffix,r.EmailPromotion,\"\n",
    "    \"r.AdditionalContactInfo,r.Demographics \"\n",
    "    \"FROM Product p \"\n",
    "    \"JOIN PurchaseOrderDetail d ON p.ProductID = d.ProductID \"\n",
    "    \"JOIN PurchaseOrderHeader h ON d.PurchaseOrderID = h.PurchaseOrderID \"\n",
    "    \"JOIN Employee e ON h.EmployeeID = e.BusinessEntityID \"\n",
    "    \"JOIN Person r ON e.BusinessEntityID = r.BusinessEntityID\",\n",
    "    connection,\n",
    "    index_col=\"PurchaseOrderDetailID\",\n",
    ")\n",
    "\n",
    "# CurrencyRate dataframe\n",
    "currency_rate_dataframe = pd.read_sql_query(\n",
    "    \"SELECT STRFTIME('%Y-%m-%d', CurrencyRateDate) AS CurrencyRateDate,AverageRate,EndOfDayRate \"\n",
    "    \"FROM CurrencyRate \"\n",
    "    \"WHERE FromCurrencyCode='USD' AND ToCurrencyCode='EUR'\",\n",
    "    connection,\n",
    "    index_col=\"CurrencyRateDate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Normalization\n",
    "\n",
    "One method introduced in the lecture and frequently used in Data Science is normalization. In order to apply this practically, we will first take a look at a part of the order_dataframe already known from Part One. More precisely, we are looking at some numeric attributes from order_dataframe.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Display the head of the SubTotal, Freight and OrderQty attributes from order_dataframe.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the head of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the head of SubTotal, Freight and OrderQty\n",
    "order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Display the minimum, maximum, mean and standard deviation of SubTotal, Freight and OrderQty.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum of SubTotal, Freight and OrderQty\n",
    "order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the maximum of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the maximum of SubTotal, Freight and OrderQty\n",
    "order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the mean of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the mean of SubTotal, Freight and OrderQty\n",
    "order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the standard deviation of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the standard deviation of SubTotal, Freight and OrderQty\n",
    "order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As can be clearly seen, the three attributes differ significantly. This can be a hindrance for some knowledge discovery tasks. For this reason, normalization is often performed, scaling the attribute values to a much smaller specified range of values.\n",
    "\n",
    "In the lecture you were introduced to three different variants of normalization: The min-max normalization, the z-score normalization and the normalization by decimal scaling.\n",
    "\n",
    "Below you can see the implementation of one of the normalization methods for the attributes SubTotal, Freight and OrderQty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# It is always good to define methods that you may want to use more often as a function.\n",
    "def mystery_normalization(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # We need to normalize each column individually\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = (dataframe[column] - dataframe[column].mean()) / dataframe[\n",
    "            column\n",
    "        ].std()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Now we can apply the function we just defined to our dataframe\n",
    "mystery_normalization_dataframe = mystery_normalization(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "mystery_normalization_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Determine whether the above function mystery_normalization is an implementation of min-max normalization, the z-score normalization, or normalization by decimal scaling.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "<b>The function is an implementation of:</b>\n",
    "1. [ ] Min-max normalization (for the interval [0, 1])\n",
    "2. [ ] Z-score normalization\n",
    "3. [ ] Normalization by decimal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<b>The function is an implementation of:</b>\n",
    "1. [ ] Min-max normalization (for the interval [0, 1])\n",
    "2. [X] Z-score normalization\n",
    "3. [ ] Normalization by decimal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Implement a function for each of the three normalization methods you got to know. (You may, of course, reuse the above code when you work on the corresponding function).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a min-max normalization for the interval [0, 1]\n",
    "def min_max_normalization(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the min-max normalization\n",
    "min_max_dataframe = min_max_normalization(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "min_max_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a min-max normalization for the interval [0, 1]\n",
    "def min_max_normalization(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # We need to normalize each column individually\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = (dataframe[column] - dataframe[column].min()) / (\n",
    "            dataframe[column].max() - dataframe[column].min()\n",
    "        )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the min-max normalization\n",
    "min_max_dataframe = min_max_normalization(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "min_max_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation of the min_max_dataframe\n",
    "print(\"Minimum:\")\n",
    "print(min_max_dataframe.min())\n",
    "\n",
    "print(\"\\nMaximum:\")\n",
    "print(min_max_dataframe.max())\n",
    "\n",
    "print(\"\\nMean:\")\n",
    "print(min_max_dataframe.mean())\n",
    "\n",
    "print(\"\\nStandard deviation:\")\n",
    "print(min_max_dataframe.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a z-score normalization\n",
    "def z_score_normalization(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the z_score normalization\n",
    "z_score_dataframe = z_score_normalization(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "z_score_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a z-score normalization\n",
    "def z_score_normalization(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # We need to normalize each column individually\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = (dataframe[column] - dataframe[column].mean()) / dataframe[\n",
    "            column\n",
    "        ].std()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the z_score normalization\n",
    "z_score_dataframe = z_score_normalization(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "z_score_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation of the z_score_dataframe\n",
    "print(\"Minimum:\")\n",
    "print(z_score_dataframe.min())\n",
    "\n",
    "print(\"\\nMaximum:\")\n",
    "print(z_score_dataframe.max())\n",
    "\n",
    "print(\"\\nMean:\")\n",
    "print(z_score_dataframe.mean())\n",
    "\n",
    "print(\"\\nStandard deviation:\")\n",
    "print(z_score_dataframe.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a normalization by decimal scaling\n",
    "def normalization_by_decimal_scaling(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the normalization_by_decimal_scaling\n",
    "decimal_scaling_dataframe = normalization_by_decimal_scaling(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a normalization by decimal scaling\n",
    "def normalization_by_decimal_scaling(dataframe_to_normalize):\n",
    "    # We need to copy the dataframe_to_normalize to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_normalize.copy()\n",
    "\n",
    "    # We need to normalize each column individually\n",
    "    for column in dataframe.columns:\n",
    "        # Find k\n",
    "        k = 0\n",
    "        while dataframe[column].abs().max() / (10 ** k) >= 1:\n",
    "            k += 1\n",
    "\n",
    "        # Compute the normalization of the column\n",
    "        dataframe[column] = dataframe[column] / (10 ** k)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the normalization_by_decimal_scaling\n",
    "decimal_scaling_dataframe = normalization_by_decimal_scaling(\n",
    "    order_dataframe[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation of the decimal_scaling_dataframe\n",
    "print(\"Minimum:\")\n",
    "print(decimal_scaling_dataframe.min())\n",
    "\n",
    "print(\"\\nMaximum:\")\n",
    "print(decimal_scaling_dataframe.max())\n",
    "\n",
    "print(\"\\nMean:\")\n",
    "print(decimal_scaling_dataframe.mean())\n",
    "\n",
    "print(\"\\nStandard deviation:\")\n",
    "print(decimal_scaling_dataframe.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "It can be clearly seen that not all normalization methods lead to the same result. It is therefore always important to consider which normalization method best serves your purpose. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Consider when the various normalization methods presented might be beneficial.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "- <b>Min-max normalization:</b><br />\n",
    "Min-max normalization is advantageous when values must be secured in a fixed interval and this interval shall be used as good as possible. For example, in certain deep learning methods it is essential that values lie in the value range [0, 1] in order to avoid incorrect results.\n",
    "\n",
    "- <b>Z-score normalization:</b><br />\n",
    "The goal of Z-score normalization (also called standardization) is not to bring all values into a fixed range of values. In this type of normalization, the attributes are aligned in a different way. Thus, the Z-Score normalization achieves that the mean of all attributes is as close as possible to 0 and the attribute values have a standard deviation of 1 to each other. \n",
    "\n",
    "- <b>Normalization by decimal scaling:</b><br />\n",
    "Although normalization by decimal scaling assures the user that all output values are in the value range [-1, -1], it rarely actually uses this range (see example). The advantage compared to min-max normalization is that normalization is not done with arbitrary divisors, but with a power of ten. Since we humans are used to the decimal system, the connection between value and normalized value is easier to recognize. (Min-Max normalization: 19953.6 becomes 1.0 - Normalization by decimal scaling: 19953.6 becomes 0.199536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Discretization\n",
    "\n",
    "Another commonly used method is discretization. This is used to convert a continuous attribute into an attribute with discrete values. Of course, this method is best demonstrated on an attribute that is as continuous as possible, which is why we take a look at the new currency_rate_dataframe, which represents the USD to EUR exchange rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Print the head of currency_rate_dataframe\n",
    "currency_rate_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_dataframe.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In Knowledge Discovery, with such continuous attributes, it often makes more sense to divide the attribute values into \"groups\", since you can then work with fewer different values. Graphically, we have also already applied a method in Part One that does just that.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Draw a histogram with five bins for the AverageRate and EndOfDayRate attributes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw a histogram for AverageRate and EndOfDayRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw a histogram for AverageRate and EndOfDayRate\n",
    "currency_rate_dataframe.hist(bins=5, rwidth=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Although the histogram analysis divides the individual values into different groups and thus basically leads to a discretization of the data, it is not possible to map the temporal relationship of our data set with it. \n",
    "\n",
    "An alternative, with whose assistance the temporal can be maintained, is the Binning method. The simplest variant is to divide the range of values into several bins of the same interval size, similar to the histogram analysis (equal-width partitioning). This is possible in Pandas via the cut function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Use cut() to distribute the attribute values of AverageRate into five bins with equal width. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.cut.html\">Pandas documentation</a>)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal width\n",
    "pd.cut(currency_rate_dataframe[\"AverageRate\"], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Find out how to interpret the interval notation used in the new attribute values and what boundaries each of the five bins has. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.cut.html\">Pandas documentation</a>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<u>Interval notation:</u>\n",
    "\n",
    "The interval notation used in the new attribute values can be read as \"lower limit, upper limit\", where the brackets indicate whether the limit is to be understood as inclusive (\"[\" or \"]\") or exclusive (\"(\" or )\"). I.e. whether the limit is part of the interval (inclusive), or whether the interval begins shortly after/before the limit (exclusive).\n",
    "\n",
    "<u>Boundaries:</u>\n",
    "\n",
    "- <b>Bin 1:</b> From 0.961 (excl.) to 1.01 (inkl.)\n",
    "- <b>Bin 2:</b> From 1.01 (excl.) to 1.06 (inkl.)\n",
    "- <b>Bin 3:</b> From 1.06 (excl.) to 1.109 (inkl.)\n",
    "- <b>Bin 4:</b> From 1.109 (excl.) to 1.158 (inkl.)\n",
    "- <b>Bin 5:</b> From 1.158 (excl.) to 1.208 (inkl.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Besides equal-width partioning, Pandas also supports equal-depth partioning. I.e. not to set the bins so that each interval is the same size, but that each bin contains approximately the same number of values. This is done with the qcut function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Use qcut() to distribute the attribute values of AverageRate into five bins with equal depth. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.qcut.html\">Pandas documentation</a>)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal depth\n",
    "pd.qcut(currency_rate_dataframe[\"AverageRate\"], 5)\n",
    "\n",
    "# Note: In this case your have to set the number of quantiles not bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The disadvantage of this pure partitioning is that the attribute values are no longer purely numerical attributes and thus further processing is only possible to a limited extent. Therefore, one or two representative values are often selected for each bin, to which all values within the bin are smoothed.\n",
    "\n",
    "In the lecture the variant smoothing by bin means was presented, which we will now have a look at here. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Implement a method for smoothing by bin means with equal width partitioning by completing the following program skeleton. You may of course use cut(), but you don't have to. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a method for smoothing by bin means with equal width partitioning\n",
    "def smoothing_by_bin_means_with_equal_width_part(dataframe_to_smooth, bins=10):\n",
    "    # We need to copy the dataframe_to_smooth to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_smooth.copy()\n",
    "\n",
    "    # Smooth every column\n",
    "    for column in dataframe.columns:\n",
    "\n",
    "        # ...\n",
    "        continue\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the smoothing_by_bin_means\n",
    "currency_rate_smoothed_dataframe = smoothing_by_bin_means_with_equal_width_part(\n",
    "    currency_rate_dataframe, 5\n",
    ")\n",
    "currency_rate_smoothed_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a method for smoothing by bin means with equal width partitioning\n",
    "def smoothing_by_bin_means_with_equal_width_part(dataframe_to_smooth, bins=10):\n",
    "    # We need to copy the dataframe_to_smooth to avoid overriding content in the exisiting dataframe\n",
    "    dataframe = dataframe_to_smooth.copy()\n",
    "\n",
    "    # Smooth every column\n",
    "    for column in dataframe.columns:\n",
    "        # Calculate the bin affiliations with cut()\n",
    "        bin_affiliations = pd.cut(currency_rate_dataframe[column], bins, labels=False)\n",
    "\n",
    "        # For each bin\n",
    "        for bin in range(bins):\n",
    "            # Set every bin to the match the mean of this bin\n",
    "            dataframe[bin_affiliations == bin] = dataframe[\n",
    "                bin_affiliations == bin\n",
    "            ].mean()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# Apply the smoothing_by_bin_means\n",
    "currency_rate_smoothed_dataframe = smoothing_by_bin_means_with_equal_width_part(\n",
    "    currency_rate_dataframe, 5\n",
    ")\n",
    "currency_rate_smoothed_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_smoothed_dataframe.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In addition to histogram analysis and binning, there are other methods of discretization. One of them - clustering - will be covered later in the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Data reduction\n",
    "\n",
    "Another important part of preprocessing can be to reduce the amount of data to be analyzed. One focus in the lecture was Principal Component Analysis, which we now want to look at in practice.\n",
    "\n",
    "Subject of analysis is still the currency_rate_dataframe, where the AverageRate and the EndOfDayRate are visibly the same. It is therefore to be expected that a large part of the redundancy can be eliminated by the PCA.\n",
    "\n",
    "In order to generate a deep understanding on PCA, you will now first apply the steps presented in the lecture one by one yourself before resorting to an encapsulating function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Standardize the currency_rate_dataframe to ensure that all attributes are included in the analysis to the same extent. (Hint: You might want to use one of your previous defined functions) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize the currency_rate_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize the currency_rate_dataframe\n",
    "standardized_currency_rate_dataframe = z_score_normalization(currency_rate_dataframe)\n",
    "\n",
    "# Although not part of the question, it is usually helpful to display the result of the calculation\n",
    "standardized_currency_rate_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Calculate the covariance matrix for the standardized dataframe. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html\">Pandas documentation</a>) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix\n",
    "covariance_matrix = standardized_currency_rate_dataframe.cov()\n",
    "\n",
    "# Display the covariance matrix\n",
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Calculate the associated eigenvalues and eigenvectors. (Help: <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html\">Numpy documentation</a>) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the associated eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the associated eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Display the eigenvalues and eigenvectors\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Calculate the percentage of information per eigenvector. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of information per eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of information per eigenvector\n",
    "relative_information_share = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Print the information\n",
    "print(\n",
    "    \"First eigenvektor: approx. {0:.0f}% (Exact: \".format(\n",
    "        relative_information_share[0] * 100\n",
    "    )\n",
    "    + str(relative_information_share[0])\n",
    "    + \")\"\n",
    ")\n",
    "print(\n",
    "    \"Second eigenvektor: approx. {0:.0f}% (Exact: \".format(\n",
    "        relative_information_share[1] * 100\n",
    "    )\n",
    "    + str(relative_information_share[1])\n",
    "    + \")\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Select the feature matrix so that the transformation preserves at least 80% of the information contained in the standardized currency_rate_dataframe. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Select the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Select the feature matrix\n",
    "# The first eigenvector contains nearly all information => select only that one\n",
    "feature_matrix = eigenvectors[1]\n",
    "\n",
    "# Print the feature_matrix\n",
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Perform the transformation of the standardized data frame using the feature matrix and display the result. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html\">Pandas documentation</a>) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform the transformation and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform the transformation\n",
    "transformated_currency_rate_dataframe = pd.DataFrame(\n",
    "    data=standardized_currency_rate_dataframe.dot(feature_matrix)\n",
    ")\n",
    "\n",
    "# Display the transformated dataframe\n",
    "transformated_currency_rate_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "PCA in this case allowed the two original attributes to be merged into one without any significant loss of information. Of course, it is very cumbersome to execute PCA step by step manually each time, which is why PCA is also included in some ML frameworks, such as scikit learn. \n",
    "\n",
    "It is very important to know exactly what the framework does for you and what it does not. For example, the PCA within scikit learn does not include standardization, although feature scaling <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\">is strongly recommended</a> by the framework itself as a preceding step.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> Use the PCA from scikit learn to transform the standardized currency_rate_dataframe a second time. This time you may assume that only one component is expected to be used as a result. Display the result. (Help: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">Scikit learn documentation</a>) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the PCA from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a PCA object with sklearn\n",
    "pca = sklearn.decomposition.PCA(n_components=1)\n",
    "\n",
    "# Compute the principal components for the standardized_currency_rate_dataframe\n",
    "transformated_currency_rate_dataframe = pd.DataFrame(\n",
    "    data=pca.fit_transform(standardized_currency_rate_dataframe),\n",
    "    index=standardized_currency_rate_dataframe.index,\n",
    ")\n",
    "\n",
    "# Display the transformated dataframe\n",
    "transformated_currency_rate_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task:</b> It may be that the results of the manual PCA and the PCA per scikit learn differ. Consider why both results can be correct, even if they differ. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "In PCA, an important step is the determination of the eigenvalues and eigenvectors. This step is done by solving a system of equations for which there may be several solutions. In particular with previous standardization it can occur that two solutions are simply mirrored, whereby a likewise mirrored result in the transformed data set can be explained"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
