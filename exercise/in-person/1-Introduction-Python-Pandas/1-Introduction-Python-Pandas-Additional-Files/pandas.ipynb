{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Get to know pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is an open-source library providing intuitive data structures for data analysis, data transformation, cleaning, and, with the help of matplotlib, simple data visualizations.\n",
    "\n",
    "Most pandas' functionality is based on the numpy library, which is a optimized library providing efficient data structures and mathematical functions.\n",
    "\n",
    "For all tasks within this JupyterNotebook refer to the extensive [user guide](https://pandas.pydata.org/docs/user_guide/index.html) and [API](https://pandas.pydata.org/docs/reference/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pandas simply import it like in the following code cell. Typically, this library is aliased with `pd`. It became the de-facto standard to import it with this alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important datastructure within pandas are pandas `Series`. \n",
    "\n",
    "They represent a one-dimensional data structure utilized to store a sequence of data (for example, a single column of a table).\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 1**:\n",
    "    \n",
    "Create a pandas `Series` using the variable `lectures`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "lectures = [\n",
    "    \"Course Introduction\",\n",
    "    \"KDD Introduction\",\n",
    "    \"Getting To Know Your Data\",\n",
    "    \"Data Preprocessing\",\n",
    "    \"OLAP\",\n",
    "    \"Frequent Pattern\",\n",
    "    \"Classification\",\n",
    "    \"Cluster\",\n",
    "    \"Outlier\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lectures = [\n",
    "    \"Course Introduction\",\n",
    "    \"KDD Introduction\",\n",
    "    \"Getting To Know Your Data\",\n",
    "    \"Data Preprocessing\",\n",
    "    \"OLAP\",\n",
    "    \"Frequent Pattern\",\n",
    "    \"Classification\",\n",
    "    \"Cluster\",\n",
    "    \"Outlier\",\n",
    "]\n",
    "\n",
    "lecture_series = pd.Series(lectures)\n",
    "lecture_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another significant data structure are the two-dimensional pandas `DataFrames`.\n",
    "\n",
    "These are typically used to represent tables and internally employ pandas `Series` to depict columns.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Create a pandas `DataFrame` using the `Series` you have created. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df = pd.DataFrame(lecture_series, columns=[\"Lecture Name\"])\n",
    "lecture_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a `DataFrame`is two-dimensional you can also add more than one column.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 3:**\n",
    "    \n",
    "Add the column `Status` to your `DataFrame`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "status = [True, True, False, False, False, False, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "status = [True, True, False, False, False, False, False, False, False]\n",
    "lecture_df[\"Status\"] = status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame` may hold many rows. Viewing the whole `DataFrame` at once might not always be the best idea. \n",
    "\n",
    "However, it is possible to only view the first or the last couple of rows. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 4:**\n",
    "    \n",
    "Use `head()` and `tail()` to take a look at the first and last couple of rows of your `DataFrame`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to data, it is also possible to look up fundamental information about a `DataFrame`.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 5:**\n",
    "    \n",
    "Determine the number of rows and the number of columns your `DataFrame` has.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of rows: {lecture_df.shape[0]}\",\n",
    "    f\"Number of columns: {lecture_df.shape[1]}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 6:**\n",
    "    \n",
    "Get the names of all columns your `DataFrame` has.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to select one or more specific columns.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 7:**\n",
    "    \n",
    "Select only the `Lecture Names` column of your `DataFrame`.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df[\"Lecture Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as selecting individual columns is possible, one can also select rows.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 8:**\n",
    "    \n",
    "Select only the second row of your `DataFrame`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "Select all rows of your `DataFrame` starting with the third row.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df.loc[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3. Sort a `DataFrame` by Index or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame`can be sorted by its index as well as column values.\n",
    "\n",
    "### 1.2.3.1. Sort By Index\n",
    "\n",
    "Function `sort_index` sorts a `DataFrame` by its index alphanumerically. Following parameters are available (among others, refer to [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html)):\n",
    "\n",
    "- `axis`: Default `0`, meaning it sorts rows.\n",
    "- `ascending`: Default `True`, set to `False` for descending sort.\n",
    "- `inplace`: Default `False`.\n",
    "- `na_position`: Default `last`. To place NaNs (Not a Number, `NULL` in SQL) first, set to `first`.\n",
    "- `kind`: Sorting algorithm, default `quicksort`. Choose between: `quicksort`, `mergesort`, `heapsort`, `stable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get our untouched DataFrame\n",
    "df = df_original.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort descending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3.2. Sort by Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort a `DataFrame` by column, use function `sort_values`. It has the following parameters (refer to [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)):\n",
    "- `by`: Column name or list of column names to sort by.\n",
    "- `axis`: Default `0`, meaning it sorts rows.\n",
    "- `inplace`: Default `False`.\n",
    "- `na_position`: Default `last`. To place NaNs (Not a Number, `NULL` in SQL) first, set to `first`.\n",
    "- `kind`: Sorting algorithm, default `quicksort`. Choose between: `quicksort`, `mergesort`, `heapsort`, `stable`.\n",
    "- `ignore_index`: Default `False` to retain current index. Set to `True` to generate a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Lecture Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Lecture Name\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4. Statistics, Aggregation, and Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is based on numpy and thus, provides an extensive amount of mathematical functions out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from order_example import articles\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides functions to describe a distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean value of column \"PRICE\"\n",
    "articles.PRICE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median value of column \"PRICE\"\n",
    "articles.PRICE.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max value of column \"PRICE\"\n",
    "articles.PRICE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likewise, min value of column \"PRICE\"\n",
    "articles.PRICE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of column \"PRICE\"\n",
    "articles.PRICE.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of column \"PRICE\"\n",
    "articles.PRICE.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics are automatically calculated at once by `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.PRICE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply this function on the whole DataFrame\n",
    "articles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or transpose the output\n",
    "articles.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all article prices\n",
    "articles.PRICE.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum of article prices\n",
    "articles.PRICE.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values\n",
    "articles.PRICE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of unique values\n",
    "articles.PRICE.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to calculate several functions in one statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.agg({\"PRICE\": [min, max, \"nunique\", \"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also group by some column and apply a function based on this group\n",
    "articles.groupby(\"TYPE\").count().loc[:, \"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all article names in a group\n",
    "articles.groupby(\"TYPE\").agg({\"NAME\": list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article names in a group as a comma separated string\n",
    "articles.groupby(\"TYPE\").agg({\"NAME\": \", \".join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.groupby(\"TYPE\").mean().loc[:, \"PRICE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5. Content Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often than not it is desirable to change a `DataFrame`'s content. This could include either adding a new row/column or modify a single or multiple cells at once. Additionally, when having two `DataFrame`s it may be desireable to merge, join, or concatenate these `DataFrame`s.\n",
    "\n",
    "Thus, we take a look at the following:\n",
    "\n",
    "1. Add a new row/column.\n",
    "2. Modify a specific cell or multiple cells.\n",
    "3. Delete row or column.\n",
    "4. Merge or Join two `DataFrame`s.\n",
    "6. Concatenate two or more `DataFrame`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.1. Add a New Row or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting new rows or columns is possible in different ways:\n",
    "1. Select a row or column that does not yet exist and simply assign a value to it.\n",
    "2. Insert a column at a specific position with `insert(loc, column, value)`.\n",
    "3. Append new rows or columns with `concat`. Later more on this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a new row or column by selecting an index that does not yet exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Exam Relevant\"] = [False] + [True] * (df.shape[0] - 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[10, \"Lecture Name\"] = \"Unnamed Lecture\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column with `insert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = [i for i in range(42, 42 + df.shape[0])]\n",
    "df.insert(0, \"Better Number\", new_values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.2. Modify a Specific Cell or Multiple Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying cells or multiple cells at once can be achieved in various ways: \n",
    "1. using indexing (label-based or index-based), boolean masking or using a callable. For instance, set all lectures \"Done\" column to `True` with boolean masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all values of attribute \"Done\" to True when False\n",
    "df.loc[df[\"Done\"] == False, \"Done\"] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `apply` is a useful function able to apply any function along an axis of a `DataFrame`.\n",
    "3. `applymap` applies a function element-wise instead along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Lecture Name\"] = df.loc[:, \"Lecture Name\"].apply(str.upper)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_random_weight(x):\n",
    "    return (x - 1) * random.choice(range(10)) / 100\n",
    "\n",
    "\n",
    "df[\"Imaginary Weight\"] = df.loc[:, \"Number\"].apply(get_random_weight)\n",
    "# apply(get_random_weight) is short for apply(lambda x: get_random_weight(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `applymap` is similar to `apply` but is applied elementwise, meaning that it is applied on all cells unlike `apply` which is applied column- or row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_cells = df.applymap(lambda x: len(str(x)))\n",
    "length_of_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.3. Delete Row or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting rows or columns can be done in different ways:\n",
    "1. Selecting everything you want to keep and assign it to the same variable.\n",
    "2. Using function `pop`. This function removes a column from a `DataFrame` and returns the removed row/column as a `Series`.\n",
    "3. Using function `drop` to remove a row or column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"Exam Relevant\"] == True, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_numbers = df.pop(\"Better Number\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_exam_relevant = df.drop(\n",
    "    labels=[\"Exam Relevant\"],\n",
    "    axis=1,  # to drop a column. To drop a row set axis=0\n",
    "    inplace=False,  # No inplace drop, this returns the new DataFrame\n",
    ")\n",
    "df_wo_exam_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.4. Merge or Join Two `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` is a function to join two `DataFrame`s based on some keys (columns). It is similar to `JOIN` in SQL. In pandas another function exists, called `join`. The difference is the following:\n",
    "- `merge` merges or \"joins\" two `DataFrame`s based on columns or indexes.\n",
    "- `merge` can sort join keys lexicographically, add suffixes to columns in result `DataFrame`\n",
    "- Per default, `join` performs a join based on two `DataFrame`s indices.\n",
    "- `join` uses `merge` internally when joining/merging index-on-index or column(s)-on-index.\n",
    "Thus, `join` saves typing time when you want to join or merge two `DataFrame`s by their index.\n",
    "\n",
    "Both `merge` and `join` supports all SQL `JOIN`-operations:\n",
    "\n",
    "| **Method** | **SQL**            | **Description**                                       |\n",
    "|------------|--------------------|-------------------------------------------------------|\n",
    "| `left`     | `LEFT OUTER JOIN`  | Use keys from left `DataFrame` only                   |\n",
    "| `right`    | `RIGHT OUTER JOIN` | Use keys from right `DataFrame` only                  |\n",
    "| `outer`    | `FULL OUTER JOIN`  | Use union of keys on both `DataFrame`s                |\n",
    "| `inner`    | `INNER JOIN`       | Use intersection of keys from both `DataFrame`s       |\n",
    "| `cross`    | `CROSS JOIN`       | Create cartesian product of rows of both `DataFrame`s |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport, in case they have been modified\n",
    "from order_example import articles, customers, orders, order_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "\n",
    "| **Method** | **SQL**            | **Description**                                       |\n",
    "|------------|--------------------|-------------------------------------------------------|\n",
    "| `left`     | `LEFT OUTER JOIN`  | Use keys from left `DataFrame` only                   |\n",
    "| `right`    | `RIGHT OUTER JOIN` | Use keys from right `DataFrame` only                  |\n",
    "| `outer`    | `FULL OUTER JOIN`  | Use union of keys on both `DataFrame`s                |\n",
    "| `inner`    | `INNER JOIN`       | Use intersection of keys from both `DataFrame`s       |\n",
    "| `cross`    | `CROSS JOIN`       | Create cartesian product of rows of both `DataFrame`s |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily merge/join the `DataFrame`s `orders` and `customers` to add the names to each order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(orders, customers, on=\"CID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have customers that did not yet buy something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join = pd.merge(orders, customers, on=\"CID\", how=\"outer\")\n",
    "customers_no_buy = outer_join[outer_join[\"OID\"].isnull()]\n",
    "customers_no_buy[[\"CID\", \"NAME\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 4:**\n",
    "    \n",
    "Join `DataFrame`s `orders` and `order_position`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "orders_with_positions = pd.merge(orders, order_positions, on=\"OID\", how=\"inner\")\n",
    "orders_with_positions.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 5:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` and join with `articles`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "op_article = pd.merge(orders_with_positions, articles, on=\"AID\", how=\"inner\")\n",
    "op_article.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 6:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` to Calculate Sum of each Order.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "op_article[\"TOTAL_PRICE\"] = op_article.UNIT * op_article.PRICE\n",
    "order_sums = op_article.groupby(\"OID\").sum()\n",
    "order_sums[[\"TOTAL_PRICE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 7:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` and Join Customers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "order_sums = pd.merge(order_sums, customers, on=\"CID\", how=\"inner\")\n",
    "order_sums[[\"NAME\", \"TOTAL_PRICE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 8:**\n",
    "    \n",
    "What Articles Have Not Been Sold Yet?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles_not_sold = pd.merge(order_positions, articles, on=\"AID\", how=\"right\")\n",
    "articles_not_sold = articles_not_sold[articles_not_sold.OID.isna()]\n",
    "articles_not_sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# alternatively:\n",
    "articles.loc[~articles.AID.isin(order_positions.AID.unique()), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles.loc[articles.AID.isin(order_positions.AID.unique()) == False, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "What Articles Have Been Sold the Most (Units)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# reuse previous DataFrame\n",
    "op_article_sums = op_article.groupby([\"AID\", \"NAME\"]).sum()\n",
    "op_article_sums.sort_values(by=\"UNIT\", ascending=False).iloc[:3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customers have a sweet tooth as they prefer kiwis, strawberries, and water melons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "What Articles Have the Highest Revenue?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# reuse previous DataFrame\n",
    "op_article_sums.sort_values(by=\"TOTAL_PRICE\", ascending=False).iloc[:3, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.5. Concatenate Two or More `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation of two or more `DataFrame`s can be achieved - yet again - in multiple ways. In the previous chapter, we already discussed one way of accomplishing concatenation of two `DataFrame`s by means of a cross join. Another way is the use of `concatenate` which is able to concatenate two or more `DataFrame`s column- or row-wise.\n",
    "\n",
    "Let's prepare a function `get_df_with_random_values` and define two `DataFrame`s using this function: `df1`, and `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_random_values(range_number=10):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [random.choice(range(range_number)) for _ in range(range_number)]\n",
    "            for _ in range(range_number)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_df_with_random_values()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_df_with_random_values()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate along columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate along rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.6. Data Cleaning Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has means of data cleaning with functions like:\n",
    "1. Finding missing values with `isna`.\n",
    "2. Filling missing values with `fillna` or interpolate values with `interpolate`.\n",
    "   - `fillna` supports forward and backward filling\n",
    "   - `interpolate` supports various filling methods such as linear, nearest, polynomial, as well as interpolation methods from the library [SciPy](https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). \n",
    "3. Drop missing values with `dropna`.\n",
    "4. Return a boolean mask of duplicates with `duplicated`.\n",
    "5. Drop duplicate rows with `drop_duplicates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_mess = pd.DataFrame(\n",
    "    [\n",
    "        (21, 3.0, 10),\n",
    "        (24, 2.0, 8),\n",
    "        (21, 4.0, 12),\n",
    "        (21, 2.0, 11),\n",
    "        (24, 3.0, 8),\n",
    "        (23, np.nan, 9),\n",
    "        (21, 2.0, 8),\n",
    "        (20, 1.0, 11),\n",
    "        (25, 1.0, 7),\n",
    "        (24, 2.0, 12),\n",
    "        (23, np.nan, 8),\n",
    "        (27, 3.0, 7),\n",
    "        (21, 4.0, 12),\n",
    "        (23, 1.0, 11),\n",
    "        (20, np.nan, 7),\n",
    "        (28, np.nan, 12),\n",
    "        (26, 1.0, 10),\n",
    "        (21, np.nan, 10),\n",
    "        (20, 2.0, 9),\n",
    "        (27, 4.0, 7),\n",
    "    ],\n",
    "    columns=list(\"ABC\"),\n",
    ")\n",
    "df_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many NaN (Not a Number) values this DataFrame has\n",
    "df_mess.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "df_mess[\"B\"].interpolate(method=\"linear\", inplace=True)\n",
    "df_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any duplicate rows and if, which values they have. Remember, the function `duplicated` returns a boolean mask that can be used to select any rows that contains duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess[df_mess.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `drop_duplicates` can retain either the first or the last row of duplicates. Which to retain depends on the context at hand.  In this example, it does not matter which row to keep.\n",
    "\n",
    "In the case, however, this example would be a time series of, let's say, temperature and some other observed signals indexed by time (imagine this index is a time step), then these *duplicates* are none.\n",
    "\n",
    "For now, in this example let's drop duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess.drop_duplicates(inplace=True)\n",
    "# verify duplicates have been dropped\n",
    "df_mess[df_mess.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.7. Read and Write Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we pushed reading and writing data and instead created a `DataFrame` from scratch. That is, however, not feasible.\n",
    "\n",
    "Pandas offers methods to read and write a `DataFrame` from and to various data formats. Among them are, naturally, column separated files (csv), JSON files, dictionary, string, but also SQL, pickle files, latex, and markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 10:**\n",
    "    \n",
    "Write the `DataFrame`s of article, customer, order, and order positions to CSV files.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles.to_csv(\"articles.csv\", index=False)\n",
    "customers.to_csv(\"customers.csv\", index=False)\n",
    "orders.to_csv(\"orders.csv\", index=False)\n",
    "order_positions.to_csv(\"order_positions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.8. Database Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas and its `DataFrame` function `to_sql` it is easy to extract data from database tables or write to a table. For this, `to_sql` requires a database engine. More specifically, a database engine from `sqlalchemy`. Once established and connected to a database, tables can be created, written to, or appended. Likewise, with `from_sql` extracts data from a table and returns it as a `DataFrame`.\n",
    "\n",
    "`sqlalchemy` is an Object Relational Mapper (ORM) that maps (Python) objects, more particularly `DataFrame`s, to tables. For a quick start in ORM in `sqlalchemy` refer to its [documentation](https://docs.sqlalchemy.org/en/14/orm/quickstart.html).\n",
    "\n",
    "Python supports SQLite databases out of the box without any additional packages. Check out Python's [documentation](https://docs.python.org/3/library/sqlite3.html) on how to use SQLite.\n",
    "\n",
    "Back to `sqlalchemy`:\n",
    "In order to use `sqlalchemy` with pandas, we first need to import this library and then create a database engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///orders.db\", echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_sql(name=\"article\", con=engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's SQLite interface is not much different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_engine = sqlite3.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python's internal SQLite support it is possible to create a in-memory database. You can, however, still persist your database by specifying a file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read this table with `read_sql`. With the parameter `sql`, we can query any table, join tables as we go, or just specify one table name as a short hand for `SELECT * FROM table_name;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read article from database\n",
    "articles_db = pd.read_sql(sql=\"article\", con=engine)\n",
    "articles_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetables = pd.read_sql(\n",
    "    sql=\"SELECT * FROM article WHERE TYPE='Vegetable';\", con=engine\n",
    ")\n",
    "vegetables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 11:**\n",
    "    \n",
    "Write all other `DataFrame`s to the database and access them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "customers.to_sql(name=\"customer\", con=engine, index=False)\n",
    "orders.to_sql(name=\"order\", con=engine, index=False)\n",
    "order_positions.to_sql(name=\"order_position\", con=engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "customers_db = pd.read_sql(sql=\"customer\", con=engine)\n",
    "customers_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 12:**\n",
    "    \n",
    "Write a SQL query that returns the top three articles sold unit-wise and print out the returning DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "most_sold_articles = pd.read_sql(\n",
    "    sql=\"\"\"SELECT AID, NAME, sum(UNIT) as UNIT\n",
    "FROM order_position \n",
    "JOIN article using(AID)\n",
    "GROUP BY AID, NAME\n",
    "ORDER BY UNIT DESC\n",
    "LIMIT 3;\"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "most_sold_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 13:**\n",
    "    \n",
    "Likewise, write a SQL query that returns the top three customers revenue-wise and print out the returning DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "In case you named your table `order`, keep in mind that this word is a reserved keyword and thus, must be quoted. Alternatively, you can name this table differently like `orders` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "best_customers = pd.read_sql(\n",
    "    sql=\"\"\"SELECT CID, customer.NAME, sum(UNIT * PRICE) as REVENUE\n",
    "FROM order_position\n",
    "JOIN 'order' using(OID)\n",
    "JOIN article using(AID)\n",
    "JOIN customer using(CID)\n",
    "GROUP BY CID, customer.NAME\n",
    "ORDER BY REVENUE DESC\n",
    "LIMIT 3;\"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "best_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.9. What's More?\n",
    "Check out the extensive [user guide](https://pandas.pydata.org/docs/user_guide/index.html#user-guide) for more functionality."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
