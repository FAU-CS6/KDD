{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 2. Data Analysis & Preprocessing\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Data Analysis & Preprocessing*.\\\n",
    "The series itself includes practical exercises for lectures *3. Getting to Know Your Data* and *4. Data Preprocessing*. \n",
    "\n",
    "This exercise series is divided into three parts. There will be one exercise session per part (= one part per week):\n",
    "\n",
    "- **2.1.** [Getting to Know Your Data](./2.1-Getting-to-Know-Your-Data.ipynb) (*notebook of the week before last*)\n",
    "- **2.2.** [Preprocessing - Data Cleaning & Data Integration](./2.2-Preprocessing-Data-Cleaning-and-Integration.ipynb) (*next weeks notebook*) (*last weeks notebook*)\n",
    "- **2.3.** Preprocessing - Data Reduction, Data Transformation & Data Discretization (*this notebook*)\n",
    "    - **2.3.1.** [Normalization](#2.3.1.-Normalization)\n",
    "    - **2.3.2.** [Discretization](#2.3.2.-Discretization)\n",
    "    - **2.3.3.** [Data Reduction](#2.3.3.-Data-Reduction)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "Work on the respective part yourself **BEFORE** each exercise session. The exercise session is **NOT** intended to take a first look at the exercise sheet, but to solve problems students had while preparing the exercise sheet beforehand.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## 2.3. Preprocessing - Data Reduction, Data Transformation & Data Discretization\n",
    "\n",
    "In this part you will apply the theoretical knowledge gained in the second part of the lecture *4. Data Preprocessing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import os\n",
    "import urllib.request\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the clean DataFrame(s)\n",
    "# Order DataFrame\n",
    "order_df = pd.read_sql_query(\n",
    "    \"SELECT p.ProductID,p.Name,p.ProductNumber,p.MakeFlag,p.FinishedGoodsFlag,p.Color,p.SafetyStockLevel,\"\n",
    "    \"p.ReorderPoint,p.StandardCost,p.ListPrice,p.Size,p.SizeUnitMeasureCode,p.WeightUnitMeasureCode,p.Weight,\"\n",
    "    \"p.DaysToManufacture,p.ProductLine,p.Class,p.Style,p.ProductSubcategoryID,p.ProductModelID,p.SellStartDate,\"\n",
    "    \"p.SellEndDate,p.DiscontinuedDate,d.PurchaseOrderID,d.PurchaseOrderDetailID,d.DueDate,d.OrderQty,d.ProductID,\"\n",
    "    \"d.UnitPrice,d.ReceivedQty,d.RejectedQty,h.RevisionNumber,h.Status,h.EmployeeID,h.VendorID,h.ShipMethodID,\"\n",
    "    \"h.OrderDate,h.ShipDate,h.SubTotal,h.TaxAmt,h.Freight,h.TotalDue,e.NationalIDNumber,e.LoginID,e.OrganizationNode,\"\n",
    "    \"e.JobTitle,e.BirthDate,e.MaritalStatus,e.Gender,e.HireDate,e.SalariedFlag,e.VacationHours,e.SickLeaveHours,\"\n",
    "    \"e.CurrentFlag,r.PersonType,r.NameStyle,r.Title,r.FirstName,r.MiddleName,r.LastName,r.Suffix,r.EmailPromotion,\"\n",
    "    \"r.AdditionalContactInfo,r.Demographics \"\n",
    "    \"FROM Product p \"\n",
    "    \"JOIN PurchaseOrderDetail d ON p.ProductID = d.ProductID \"\n",
    "    \"JOIN PurchaseOrderHeader h ON d.PurchaseOrderID = h.PurchaseOrderID \"\n",
    "    \"JOIN Employee e ON h.EmployeeID = e.BusinessEntityID \"\n",
    "    \"JOIN Person r ON e.BusinessEntityID = r.BusinessEntityID\",\n",
    "    connection,\n",
    "    index_col=\"PurchaseOrderDetailID\",\n",
    ")\n",
    "\n",
    "# CurrencyRate DataFrame\n",
    "currency_rate_df = pd.read_sql_query(\n",
    "    \"SELECT STRFTIME('%Y-%m-%d', CurrencyRateDate) AS CurrencyRateDate,AverageRate,EndOfDayRate \"\n",
    "    \"FROM CurrencyRate \"\n",
    "    \"WHERE FromCurrencyCode='USD' AND ToCurrencyCode='EUR'\",\n",
    "    connection,\n",
    "    index_col=\"CurrencyRateDate\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### 2.3.1. Normalization\n",
    "\n",
    "One method introduced in the lecture and frequently used in Data Science is normalization. In order to apply this practically, we will first take a look at a part of the `order_df` already known from Part One. More precisely, we are looking at some numeric attributes from `order_df`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1:**\n",
    "    \n",
    "Display the head of the attributes `SubTotal`, `Freight` and `OrderQty` from `order_df`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the head of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the head of SubTotal, Freight and OrderQty\n",
    "order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Display the minimum, maximum, mean, and standard deviation of `SubTotal`, `Freight` and `OrderQty`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean, and standard deviation of SubTotal, Freight and OrderQty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean, and standard deviation of SubTotal, Freight and OrderQty\n",
    "order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]].agg([\"min\", \"max\", \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As can be seen, the three attributes differ significantly with respect to their distribution of values posing a hindrance for some knowledge discovery tasks. To alleviate this problem, normalization is employed to scale attribute values to a smaller range.\n",
    "\n",
    "In the lecture you were introduced to three different variants of normalization: min-max normalization, z-score normalization, and normalization by decimal scaling.\n",
    "\n",
    "Below you can see the implementation of one of the normalization methods for the attributes `SubTotal`, `Freight` and `OrderQty`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute a \"mystery\" normalization in a \"Pythonic\" way\n",
    "def mystery_normalization(df):\n",
    "    # Compute the normalization with a sinple formula\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "\n",
    "# Apply previously defined function\n",
    "mystery_normalization_df = mystery_normalization(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "mystery_normalization_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 3:** \n",
    "    \n",
    "Determine whether the above function `mystery_normalization` is an implementation of min-max normalization, z-score normalization, or normalization by decimal scaling.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "**The function is an implementation of:**\n",
    "1. [ ] Min-max normalization (for the interval [0, 1])\n",
    "2. [ ] Z-score normalization\n",
    "3. [ ] Normalization by decimal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**The function is an implementation of:**\n",
    "1. [ ] Min-max normalization (for the interval [0, 1])\n",
    "2. [X] Z-score normalization\n",
    "3. [ ] Normalization by decimal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 4:**\n",
    "    \n",
    "Implement a function for each of the three normalization methods you got to know. (You may, of course, reuse the above code when you work on the corresponding function).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement min-max normalization for the interval [0, 1]\n",
    "def min_max_normalization(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply min-max normalization\n",
    "min_max_df = min_max_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "min_max_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 1: Min-max normalization for the interval [0, 1] with for-loop\n",
    "def min_max_normalization(df):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    normalized = df.copy()\n",
    "\n",
    "    # Normalize each column individually\n",
    "    for column in normalized.columns:\n",
    "        normalized[column] = (df[column] - df[column].min()) / (\n",
    "            df[column].max() - df[column].min()\n",
    "        )\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Apply min-max normalization\n",
    "min_max_df = min_max_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "min_max_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 2: \"Pythonic\" min-max normalization for the interval [0, 1]\n",
    "def min_max_normalization(df):\n",
    "    # Compute the min-max normalization with a simple formula\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "\n",
    "# Apply min-max normalization\n",
    "min_max_df = min_max_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "min_max_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean, and standard deviation values of min_max_df\n",
    "min_max_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement z-score normalization\n",
    "def z_score_normalization(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply z-score normalization\n",
    "z_score_df = z_score_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "z_score_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 1: Z-score normalization with for-loop\n",
    "def z_score_normalization(df):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    normalized = df.copy()\n",
    "\n",
    "    # Normalize each column individually\n",
    "    for column in normalized.columns:\n",
    "        normalized[column] = (\n",
    "            normalized[column] - normalized[column].mean()\n",
    "        ) / normalized[column].std()\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Apply z-score normalization\n",
    "z_score_df = z_score_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "z_score_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 2: \"Pythonic\" z-score normalization\n",
    "def z_score_normalization(df):\n",
    "    # Compute the z-score normalization with a simple formula\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "\n",
    "# Apply z-score normalization\n",
    "z_score_df = z_score_normalization(order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]])\n",
    "z_score_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation values of z_score_df\n",
    "z_score_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement normalization by decimal scaling\n",
    "def normalization_by_decimal_scaling(df):\n",
    "    # ...\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply normalization_by_decimal_scaling\n",
    "decimal_scaling_df = normalization_by_decimal_scaling(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 1: Normalization by decimal scaling with for-loop\n",
    "def normalization_by_decimal_scaling(df):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    normalized = df.copy()\n",
    "\n",
    "    # Normalize each column individually\n",
    "    for column in normalized.columns:\n",
    "        # Find k\n",
    "        k = 0\n",
    "        while normalized[column].abs().max() / (10**k) >= 1:\n",
    "            k += 1\n",
    "\n",
    "        # Compute normalization of the column\n",
    "        normalized[column] = normalized[column] / (10**k)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Apply normalization_by_decimal_scaling\n",
    "decimal_scaling_df = normalization_by_decimal_scaling(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution 2: \"Pythonic\" normalization by decimal scaling method\n",
    "def normalization_by_decimal_scaling(df):\n",
    "    # Compute the decimal scaling normalization with a simple formula\n",
    "    return df / 10 ** (np.ceil(np.log10(df.abs().max())))\n",
    "\n",
    "\n",
    "# Apply normalization_by_decimal_scaling\n",
    "decimal_scaling_df = normalization_by_decimal_scaling(\n",
    "    order_df[[\"SubTotal\", \"Freight\", \"OrderQty\"]]\n",
    ")\n",
    "decimal_scaling_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the minimum, maximum, mean and standard deviation values of decimal_scaling_df\n",
    "decimal_scaling_df.agg([min, max, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Note that each normalization results in different scaled values. It is therefore important to consider which normalization method best serves your purpose. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 5:**\n",
    "    \n",
    "Consider when the various normalization methods presented might be beneficial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "- **Min-max normalization:**\n",
    "\n",
    "Min-max normalization is advantageous when values must be secured in a fixed interval and this interval shall be used as good as possible. For example, in certain deep learning methods it is essential that values lie in the value range [0, 1] in order to avoid incorrect results.\n",
    "\n",
    "- **Z-score normalization:**\n",
    "\n",
    "The goal of z-score normalization (also called standardization) is not to bring all values into a fixed range of values. In this type of normalization, the attributes are aligned in a different way. Thus, the Z-Score normalization achieves that the mean of all attributes is as close as possible to 0 and the attribute values have a standard deviation of 1 to each other. \n",
    "\n",
    "- **Normalization by decimal scaling:**\n",
    "\n",
    "Although normalization by decimal scaling assures the user that all output values are in the value range [-1, -1], it rarely actually uses this range (see example). The advantage compared to min-max normalization is that normalization is not done with arbitrary divisors, but with a power of ten. Since we humans are used to the decimal system, the connection between value and normalized value is easier to recognize. (Min-Max normalization: 19953.6 becomes 1.0 - Normalization by decimal scaling: 19953.6 becomes 0.199536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### 2.3.2. Discretization\n",
    "\n",
    "Another commonly used method is discretization. This is used to convert a continuous values to discrete values. This method is best demonstrated on a continuous attribute which is why we take a look at the `currency_rate_df` which represents exchange rates from USD to EUR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Print the head of currency_rate_df\n",
    "currency_rate_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_df.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Sometimes it is desirable to divide attribute values into *groups* resulting in fewer values that are easier to handle. Graphically, we have already applied a method in Part One that does just that: Histogram plot.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 6:**\n",
    "    \n",
    "Draw a histogram with five bins for the attributes `AverageRate` and `EndOfDayRate`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw a histogram for AverageRate and EndOfDayRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw a histogram for AverageRate and EndOfDayRate\n",
    "currency_rate_df.hist(bins=5, rwidth=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Although histogram analysis divides individual values into different groups and thus discretizes data, it completely ignores the temporal aspect. More specifically, both attributes `AverageRate` and `EndOfDayRate` constitute time series, i. e. data is indexed by time, and binning data values ignores this time dependency. \n",
    "\n",
    "Alternatively, data can be binned and thus discretize by maintaining the temporal nature by partitioning the data values to equal-width bins. This is possible in pandas via the fucntion `cut`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 7:**\n",
    "    \n",
    "Use `cut()` to distribute the attribute values of `AverageRate` into five bins with equal width. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.cut.html))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute the attribute values of AverageRate into bins with equal width\n",
    "pd.cut(currency_rate_df[\"AverageRate\"], bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 8:**\n",
    "    \n",
    "Find out how to interpret the interval notation used in the new attribute values and what boundaries each of the five bins has. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Interval notation:** Interval notation can be read as \"lower limit, upper limit\", where the brackets indicate whether a value is included (\"[\" or \"]\") or excluded (\"(\" or )\") in the limit.\n",
    "\n",
    "**Boundaries:**\n",
    "\n",
    "- **Bin 1:** From 0.961 (excl.) to 1.01 (inkl.)\n",
    "- **Bin 2:** From 1.01 (excl.) to 1.06 (inkl.)\n",
    "- **Bin 3:** From 1.06 (excl.) to 1.109 (inkl.)\n",
    "- **Bin 4:** From 1.109 (excl.) to 1.158 (inkl.)\n",
    "- **Bin 5:** From 1.158 (excl.) to 1.208 (inkl.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Besides equal-width partioning, pandas also supports equal-depth partioning where each bin contains approximately the same *number of values*. This is done with the function `qcut`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "Use `qcut()` to arrange attribute values of AverageRate into five bins with equal depth. (Help: <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.qcut.html\">pandas documentation</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute attribute values of AverageRate into bins with equal depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Distribute attribute values of AverageRate into bins with equal depth\n",
    "pd.qcut(currency_rate_df[\"AverageRate\"], q=5)\n",
    "\n",
    "# Note: Parameter q specifies the number of quantiles, not the number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The disadvantage of this \"pure\" partitioning is that the attribute values are no longer purely numerical attributes and thus further processing is only possible to a limited extent. Therefore, one or two representative values are often selected for each bin, to which all values within the bin are smoothed.\n",
    "\n",
    "In the lecture the variant smoothing by bin means was presented, which we will now have a look at here. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 10:**\n",
    "    \n",
    "Implement a method for smoothing by bin means with equal width partitioning by completing the following program skeleton. You may use `cut()`, but you don't have to. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a method for smoothing by bin means with equal width partitioning\n",
    "def smoothing_by_bin_means_with_equal_width_part(df, bins=10):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    smoothed = df.copy()\n",
    "\n",
    "    # Smooth every column\n",
    "    for column in smoothed.columns:\n",
    "        # ...\n",
    "        continue\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "# Apply smoothing_by_bin_means\n",
    "currency_rate_smoothed_df = smoothing_by_bin_means_with_equal_width_part(\n",
    "    currency_rate_df, 5\n",
    ")\n",
    "currency_rate_smoothed_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a method for smoothing by bin means with equal width partitioning\n",
    "def smoothing_by_bin_means_with_equal_width_part(dataframe_to_smooth, bins=10):\n",
    "    # Create a copy to avoid overriding original content\n",
    "    smoothed = dataframe_to_smooth.copy()\n",
    "\n",
    "    # Smooth every column\n",
    "    for column in smoothed.columns:\n",
    "        # Calculate the bin affiliations with cut()\n",
    "        bin_affiliations = pd.cut(dataframe_to_smooth[column], bins, labels=False)\n",
    "\n",
    "        # For each bin\n",
    "        for bin in range(bins):\n",
    "            # Set every bin to the mean of the matched bin\n",
    "            smoothed[bin_affiliations == bin] = smoothed[bin_affiliations == bin].mean()\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "# Apply smoothing_by_bin_means\n",
    "currency_rate_smoothed_df = smoothing_by_bin_means_with_equal_width_part(\n",
    "    currency_rate_df, 5\n",
    ")\n",
    "currency_rate_smoothed_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Draw the progression of the two rates over time\n",
    "currency_rate_smoothed_df.plot(subplots=True)\n",
    "plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In addition to histogram analysis and binning, there are other methods of discretization. One of them - clustering - will be covered later in the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### 2.3.3. Data Reduction\n",
    "\n",
    "Another important part of data preprocessing is to reduce the dimentionality of data. One focus in the lecture was Principal Component Analysis (PCA), which we now utilize.\n",
    "\n",
    "Subject of analysis is still the `currency_rate_df` where `AverageRate` and `EndOfDayRate` are visibly the same. It is, therefore, expected that a large part of the redundancy can be eliminated by PCA.\n",
    "\n",
    "In order to generate a deep understanding on PCA, you will first apply the steps presented in the lecture before resorting to a library function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 11:**\n",
    "    \n",
    "Standardize the `currency_rate_df` to ensure that all attributes are included in the analysis to the same extent. (Hint: You may use one of your previous defined functions) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize currency_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Standardize currency_rate_df\n",
    "standardized_currency_rate_df = z_score_normalization(currency_rate_df)\n",
    "\n",
    "# Although not part of the question, it is usually helpful to display the result of the calculation\n",
    "standardized_currency_rate_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 12:**\n",
    "    \n",
    "Calculate the covariance matrix for the standardized `DataFrame`. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate covariance matrix\n",
    "covariance_matrix = standardized_currency_rate_df.cov()\n",
    "\n",
    "# Display the covariance matrix\n",
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 13:**\n",
    "    \n",
    "Calculate the associated eigenvalues and eigenvectors. (Help: [NumPy documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the associated eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the associated eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Display the eigenvalues and eigenvectors\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 14:**\n",
    "    \n",
    "Calculate the percentage of information per eigenvector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of information per eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of information per eigenvector\n",
    "relative_information_share = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Print the information\n",
    "print(\n",
    "    \"First eigenvektor: approx. {0:.0f}% (Exact: \".format(\n",
    "        relative_information_share[0] * 100\n",
    "    )\n",
    "    + str(relative_information_share[0])\n",
    "    + \")\"\n",
    ")\n",
    "print(\n",
    "    \"Second eigenvektor: approx. {0:.0f}% (Exact: \".format(\n",
    "        relative_information_share[1] * 100\n",
    "    )\n",
    "    + str(relative_information_share[1])\n",
    "    + \")\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 15:**\n",
    "    \n",
    "Select the feature matrix so that the transformation preserves at least 80% of the information contained in the standardized `currency_rate_df`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Select the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Select the feature matrix\n",
    "# The first eigenvector contains nearly all information => select only that one\n",
    "feature_matrix = eigenvectors[1]\n",
    "\n",
    "# Print the feature_matrix\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 16:**\n",
    "    \n",
    "Perform the transformation of the standardized data frame using the feature matrix and display the result. (Help: [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html)) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform the transformation and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform the transformation\n",
    "transformated_currency_rate_df = pd.DataFrame(\n",
    "    data=standardized_currency_rate_df.dot(feature_matrix)\n",
    ")\n",
    "\n",
    "# Display the transformated DataFrame\n",
    "transformated_currency_rate_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In this exercise, PCA allowed the two original attributes to be merged into one without any significant loss of information. Of course, it is very cumbersome to execute PCA step by step manually each time which is why PCA is also included in some ML frameworks such as scikit-learn. \n",
    "\n",
    "It is very important to know exactly what the framework does for you and what it does not. For example, scikit-learn's PCA does not include standardization, although feature scaling [is strongly recommended](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) by the framework itself as a preceding step.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 17:**\n",
    "    \n",
    "Use scikit-learn's PCA to transform the standardized `currency_rate_df` a second time. This time you may assume that only one component is expected to be used as a result. Display the result. (Help: [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use PCA from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = sklearn.decomposition.PCA(n_components=1)\n",
    "\n",
    "# Compute the principal components for the standardized_currency_rate_df\n",
    "transformated_currency_rate_df = pd.DataFrame(\n",
    "    data=pca.fit_transform(standardized_currency_rate_df),\n",
    "    index=standardized_currency_rate_df.index,\n",
    ")\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "transformated_currency_rate_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 18:**\n",
    "    \n",
    "It may be that the results of the manual PCA and scikit-learn's PCA differ. Explain why both results may well be correct. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "One of the steps in PCA is the determination of eigenvalues and eigenvectors. This step is done by solving a system of equations that may lead to several correct solutions. Of course, different solutions do not lead to an identical result of the PCA, but since they are all valid solutions for the corresponding system of equations, all results are also valid results of a PCA. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
