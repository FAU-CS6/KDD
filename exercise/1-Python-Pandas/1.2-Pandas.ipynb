{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Python & pandas\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Python & pandas*.\\\n",
    "The series itself includes practical introductions to *Python* and *pandas*. \n",
    "\n",
    "This exercise series is divided into two parts:\n",
    "\n",
    "- **1.1.** [Brief Introductions to Python](./1.1-Python.ipynb) (*other notebook*)\n",
    "- **1.2.** Brief Introduction to pandas (*this notebook*)\n",
    "    - **1.2.1.** [Central Data Structures: Series and DataFrame](#1.2.1.-Central-Data-Structures:-Series-and-DataFrame)\n",
    "        - **1.2.1.1.** [Object Creation](#1.2.1.1.-Object-Creation)\n",
    "        - **1.2.1.2.** [Accessing First or Last Rows of a DataFrame](#1.2.1.2.-Accessing-First-or-Last-Rows-of-a-DataFrame)\n",
    "        - **1.2.1.3.** [Get Number of Rows and Columns](#1.2.1.3.-Get-Number-of-Rows-and-Columns)\n",
    "        - **1.2.1.4.** [Get Number of Cells in a DataFrame or Series](#1.2.1.4.-Get-Number-of-Cells-in-a-DataFrame-or-Series)\n",
    "        - **1.2.1.5.** [Get Memory Consumption in Bytes](#1.2.1.5.-Get-Memory-Consumption-in-Bytes)\n",
    "        - **1.2.1.6.** [Get Useful DataFrame Information](#1.2.1.6.-Get-Useful-DataFrame-Information)\n",
    "    - **1.2.2.** [Indexing and Selection](#1.2.2.-Indexing-and-Selection)\n",
    "        - **1.2.2.1.** [Access Specific Columns](#1.2.2.1.-Access-Specific-Columns)\n",
    "        - **1.2.2.2.** [Label-based vs. Integer-based Indexing](#1.2.2.2.-Label-based-vs.-Integer-based-Indexing)\n",
    "    - **1.2.3.** [Sort a `DataFrame` by Index or Column](#1.2.3.-Sort-a-DataFrame-by-Index-or-Column)\n",
    "        - **1.2.3.1.** [Sort By Index](#1.2.3.1.-Sort-By-Index)\n",
    "        - **1.2.3.2.** [Sort by Column](#1.2.3.2.-Sort-by-Column)\n",
    "    - **1.2.4.** [Statistics, Aggregation, and Groups](#1.2.4.-Statistics,-Aggregation,-and-Groups)\n",
    "    - **1.2.5.** [Content Modification](#1.2.5.-Content-Modification)\n",
    "        - **1.2.5.1.** [Add a New Row or Column](#1.2.5.1.-Add-a-New-Row-or-Column)\n",
    "        - **1.2.5.2.** [Modify a Specific Cell or Multiple Cells](#1.2.5.2.-Modify-a-Specific-Cell-or-Multiple-Cells)\n",
    "        - **1.2.5.3.** [Delete Row or Column](#1.2.5.3.-Delete-Row-or-Column)\n",
    "        - **1.2.5.4.** [Merge or Join Two `DataFrame`s](#1.2.5.4.-Merge-or-Join-Two-DataFrames)\n",
    "        - **1.2.5.5.** [Concatenate Two or More `DataFrame`s](#1.2.5.5.-Concatenate-Two-or-More-DataFrames)\n",
    "    - **1.2.6.** [Data Cleaning Operations](#1.2.6.-Data-Cleaning-Operations)\n",
    "    - **1.2.7.** [Read and Write Data](#1.2.7.-Read-and-Write-Data)\n",
    "    - **1.2.8.** [Database Access](#1.2.8.-Database-Access)\n",
    "    - **1.2.9.** [What's More?](#1.2.9.-What's-More?)\n",
    "\n",
    "This exercise series is optional and is especially recommended for students who have little to no experience with *Python* and/or *pandas*.\\\n",
    "If this resonates with you, please execute the individual code segments one by one and try to understand their functionality.\\\n",
    "In this notebook there are also some tasks that you can be used to test your proficiency.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "There is no exercise session for this exercise series. If you have questions about individual code segments/tasks, please post them on the StudOn forum.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Brief Introduction to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is an open-source library providing intuitive data structures for data analysis, data transformation, cleaning, and, with the help of matplotlib, simple data visualizations.\n",
    "\n",
    "Most pandas' functionality is based on the numpy library, which is a optimized library providing efficient data structures and mathematical functions.\n",
    "\n",
    "Note that this JupyterNotebook merely gives an overview and is by no means complete. Yet, this notebook introduces pandas' central data structures (Series and DataFrame), how to index and select data from a DataFrame, as well as operations for data cleaning, sorting, aggregating, and grouping data.\n",
    "For detailed explanations and further topics like time series and date functionality refer to the extensive [user guide](https://pandas.pydata.org/docs/user_guide/index.html) and [API](https://pandas.pydata.org/docs/reference/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pandas simply import it like in the following code cell. Typically, this library is aliased with `pd`. It became the de-facto standard to import it with this alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1. Central Data Structures: Series and DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas represents data in a table format called `DataFrame` in which every column represents a `Series`. A `Series` holds data of a specific type such as String or integer. Under the hood, a `Series` is nothing more than a one-dimensional numpy array with a heading (column name). In this way, a `DataFrame` is a table with several columns (`Series`) that has column names. \n",
    "\n",
    "![DataFrame](img/01_table_dataframe.svg)\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "**Difference: pandas vs. numpy**\n",
    "\n",
    "- pandas may have trouble with extremely large DataFrames. It becomes sluggish and slow compared to numpy arrays.\n",
    "- pandas `DataFrame`s hold tabular data of mixed types, whereas numpy arrays do not support mixed types.    \n",
    "- numpy arrays are different to Python lists. A numpy array can only hold values of the same type whereas a Python list can hold mixed data types.\n",
    "- numpy arrays are optimized and thus consume less (memory) storage.\n",
    "- numpy library is written in C and provides an API for Python. Thus, code is already compiled which makes access and computations faster. Python code is merely interpreted and thus slower.\n",
    "- Mathematical operations on numpy arrays behave just like mathematical operations on matrices and vectors (e. g. multiplication, addition).\n",
    "- numpy provides more or less the same functionality as pandas.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.1. Object Creation\n",
    "\n",
    "Different ways exist to create a pandas `DataFrame`. These include, for instance, creation from a list, numpy array, dictionary and list of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `DataFrame` Creation From a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[\n",
    "        [1, \"Course Introduction\", True],\n",
    "        [2, \"KDD Introduction\", True],\n",
    "        [3, \"Getting To Know Your Data\", False],\n",
    "        [4, \"Data Preprocessing\", False],\n",
    "        [5, \"OLAP\", False],\n",
    "        [6, \"Frequent Pattern\", False],\n",
    "        [7, \"Classification\", False],\n",
    "        [8, \"Cluster\", False],\n",
    "        [9, \"Outlier\", False],\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `DataFrame` Creation From a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Number\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        \"Lecture Name\": [\n",
    "            \"Course Introduction\",\n",
    "            \"KDD Introduction\",\n",
    "            \"Getting To Know Your Data\",\n",
    "            \"Data Preprocessing\",\n",
    "            \"OLAP\",\n",
    "            \"Frequent Pattern\",\n",
    "            \"Classification\",\n",
    "            \"Cluster\",\n",
    "            \"Outlier\",\n",
    "        ],\n",
    "        \"Done\": [True, True, False, False, False, False, False, False, False],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of creating a `DataFrame` from a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[\n",
    "        {\"Number\": 1, \"Lecture Name\": \"Course Introduction\", \"Done\": True},\n",
    "        {\n",
    "            \"Number\": 2,\n",
    "            \"Lecture Name\": \"KDD Introduction\",\n",
    "            \"Done\": True,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 3,\n",
    "            \"Lecture Name\": \"Getting To Know Your Data\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 4,\n",
    "            \"Lecture Name\": \"Data Preprocessing\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 5,\n",
    "            \"Lecture Name\": \"OLAP\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 6,\n",
    "            \"Lecture Name\": \"Frequent Pattern\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 7,\n",
    "            \"Lecture Name\": \"Classification\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 8,\n",
    "            \"Lecture Name\": \"Cluster\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "        {\n",
    "            \"Number\": 9,\n",
    "            \"Lecture Name\": \"Outlier\",\n",
    "            \"Done\": False,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we copy this DataFrame for later use\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas automatically derives data types. You can view them with the function `dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data type of column \"Lecture Name\" is of type `object`. [Documentation of `dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html) reveals that \"[c]olumns with mixed types are stored with the `object` dtype\". Generally, pandas has two ways to store strings: in an `object` dtype capable of holding any Python object, or `StringDtype`. It is, however, recommended to use `StringDtype` for strings. Conversion of dtype can be achieved with the function `astype`, which is available for both `DataFrame` and `Series`. \n",
    "\n",
    "Changing dtype of a specific column of a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"Lecture Name\": \"string\"})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 1**:\n",
    "    \n",
    "Create a pandas Series Using the Variable `lectures`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures = [\n",
    "    \"Course Introduction\",\n",
    "    \"KDD Introduction\",\n",
    "    \"Getting To Know Your Data\",\n",
    "    \"Data Preprocessing\",\n",
    "    \"OLAP\",\n",
    "    \"Frequent Pattern\",\n",
    "    \"Classification\",\n",
    "    \"Cluster\",\n",
    "    \"Outlier\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_series = pd.Series(lectures)\n",
    "lecture_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Create a pandas DataFrame Using The Created Series. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "lecture_df = pd.DataFrame(lecture_series, columns=[\"Lecture Name\"])\n",
    "lecture_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.2. Accessing First or Last Rows of a DataFrame\n",
    "\n",
    "A `DataFrame` may hold many rows. Viewing the whole `DataFrame` at once might not always be the best idea. However, it is possible to only view the first or the last couple of rows. A default of five rows will be displayed each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.3. Get Number of Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {df.shape[0]}\", f\"Number of columns: {df.shape[1]}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.4. Get Number of Cells in a DataFrame or Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.5. Get Memory Consumption in Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.6. Get Useful DataFrame Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2. Indexing and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame` consists of one or more columns with one or more rows. pandas provides the data type `Index` and automatically creates an row index (called `index`) and column index (called `columns`) for you when creating a `DataFrame`. Many pandas functions operate on these indices and are often refered to as `axis` where `axis=0` corresponds to columns and `axis=1`corresponds to rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example of `DataFrame` creation used a simple Python list without any column names, whereas later dictionary examples set column names based on dictionary keys. It is also possible to explicitly define column names at `DataFrame` creation with the parameter `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, indices `columns` and `index` of a `DataFrame` is accessible via their respective variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the row index (`index`) is not stored as a list but as a `RangeIndex`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both indices are accessible at once via `axes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2.1. Access Specific Columns\n",
    "\n",
    "It is possible to select one or more specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Lecture Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a column has no whitespaces it is also possible to access a column in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select specific columns by a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Number\", \"Lecture Name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that selecting a single column always returns a `Series` whereas selecting multiple columns returns a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[[\"Number\", \"Lecture Name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2.2. Label-based vs. Integer-based Indexing\n",
    "\n",
    "pandas provides two ways of indexing and selection: label-based and integer-based.\n",
    "\n",
    "**Label-based Indexing** \n",
    "\n",
    "Label-based indexing operates using names of columns or rows. `DataFrame` provides the function `loc` that, according to documentation, primarily operates on names but also works with a boolean array.\n",
    "\n",
    "Following inputs are allowed:\n",
    "1. Single label for a single row/column.\n",
    "2. List of labels for multiple rows/columns.\n",
    "3. A slice object with labels for mulitple rows/columns. A pandas slice object includes both start and end lables, whereas typical Python slice objects exclude the end position/label!.\n",
    "4. Boolean array for multiple rows/columns. Here, value `False` does not include a row/column.\n",
    "5. A `callable` function.\n",
    "\n",
    "Syntax for `loc` differs slightly to the usual function calls as instead of rounded brackets it expects corner brackets: `loc[row_name, col_name]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the name of lecture 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2, \"Lecture Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select multiple rows with a slice object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2:, \"Lecture Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all lecture names that have already been held with a callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[lambda df: df.Done, \"Lecture Name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index-based Indexing** \n",
    "\n",
    "Index-based indexing is similar to its label-based counterpart, yet operates on integer numbers for both row and columns. \n",
    "\n",
    "Following inputs are allowed:\n",
    "1. Single integer for row/column.\n",
    "2. List of integers for mulitple rows/columns.\n",
    "3. A slice object with integers for multiple rows/columns. A pandas slice object includes both start and end lables, whereas typical Python slice objects exclude the end position/label!.\n",
    "4. Boolean array for multiple rows/columns. Here, value `False` does not include a row/column.\n",
    "5. A `callable` function.\n",
    "\n",
    "Index-based indexing is used with `iloc` instead of `loc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 3:**\n",
    "    \n",
    "Play with Index-based Indexing.\n",
    "\n",
    "- Select a single cell.\n",
    "- Select multiple cells.\n",
    "- Select all lectures whose name contains \"Introduction\". Use a callable. Hint: Lambda.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO Select a single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Select a single cell\n",
    "print(df.iloc[0, 0])\n",
    "print(df.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO Select multiple cells at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Select multiple cells at once\n",
    "df.iloc[:4, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO Select cells with a callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Select cells with a callable\n",
    "# boolean masks only work when you discard the index and pass only the list itself:\n",
    "df.iloc[lambda df: (df[\"Lecture Name\"].str.contains(\"Introduction\")).values, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "(df[\"Lecture Name\"].str.contains(\"Introduction\")).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "df.iloc[lambda df: (df.Done == False).index, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3. Sort a `DataFrame` by Index or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrame`can be sorted by its index as well as column values.\n",
    "\n",
    "### 1.2.3.1. Sort By Index\n",
    "\n",
    "Function `sort_index` sorts a `DataFrame` by its index alphanumerically. Following parameters are available (among others, refer to [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html)):\n",
    "\n",
    "- `axis`: Default `0`, meaning it sorts rows.\n",
    "- `ascending`: Default `True`, set to `False` for descending sort.\n",
    "- `inplace`: Default `False`.\n",
    "- `na_position`: Default `last`. To place NaNs (Not a Number, `NULL` in SQL) first, set to `first`.\n",
    "- `kind`: Sorting algorithm, default `quicksort`. Choose between: `quicksort`, `mergesort`, `heapsort`, `stable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get our untouched DataFrame\n",
    "df = df_original.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort descending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3.2. Sort by Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort a `DataFrame` by column, use function `sort_values`. It has the following parameters (refer to [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)):\n",
    "- `by`: Column name or list of column names to sort by.\n",
    "- `axis`: Default `0`, meaning it sorts rows.\n",
    "- `inplace`: Default `False`.\n",
    "- `na_position`: Default `last`. To place NaNs (Not a Number, `NULL` in SQL) first, set to `first`.\n",
    "- `kind`: Sorting algorithm, default `quicksort`. Choose between: `quicksort`, `mergesort`, `heapsort`, `stable`.\n",
    "- `ignore_index`: Default `False` to retain current index. Set to `True` to generate a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Lecture Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Lecture Name\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4. Statistics, Aggregation, and Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is based on numpy and thus, provides an extensive amount of mathematical functions out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from order_example import articles\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides functions to describe a distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean value of column \"PRICE\"\n",
    "articles.PRICE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median value of column \"PRICE\"\n",
    "articles.PRICE.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max value of column \"PRICE\"\n",
    "articles.PRICE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likewise, min value of column \"PRICE\"\n",
    "articles.PRICE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of column \"PRICE\"\n",
    "articles.PRICE.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of column \"PRICE\"\n",
    "articles.PRICE.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics are automatically calculated at once by `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.PRICE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply this function on the whole DataFrame\n",
    "articles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or transpose the output\n",
    "articles.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all article prices\n",
    "articles.PRICE.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cummulative sum of article prices\n",
    "articles.PRICE.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values\n",
    "articles.PRICE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of unique values\n",
    "articles.PRICE.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to calculate several functions in one statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.agg({\"PRICE\": [min, max, \"nunique\", \"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also group by some column and apply a function based on this group\n",
    "articles.groupby(\"TYPE\").count().loc[:, \"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all article names in a group\n",
    "articles.groupby(\"TYPE\").agg({\"NAME\": list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article names in a group as a comma separated string\n",
    "articles.groupby(\"TYPE\").agg({\"NAME\": \", \".join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.groupby(\"TYPE\").mean().loc[:, \"PRICE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5. Content Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often than not it is desirable to change a `DataFrame`'s content. This could include either adding a new row/column or modify a single or multiple cells at once. Additionally, when having two `DataFrame`s it may be desireable to merge, join, or concatenate these `DataFrame`s.\n",
    "\n",
    "Thus, we take a look at the following:\n",
    "\n",
    "1. Add a new row/column.\n",
    "2. Modify a specific cell or multiple cells.\n",
    "3. Delete row or column.\n",
    "4. Merge or Join two `DataFrame`s.\n",
    "6. Concatenate two or more `DataFrame`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.1. Add a New Row or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting new rows or columns is possible in different ways:\n",
    "1. Select a row or column that does not yet exist and simply assign a value to it.\n",
    "2. Insert a column at a specific position with `insert(loc, column, value)`.\n",
    "3. Append new rows or columns with `concat`. Later more on this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a new row or column by selecting an index that does not yet exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Exam Relevant\"] = [False] + [True] * (df.shape[0] - 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[10, \"Lecture Name\"] = \"Unnamed Lecture\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column with `insert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = [i for i in range(42, 42 + df.shape[0])]\n",
    "df.insert(0, \"Better Number\", new_values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.2. Modify a Specific Cell or Multiple Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying cells or multiple cells at once can be achieved in various ways: \n",
    "1. using indexing (label-based or index-based), boolean masking or using a callable. For instance, set all lectures \"Done\" column to `True` with boolean masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all values of attribute \"Done\" to True when False\n",
    "df.loc[df[\"Done\"] == False, \"Done\"] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `apply` is a useful function able to apply any function along an axis of a `DataFrame`.\n",
    "3. `applymap` applies a function element-wise instead along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Lecture Name\"] = df.loc[:, \"Lecture Name\"].apply(str.upper)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_random_weight(x):\n",
    "    return (x - 1) * random.choice(range(10)) / 100\n",
    "\n",
    "\n",
    "df[\"Imaginary Weight\"] = df.loc[:, \"Number\"].apply(get_random_weight)\n",
    "# apply(get_random_weight) is short for apply(lambda x: get_random_weight(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `applymap` is similar to `apply` but is applied elementwise, meaning that it is applied on all cells unlike `apply` which is applied column- or row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_cells = df.applymap(lambda x: len(str(x)))\n",
    "length_of_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.3. Delete Row or Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting rows or columns can be done in different ways:\n",
    "1. Selecting everything you want to keep and assign it to the same variable.\n",
    "2. Using function `pop`. This function removes a column from a `DataFrame` and returns the removed row/column as a `Series`.\n",
    "3. Using funciton `drop` to remove a row or column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"Exam Relevant\"] == True, :]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_numbers = df.pop(\"Better Number\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_exam_relevant = df.drop(\n",
    "    labels=[\"Exam Relevant\"],\n",
    "    axis=1,  # to drop a column. To drop a row set axis=0\n",
    "    inplace=False,  # No inplace drop, this returns the new DataFrame\n",
    ")\n",
    "df_wo_exam_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.4. Merge or Join Two `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`merge` is a function to join two `DataFrame`s based on some keys (columns). It is similar to `JOIN` in SQL. In pandas another function exists, called `join`. The difference is the following:\n",
    "- `merge` merges or \"joins\" two `DataFrame`s based on columns or indexes.\n",
    "- `merge` can sort join keys lexicographically, add suffixes to columns in result `DataFrame`\n",
    "- Per default, `join` performs a join based on two `DataFrame`s indices.\n",
    "- `join` uses `merge` internally when joining/merging index-on-index or column(s)-on-index.\n",
    "Thus, `join` saves typing time when you want to join or merge two `DataFrame`s by their index.\n",
    "\n",
    "Both `merge` and `join` supports all SQL `JOIN`-operations:\n",
    "\n",
    "| **Method** | **SQL**            | **Description**                                       |\n",
    "|------------|--------------------|-------------------------------------------------------|\n",
    "| `left`     | `LEFT OUTER JOIN`  | Use keys from left `DataFrame` only                   |\n",
    "| `right`    | `RIGHT OUTER JOIN` | Use keys from right `DataFrame` only                  |\n",
    "| `outer`    | `FULL OUTER JOIN`  | Use union of keys on both `DataFrame`s                |\n",
    "| `inner`    | `INNER JOIN`       | Use intersection of keys from both `DataFrame`s       |\n",
    "| `cross`    | `CROSS JOIN`       | Create cartesian product of rows of both `DataFrame`s |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport, in case they have been modified\n",
    "from order_example import articles, customers, orders, order_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "\n",
    "| **Method** | **SQL**            | **Description**                                       |\n",
    "|------------|--------------------|-------------------------------------------------------|\n",
    "| `left`     | `LEFT OUTER JOIN`  | Use keys from left `DataFrame` only                   |\n",
    "| `right`    | `RIGHT OUTER JOIN` | Use keys from right `DataFrame` only                  |\n",
    "| `outer`    | `FULL OUTER JOIN`  | Use union of keys on both `DataFrame`s                |\n",
    "| `inner`    | `INNER JOIN`       | Use intersection of keys from both `DataFrame`s       |\n",
    "| `cross`    | `CROSS JOIN`       | Create cartesian product of rows of both `DataFrame`s |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily merge/join the `DataFrame`s `orders` and `customers` to add the names to each order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(orders, customers, on=\"CID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have customers that did not yet buy something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join = pd.merge(orders, customers, on=\"CID\", how=\"outer\")\n",
    "customers_no_buy = outer_join[outer_join[\"OID\"].isnull()]\n",
    "customers_no_buy[[\"CID\", \"NAME\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 4:**\n",
    "    \n",
    "Join `DataFrame`s `orders` and `order_position`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "orders_with_positions = pd.merge(orders, order_positions, on=\"OID\", how=\"inner\")\n",
    "orders_with_positions.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 5:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` and join with `articles`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "op_article = pd.merge(orders_with_positions, articles, on=\"AID\", how=\"inner\")\n",
    "op_article.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 6:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` to Calculate Sum of each Order.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "op_article[\"TOTAL_PRICE\"] = op_article.UNIT * op_article.PRICE\n",
    "order_sums = op_article.groupby(\"OID\").sum()\n",
    "order_sums[[\"TOTAL_PRICE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 7:**\n",
    "    \n",
    "Reuse the Previous `DataFrame` and Join Customers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "order_sums = pd.merge(order_sums, customers, on=\"CID\", how=\"inner\")\n",
    "order_sums[[\"NAME\", \"TOTAL_PRICE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 8:**\n",
    "    \n",
    "What Articles Have Not Been Sold Yet?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles_not_sold = pd.merge(order_positions, articles, on=\"AID\", how=\"right\")\n",
    "articles_not_sold = articles_not_sold[articles_not_sold.OID.isna()]\n",
    "articles_not_sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# alternatively:\n",
    "articles.loc[~articles.AID.isin(order_positions.AID.unique()), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles.loc[articles.AID.isin(order_positions.AID.unique()) == False, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "What Articles Have Been Sold the Most (Units)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# reuse previous DataFrame\n",
    "op_article_sums = op_article.groupby([\"AID\", \"NAME\"]).sum()\n",
    "op_article_sums.sort_values(by=\"UNIT\", ascending=False).iloc[:3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customers have a sweet tooth as they prefer kiwis, strawberries, and water melons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 9:**\n",
    "    \n",
    "What Articles Have the Highest Revenue?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# reuse previous DataFrame\n",
    "op_article_sums.sort_values(by=\"TOTAL_PRICE\", ascending=False).iloc[:3, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5.5. Concatenate Two or More `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation of two or more `DataFrame`s can be achieved - yet again - in multiple ways. In the previous chapter, we already discussed one way of accomplishing concatenation of two `DataFrame`s by means of a cross join. Another way is the use of `concatenate` which is able to concatenate two or more `DataFrame`s column- or row-wise.\n",
    "\n",
    "Let's prepare a function `get_df_with_random_values` and define two `DataFrame`s using this function: `df1`, and `df2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_random_values(range_number=10):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [random.choice(range(range_number)) for _ in range(range_number)]\n",
    "            for _ in range(range_number)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_df_with_random_values()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_df_with_random_values()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate along columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate along rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.6. Data Cleaning Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has means of data cleaning with functions like:\n",
    "1. Finding missing values with `isna`.\n",
    "2. Filling missing values with `fillna` or interpolate values with `interpolate`.\n",
    "   - `fillna` supports forward and backward filling\n",
    "   - `interpolate` supports various filling methods such as linear, nearest, polynomial, as well as interpolation methods from the library [SciPy](https://docs.scipy.org/doc/scipy/tutorial/interpolate.html). \n",
    "3. Drop missing values with `dropna`.\n",
    "4. Return a boolean mask of duplicates with `duplicated`.\n",
    "5. Drop duplicate rows with `drop_duplicates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_mess = pd.DataFrame(\n",
    "    [\n",
    "        (21, 3.0, 10),\n",
    "        (24, 2.0, 8),\n",
    "        (21, 4.0, 12),\n",
    "        (21, 2.0, 11),\n",
    "        (24, 3.0, 8),\n",
    "        (23, np.nan, 9),\n",
    "        (21, 2.0, 8),\n",
    "        (20, 1.0, 11),\n",
    "        (25, 1.0, 7),\n",
    "        (24, 2.0, 12),\n",
    "        (23, np.nan, 8),\n",
    "        (27, 3.0, 7),\n",
    "        (21, 4.0, 12),\n",
    "        (23, 1.0, 11),\n",
    "        (20, np.nan, 7),\n",
    "        (28, np.nan, 12),\n",
    "        (26, 1.0, 10),\n",
    "        (21, np.nan, 10),\n",
    "        (20, 2.0, 9),\n",
    "        (27, 4.0, 7),\n",
    "    ],\n",
    "    columns=list(\"ABC\"),\n",
    ")\n",
    "df_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many NaN (Not a Number) values this DataFrame has\n",
    "df_mess.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling\n",
    "df_mess[\"B\"].interpolate(method=\"linear\", inplace=True)\n",
    "df_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any duplicate rows and if, which values they have. Remember, the function `duplicated` returns a boolean mask that can be used to select any rows that contains duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess[df_mess.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `drop_duplicates` can retain either the first or the last row of duplicates. Which to retain depends on the context at hand.  In this example, it does not matter which row to keep.\n",
    "\n",
    "In the case, however, this example would be a time series of, let's say, temperature and some other observed signals indexed by time (imagine this index is a time step), then these *duplicates* are none.\n",
    "\n",
    "For now, in this example let's drop duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess.drop_duplicates(inplace=True)\n",
    "# verify duplicates have been dropped\n",
    "df_mess[df_mess.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.7. Read and Write Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we pushed reading and writing data and instead created a `DataFrame` from scratch. That is, however, not feasible.\n",
    "\n",
    "Pandas offers methods to read and write a `DataFrame` from and to various data formats. Among them are, naturally, column separated files (csv), JSON files, dictionary, string, but also SQL, pickle files, latex, and markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 10:**\n",
    "    \n",
    "Write the `DataFrame`s of article, customer, order, and order positions to CSV files.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "articles.to_csv(\"articles.csv\", index=False)\n",
    "customers.to_csv(\"customers.csv\", index=False)\n",
    "orders.to_csv(\"orders.csv\", index=False)\n",
    "order_positions.to_csv(\"order_positions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.8. Database Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas and its `DataFrame` function `to_sql` it is easy to extract data from database tables or write to a table. For this, `to_sql` requires a database engine. More specifically, a database engine from `sqlalchemy`. Once established and connected to a database, tables can be created, written to, or appended. Likewise, with `from_sql` extracts data from a table and returns it as a `DataFrame`.\n",
    "\n",
    "`sqlalchemy` is an Object Relational Mapper (ORM) that maps (Python) objects, more particularly `DataFrame`s, to tables. For a quick start in ORM in `sqlalchemy` refer to its [documentation](https://docs.sqlalchemy.org/en/14/orm/quickstart.html).\n",
    "\n",
    "Python supports SQLite databases out of the box without any additional packages. Check out Python's [documentation](https://docs.python.org/3/library/sqlite3.html) on how to use SQLite.\n",
    "\n",
    "Back to `sqlalchemy`:\n",
    "In order to use `sqlalchemy` with pandas, we first need to import this library and then create a database engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///orders.db\", echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_sql(name=\"article\", con=engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's SQLite interface is not much different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_engine = sqlite3.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python's internal SQLite support it is possible to create a in-memory database. You can, however, still persist your database by specifying a file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read this table with `read_sql`. With the parameter `sql`, we can query any table, join tables as we go, or just specify one table name as a short hand for `SELECT * FROM table_name;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read article from database\n",
    "articles_db = pd.read_sql(sql=\"article\", con=engine)\n",
    "articles_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegetables = pd.read_sql(\n",
    "    sql=\"SELECT * FROM article WHERE TYPE='Vegetable';\", con=engine\n",
    ")\n",
    "vegetables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 11:**\n",
    "    \n",
    "Write all other `DataFrame`s to the database and access them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "customers.to_sql(name=\"customer\", con=engine, index=False)\n",
    "orders.to_sql(name=\"order\", con=engine, index=False)\n",
    "order_positions.to_sql(name=\"order_position\", con=engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "customers_db = pd.read_sql(sql=\"customer\", con=engine)\n",
    "customers_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 12:**\n",
    "    \n",
    "Write a SQL query that returns the top three articles sold unit-wise and print out the returning DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "most_sold_articles = pd.read_sql(\n",
    "    sql=\"\"\"SELECT AID, NAME, sum(UNIT) as UNIT\n",
    "FROM order_position \n",
    "JOIN article using(AID)\n",
    "GROUP BY AID, NAME\n",
    "ORDER BY UNIT DESC\n",
    "LIMIT 3;\"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "most_sold_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 13:**\n",
    "    \n",
    "Likewise, write a SQL query that returns the top three customers revenue-wise and print out the returning DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "In case you named your table `order`, keep in mind that this word is a reserved keyword and thus, must be quoted. Alternatively, you can name this table differently like `orders` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "best_customers = pd.read_sql(\n",
    "    sql=\"\"\"SELECT CID, customer.NAME, sum(UNIT * PRICE) as REVENUE\n",
    "FROM order_position\n",
    "JOIN 'order' using(OID)\n",
    "JOIN article using(AID)\n",
    "JOIN customer using(CID)\n",
    "GROUP BY CID, customer.NAME\n",
    "ORDER BY REVENUE DESC\n",
    "LIMIT 3;\"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "best_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.9. What's More?\n",
    "Check out the extensive [user guide](https://pandas.pydata.org/docs/user_guide/index.html#user-guide) for more functionality."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
