{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 3. Frequent Patterns\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Frequent Patterns*. The series itself is based on lecture *6. Mining Frequent Patterns, Associations and Correlations*. \n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing A Priori and FP-Growth\n",
    "- Part Two: Mining Frequent Patterns in the AdventureWorks Database\n",
    "\n",
    "Recall that we have two exercise groups. Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety. If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part One: Implementing A Priori and FP-Growth\n",
    "\n",
    "In this part we will take a closer look at the methods A Priori and FP-Growth, which are well known from the lecture. In the following, you will first implement both methods yourself step by step and then compare your implementation with the implementation of a common library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "We take a look at a very small data set in this part. It was already used in the lecture and should enable you to validate your code by yourself without knowing a sample solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# A very small data set in the form of a list (transactions) of sets (items)\n",
    "dataset = [\n",
    "    {\"Beer\", \"Nuts\", \"Diapers\"},\n",
    "    {\"Beer\", \"Coffee\", \"Diapers\"},\n",
    "    {\"Beer\", \"Diapers\", \"Eggs\"},\n",
    "    {\"Nuts\", \"Eggs\", \"Milk\"},\n",
    "    {\"Nuts\", \"Coffee\", \"Diapers\", \"Eggs\"},\n",
    "]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### A Priori\n",
    "\n",
    "The first method we consider is A Priori. It is a very basic approach, which requires many accesses to the data set under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "For our implementation, we first define a (data)class `Itemset`, which can be used to store a set of items together with the count of occurrences of these items in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The (data)class Itemset\n",
    "@dataclass\n",
    "class Itemset:\n",
    "    # Attributes\n",
    "    items: set\n",
    "    occurrence_count: int = 0\n",
    "\n",
    "\n",
    "# Example of usage (might be a hint for later tasks)\n",
    "# Create an example Itemset\n",
    "example_itemset = Itemset({\"Beer\", \"Nuts\"})\n",
    "\n",
    "# Increase the occurrence_count\n",
    "example_itemset.occurrence_count += 1\n",
    "\n",
    "# Check whether this itemset is a subset of a bigger set of items\n",
    "example_itemset.items.issubset({\"Beer\", \"Nuts\", \"Diapers\"})\n",
    "example_itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "We also define a class `ItemsetList`, which is a list of `Itemset`s providing some functions you might want to use in later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class ItemsetList\n",
    "class ItemsetList:\n",
    "    # Constructor\n",
    "    def __init__(self, itemsets: list[Itemset]):\n",
    "        self.itemsets = itemsets\n",
    "\n",
    "    # Functions\n",
    "    # Return all Itemsets which are containing exactly the passed items\n",
    "    def get_itemsets_with_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items == items]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing exactly the passed items\n",
    "    def contains_itemset_with_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_items(items)) > 0\n",
    "\n",
    "    # Return all Itemsets which are containing a superset of the passed items\n",
    "    def get_itemsets_with_superset_of_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items.issuperset(items)]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing a superset of the passed items\n",
    "    def contains_itemset_with_superset_of_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_superset_of_items(items)) > 0\n",
    "\n",
    "    # Return all Itemsets which are containing a subset of the passed items\n",
    "    def get_itemsets_with_subset_of_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items.issubset(items)]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing a subset of the passed items\n",
    "    def contains_itemset_with_subset_of_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_subset_of_items(items)) > 0\n",
    "\n",
    "\n",
    "# Example of usage (might be a hint for later tasks)\n",
    "# Create an example ItemsetList\n",
    "example_itemset_list = ItemsetList([])\n",
    "\n",
    "# Add our example itemset to the list\n",
    "example_itemset_list.itemsets.append(example_itemset)\n",
    "\n",
    "# Check if there is a itemset with exactly the items Beer and Nuts\n",
    "example_itemset_list.contains_itemset_with_items({\"Beer\", \"Nuts\"})\n",
    "\n",
    "# Get the itemsets with a subset of the items Beer and Nuts\n",
    "example_itemset_list.get_itemsets_with_subset_of_items({\"Beer\", \"Nuts\", \"Diapers\"})\n",
    "example_itemset_list.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The first step in A Priori is to scan the dataset once to get all 1-itemsets. To avoid scanning the dataset multiple times during the search for frequent 1-itemsets the count of occurrences of each item is determined during that step.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function below, which is intended to generate all 1-itemsets and their occurrence count based on a given dataset.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate all 1-itemsets\n",
    "def generate_one_itemsets(dataset):\n",
    "    # Initialize an ItemsetList\n",
    "    itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the itemsets\n",
    "    return itemsets\n",
    "\n",
    "\n",
    "# Get all 1-itemsets (and their occurrence count) within our dataset\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate all 1-itemsets\n",
    "def generate_one_itemsets(dataset):\n",
    "    # Initialize an ItemsetList\n",
    "    itemsets = ItemsetList([])\n",
    "\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Iterate over all items contained in that transaction\n",
    "        for item in transaction:\n",
    "            # Check whether the itemset already exists in itemsets\n",
    "            if itemsets.contains_itemset_with_items({item}):\n",
    "                # If yes just increment the items count\n",
    "                itemsets.get_itemsets_with_items({item})[0].occurrence_count += 1\n",
    "            else:\n",
    "                # If no add the item to itemsets (occurrence_count has to be 1, as it is the first occurrence)\n",
    "                itemsets.itemsets.append(Itemset({item}, 1))\n",
    "\n",
    "    # Return the itemsets\n",
    "    return itemsets\n",
    "\n",
    "\n",
    "# Get all 1-itemsets (and their occurrence count) within our dataset\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Only items that are occurring more often or the same number of times as defined in `minimal_support_count` are frequent 1-itemsets. For this reason, the next necessary step is to prune all itemsets that occur less frequently than this value.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Create the function `prune_itemsets` that removes itemsets that do not satisfy `minimal_support_count` (we use a `minimal_support_count` of 3 in this example).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to prune itemsets that occurred less then minimal_support times\n",
    "def prune_itemsets(itemsets, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Prune every itemset occuring less then three times\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 3)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to prune itemsets that occurred less then minimal_support times\n",
    "def prune_itemsets(itemsets, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # Get all itemsets that occurred at least minimal_support_count times\n",
    "    # This is very similar to the functions given in ItemsetList\n",
    "    # but yet it is not included in ItemsetList to provide a little challenge\n",
    "    frequent_itemsets.itemsets = [\n",
    "        x for x in itemsets.itemsets if x.occurrence_count >= minimal_support_count\n",
    "    ]\n",
    "\n",
    "    # Return the itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Prune every itemset occuring less then three times\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 3)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "One of the most important principles that A Priori uses is that only itemsets that are themselves frequent can lead to supersets that are frequent. So to find possible candidates for frequent 2-itemsets, only the found frequent 1-itemsets have to be combined to 2-itemsets.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Implement `generate_candidates` so that it can be used to generate length-(k+1) candidate itemsets from lenght-k frequent itemsets. You are allowed to use `itertools`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate length-k+1 candidate itemsets from length-k frequent itemsets\n",
    "def generate_candidates(frequent_k_itemsets):\n",
    "    # Initialize an ItemsetList\n",
    "    candidates = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Generate the candidates of the second level\n",
    "two_candidates = generate_candidates(frequent_one_itemsets)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate length-k+1 candidate itemsets from length-k frequent itemsets\n",
    "def generate_candidates(frequent_k_itemsets):\n",
    "    # Initialize an ItemsetList\n",
    "    candidates = ItemsetList([])\n",
    "\n",
    "    # Get k\n",
    "    k = len(frequent_k_itemsets.itemsets[0].items)\n",
    "\n",
    "    # Iterate over the frequent_k_itemsets to get all items contained in at least a single frequent_k_itemset\n",
    "    items = set()\n",
    "    for itemset in frequent_k_itemsets.itemsets:\n",
    "        # Add the items of the itemset to items\n",
    "        items = items.union(itemset.items)\n",
    "\n",
    "    # Find all combinations with lenght k+1\n",
    "    for combination in itertools.combinations(items, k + 1):\n",
    "        # Check that all subsets with length k are part of frequent_k_itemsets\n",
    "        all_k_subsets_are_part_of_frequent_k_itemsets = True\n",
    "\n",
    "        for i in range(k + 1):\n",
    "            # Convert combination into a list\n",
    "            # (== copy of the combination)\n",
    "            k_subset = list(combination)\n",
    "\n",
    "            # Remove the i-th element\n",
    "            k_subset.pop(i)\n",
    "\n",
    "            # Convert the list into set\n",
    "            k_subset = set(k_subset)\n",
    "\n",
    "            # Check if k_subset is contained in frequent_k_itemsets\n",
    "            if not frequent_k_itemsets.contains_itemset_with_items(k_subset):\n",
    "                # A k_subset is not part of frequent_k_itemsets\n",
    "                # => The combination is no candidate for k+1\n",
    "                all_k_subsets_are_part_of_frequent_k_itemsets = False\n",
    "\n",
    "                # Of course we can skipping further checking now\n",
    "                break\n",
    "\n",
    "        # If all are part of frequent_k_itemsets the combination is a candidate\n",
    "        if all_k_subsets_are_part_of_frequent_k_itemsets:\n",
    "            candidates.itemsets.append(Itemset(set(combination), 0))\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Generate the candidates of the second level\n",
    "two_candidates = generate_candidates(frequent_one_itemsets)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# You might want to check the list with an other example as well\n",
    "extra_frequent_two_itemsets = ItemsetList(\n",
    "    [\n",
    "        Itemset({\"Football\", \"Shoes\"}),\n",
    "        Itemset({\"Football\", \"Glasses\"}),\n",
    "        Itemset({\"Shoes\", \"Glasses\"}),\n",
    "        Itemset({\"Glasses\", \"Tissues\"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate the candidates of the third level\n",
    "extra_three_candidates = generate_candidates(extra_frequent_two_itemsets)\n",
    "extra_three_candidates.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After generating candidates, the next step is to scan the dataset to find out how often which candidate occurs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Finalize the `scan_candidates` function, which is used to determine how often each candidate Itemset occurs in the dataset.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to determine how often each candidate Itemset occurs in the dataset\n",
    "def scan_candidates(dataset, candidates):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Determine how often each candidate Itemset occurs\n",
    "two_candidates = scan_candidates(dataset, two_candidates)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to determine how often each candidate Itemset occurs in the dataset\n",
    "def scan_candidates(dataset, candidates):\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Get all candidates itemsets that are a subset of the items occurring in the dataset\n",
    "        subset_candidates = candidates.get_itemsets_with_subset_of_items(transaction)\n",
    "\n",
    "        # Increase the occurrence count of each candidate Itemset being a subset of the items occurring in the dataset\n",
    "        for subset_candidate in subset_candidates:\n",
    "            subset_candidate.occurrence_count += 1\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Determine how often each candidate Itemset occurs\n",
    "two_candidates = scan_candidates(dataset, two_candidates)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Once the number of occurrences of each candidate has been determined, `prune_itemsets` can be used again to remove the candidates that do not match the `minimal_support_count` of 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Prune all Itemset below the minimal_support_count of 3\n",
    "frequent_two_itemsets = prune_itemsets(two_candidates, 3)\n",
    "frequent_two_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After the initial execution of `generate_one_itemsets`, the functions `prune_itemsets`, `generate_candidates` and `scan_candidates` are executed in a loop until no further candidates or frequent itemsets can be found.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write the function a_priori that uses the functions `generate_one_itemsets`, `prune_itemsets`, `generate_candidates` and `scan_candidates` to perform a complete run of A Priori for an arbitrarily large data set.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement an A Priori wrapper\n",
    "def a_priori(dataset, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the frequent_itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Get all frequent itemsets within our dataset satisfing the minimal_support_count of 3\n",
    "frequent_itemsets = a_priori(dataset, 3)\n",
    "frequent_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement an A Priori wrapper\n",
    "def a_priori(dataset, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # Start by generating all 1-itemsets and make the first candidates for becoming frequent itemsets\n",
    "    candidate_itemsets = generate_one_itemsets(dataset)\n",
    "\n",
    "    # Start the loop that will run as long as there are candidate_itemsets\n",
    "    while len(candidate_itemsets.itemsets) > 0:\n",
    "        # Prune the candidate itemsets not satisfing the minimal_support_count\n",
    "        frequent_k_itemsets = prune_itemsets(candidate_itemsets, minimal_support_count)\n",
    "\n",
    "        # If there are no frequent_k_itemset we might also break the loop (second termination criterion)\n",
    "        if len(frequent_k_itemsets.itemsets) == 0:\n",
    "            break\n",
    "\n",
    "        # Otherwise we should add the found frequent k-itemsets to the main list of frequent_itemsets\n",
    "        frequent_itemsets.itemsets.extend(frequent_k_itemsets.itemsets)\n",
    "\n",
    "        # Prepare the next loop run\n",
    "        # Generate possible candidates\n",
    "        candidate_itemsets = generate_candidates(frequent_k_itemsets)\n",
    "\n",
    "        # Determine how often each candidate occurs\n",
    "        candidate_itemsets = scan_candidates(dataset, candidate_itemsets)\n",
    "\n",
    "    # Return the frequent_itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Get all frequent itemsets within our dataset satisfing the minimal_support_count of 3\n",
    "frequent_itemsets = a_priori(dataset, 3)\n",
    "frequent_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: Mlxtend\n",
    "\n",
    "Of course, it's tedious to program A Priori yourself every time you need it. For this reason, there are already some libraries that contain appropriate methods. On this worksheet we use `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required packages of mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "To be able to use the function `apriori` from `mlxtend` to get the frequent itemsets contained in our dataset, we first have to transform it into a suitable format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Take a look at the `mlxtend` [documentation](http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/) for information on how dataset must be structured for `apriori` and preprocess our `dataset` accordingly.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(dataset).transform(dataset)\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset, columns=transaction_encoder.columns_\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After this preparation, the determination of the frequent itemset in our dataset is possible by using `apriori`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Using `apriori` from `mlxtend`, determine the frequent itemsets in our dataset. Use a `min_support` comparable to the value we used in the previous section (`minimal_support_count` of 3).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset\n",
    "# Min support has to be 0.6 as there are 5 tuples in our dataset\n",
    "# => min_support of 0.6 == minimal_suport_count of 3 for 5 tuples)\n",
    "apriori(preprocessed_dataset, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "There are several differences between your own implementation and mlxtend's. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Consider what differences there are between your implementation and `mlxtend`'s implementation of `apriori` for the user of these functions. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Of course, the individual details also depend on your specific implementation. However, based on our specifications, it is to be expected that at least the following things will differ:\n",
    "\n",
    "- <b>The format of the input:</b><br /> \n",
    "Both `mlxtend`s implementation, and your own implementation require a specific format of the dataset on entry. \n",
    "\n",
    "- <b>The format of the output:</b><br /> \n",
    "The format of the output is also different for both variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### FP-Growth\n",
    "\n",
    "Much more complex than A Priori is the FP-Growth approach. However, since this approach requires only two scans of the data set, it also has a clear advantage, especially for larger projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "While we can continue to use some of the classes already introduced for A Priori in our implementation, FP-Growth requires some additional structures.\n",
    "\n",
    "The first thing that comes to mind is the FP-Tree, which consists of the root node and nodes for individual items. In the following we provide you with the classes `FPTree`, `RootNode` and `ItemNode`, that are supposed to serve as the correspondig data structures. Similar to the provided class `ItemsetList` they also provide some basic functionality you are allowed to use in your FP-Growth implementation.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Note**: Don't be put off by the size of the classes. In order to simplify the FP-Growth implementation for you, some functionalities have been outsourced to these classes. \n",
    "    \n",
    "First get a rough overview of the respective class (variable and function names) and come back to the details when you need the functionality.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class FPTree\n",
    "class FPTree:\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Create a root node for the tree\n",
    "        self.root = RootNode()\n",
    "\n",
    "    # Add a set of items to the tree\n",
    "    # This function can be used for adding transactions to the FPTree (occurrence_count of a single transaction = 1)\n",
    "    # Or for adding a single frequent pattern that is part of a conditional frequent pattern base to the conditional FPTree (occurrence_count is the occurrence count of this single frequent pattern)\n",
    "    def add_items_to_tree(\n",
    "        self, items, f_list: list[Itemset], occurrence_count: int = 1\n",
    "    ):\n",
    "        # Set the root node as current node\n",
    "        current_node = self.root\n",
    "\n",
    "        # Interate through the f_list\n",
    "        for item in f_list.itemsets:\n",
    "            # Check if the item is part items\n",
    "            if item.items.issubset(items):\n",
    "                # Check if the item is already present as a child of the current node\n",
    "                if len([x for x in current_node.childs if x.item in item.items]) > 0:\n",
    "                    # Set the node with the item to be the current_node\n",
    "                    current_node = [\n",
    "                        x for x in current_node.childs if x.item in item.items\n",
    "                    ][0]\n",
    "\n",
    "                    # Increase the occurence count of that node\n",
    "                    current_node.occurrence_count += occurrence_count\n",
    "                else:\n",
    "                    # Create a new node\n",
    "                    new_node = ItemNode(\n",
    "                        list(item.items)[0], occurrence_count, current_node\n",
    "                    )\n",
    "\n",
    "                    # Set the new_node as current_node\n",
    "                    current_node = new_node\n",
    "\n",
    "    # Construct the HeaderTable of the Tree\n",
    "    def get_header_table(self):\n",
    "        # Create a HeaderTable\n",
    "        header_table = HeaderTable()\n",
    "\n",
    "        # Call the add_to_header_table() function of the RootNode\n",
    "        self.root.add_to_header_table(header_table)\n",
    "\n",
    "        # Return the header_table\n",
    "        return header_table\n",
    "\n",
    "    # Get all item nodes within the FP-tree\n",
    "    def get_all_item_nodes(self):\n",
    "        # Ask the root node\n",
    "        return self.root.get_all_item_nodes()\n",
    "\n",
    "    # Check whether the FPTree has only a single path\n",
    "    def is_single_path(self):\n",
    "        # Ask the root node\n",
    "        return self.root.is_single_path()\n",
    "\n",
    "    # Check whether the FPTree is empty\n",
    "    def is_empty(self):\n",
    "        # Ask the root node\n",
    "        return self.root.is_empty()\n",
    "\n",
    "    # Print the tree\n",
    "    def print_tree(self):\n",
    "        # Print the subtree starting with the root node\n",
    "        self.root.print_subtree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class RootNode\n",
    "class RootNode:\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Set some member variables\n",
    "        self.item = \"Root-Node\"\n",
    "        self.childs = list()\n",
    "\n",
    "    # Add to HeaderTable\n",
    "    def add_to_header_table(self, header_table):\n",
    "        # The root node itself is not part of the HeaderTable\n",
    "\n",
    "        # But the all childs have to be added\n",
    "        for child in self.childs:\n",
    "            # Recursive call of the add_to_header_table() function\n",
    "            child.add_to_header_table(header_table)\n",
    "\n",
    "    # Get the predecessors (there is no predecessor for the RootNode)\n",
    "    def get_predecessors(self):\n",
    "        # Return an empty list as there are no predecessors\n",
    "        return []\n",
    "\n",
    "    # Get all item nodes within the FP-tree\n",
    "    def get_all_item_nodes(self):\n",
    "        # Init an empty list to add all ItemNodes to\n",
    "        node_list = []\n",
    "\n",
    "        # Go through all childs and add there lists to this node_list\n",
    "        for child in self.childs:\n",
    "            node_list.extend(child.get_all_item_nodes())\n",
    "\n",
    "        return node_list\n",
    "\n",
    "    # Check whether the RootNode has only a single path behind it\n",
    "    def is_single_path(self):\n",
    "        # If there is more then one child return False\n",
    "        if len(self.childs) > 1:\n",
    "            return False\n",
    "        # If there is exactly one child ask that child if there is only a single path\n",
    "        elif len(self.childs) == 1:\n",
    "            return self.childs[0].is_single_path()\n",
    "        # If there are no childs there is only a single path\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # Check whether the FPTree is empty\n",
    "    def is_empty(self):\n",
    "        # If there at least one child return False\n",
    "        if len(self.childs) >= 1:\n",
    "            return False\n",
    "        # Otherwise return true\n",
    "        return True\n",
    "\n",
    "    # Print the subtree\n",
    "    def print_subtree(self):\n",
    "        # Print the root itself\n",
    "        print(self.item)\n",
    "\n",
    "        # Print the childs\n",
    "        for child in self.childs:\n",
    "            child.print_subtree(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class ItemNode\n",
    "class ItemNode:\n",
    "    # Constructor\n",
    "    def __init__(self, item: str, occurrence_count: int, parent):\n",
    "        # Save the arguments\n",
    "        self.item = item\n",
    "        self.occurrence_count = occurrence_count\n",
    "        self.parent = parent\n",
    "\n",
    "        # Set the other parameters used later in the lifespan\n",
    "        self.childs = list()\n",
    "\n",
    "        # Save the node as child in the parent node\n",
    "        parent.childs.append(self)\n",
    "\n",
    "    # Add to HeaderTable\n",
    "    def add_to_header_table(self, header_table):\n",
    "        # Check if there already is a element for this item\n",
    "        if len([x for x in header_table.elements if x.item == self.item]) > 0:\n",
    "            # If there is already an element for this item in the HeaderTable\n",
    "            # Get the element\n",
    "            header_table_element = [\n",
    "                x for x in header_table.elements if x.item == self.item\n",
    "            ][0]\n",
    "\n",
    "            # Add the occurence count to the overall occurrence count\n",
    "            header_table_element.overall_occurrence_count += self.occurrence_count\n",
    "\n",
    "            # Add the ItemNode itself to the element_node_links\n",
    "            header_table_element.node_links.append(self)\n",
    "        else:\n",
    "            # If there is no element for this item in the HeaderTable\n",
    "            # Create a new HeaderTableElement\n",
    "            header_table_element = HeaderTableElement(\n",
    "                self.item, self.occurrence_count, [self]\n",
    "            )\n",
    "\n",
    "            # Add it to the HeaderTable\n",
    "            header_table.elements.append(header_table_element)\n",
    "\n",
    "        # Do a recursive call to add_to_header_table for all childs\n",
    "        for child in self.childs:\n",
    "            # Recursive call of the add_to_header_table() function\n",
    "            child.add_to_header_table(header_table)\n",
    "\n",
    "    # Get the predecessors of this element node (excluding the RootNode)\n",
    "    def get_predecessors(self):\n",
    "        # Get the parents predecessors\n",
    "        predecessors = self.parent.get_predecessors()\n",
    "\n",
    "        # Add the parent to the predecessors if it is not the RootNode\n",
    "        if type(self.parent) != RootNode:\n",
    "            predecessors.append(self.parent)\n",
    "\n",
    "        # Return the predecessors\n",
    "        return predecessors\n",
    "\n",
    "    # Get all item nodes within the FP-tree\n",
    "    def get_all_item_nodes(self):\n",
    "        # Init an list and add this node to it\n",
    "        node_list = [self]\n",
    "\n",
    "        # Go through all childs and add their lists to this node_list\n",
    "        for child in self.childs:\n",
    "            node_list.extend(child.get_all_item_nodes())\n",
    "\n",
    "        return node_list\n",
    "\n",
    "    # Check whether the RootNode has only a single path behind it\n",
    "    def is_single_path(self):\n",
    "        # If there is more then one child return False\n",
    "        if len(self.childs) > 1:\n",
    "            return False\n",
    "        # If there is exactly one child ask that child if there is only a single path\n",
    "        elif len(self.childs) == 1:\n",
    "            return self.childs[0].is_single_path()\n",
    "        # If there are no childs there is only a single path\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # Print the subtree\n",
    "    def print_subtree(self, level):\n",
    "        # Print the node itself\n",
    "        print(\n",
    "            (\" \" * (level - 1) * 2)\n",
    "            + \"├── \"\n",
    "            + self.item\n",
    "            + \": \"\n",
    "            + str(self.occurrence_count)\n",
    "        )\n",
    "\n",
    "        # Print the childs\n",
    "        for child in self.childs:\n",
    "            child.print_subtree(level + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In addition to the central element FP-Tree, FP-Growth also uses another smaller data structure, the header table. This data structure is provided by us, too. The class `HeaderTable` represents the table, which consists of several elements, the `HeaderTableElement`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class HeaderTable\n",
    "class HeaderTable:\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Save the arguments\n",
    "        self.elements = list()\n",
    "\n",
    "    # Print the table\n",
    "    def print_table(self):\n",
    "        # For all elements of the table\n",
    "        for element in self.elements:\n",
    "            # Print the element\n",
    "            element.print_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class HeaderTableElement\n",
    "class HeaderTableElement:\n",
    "    # Constructor\n",
    "    def __init__(self, item: str, overall_occurrence_count: int, node_links: list):\n",
    "        # Save the arguments\n",
    "        self.item = item\n",
    "        self.overall_occurrence_count = overall_occurrence_count\n",
    "        self.node_links = node_links  # Links to every item node regarding our item\n",
    "\n",
    "    # Print the element\n",
    "    def print_element(self):\n",
    "        print(\n",
    "            self.item\n",
    "            + \": \"\n",
    "            + str(self.overall_occurrence_count)\n",
    "            + \" - Count of linked nodes: \"\n",
    "            + str(len(self.node_links))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "A first important step in FP-Growth can be done with the help of the functions already implemented for A Priori. Namely, the frequent 1-itemsets of a dataset have to be determined.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Determine the frequent 1-itemsets occuring at least two times for our dataset `dataset` and store them as `frequent_one_itemsets`. You may use functions from your A Priori implementation to do this.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the frequent 1-itemsets occurring at least two times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the frequent 1-itemsets occurring at least two times\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 2)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Using the frequent 1-itemsets, the next step in the FP-Growth algorithm is to create the so-called f-list: An list of the frequent 1-itemsets in frequency-descending order.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Complete `create_f_list`, which computes the f-list required for FP-Growth from frequent 1-itemsets. Make sure that the passed frequent 1-itemsets are not overwritten by the function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function create_f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function create_f_list\n",
    "def create_f_list(frequent_one_itemsets: ItemsetList):\n",
    "    # Copy the itemset list\n",
    "    f_list = ItemsetList(frequent_one_itemsets.itemsets.copy())\n",
    "\n",
    "    # Sort the list copy and return it\n",
    "    f_list.itemsets.sort(key=lambda x: x.occurrence_count, reverse=True)\n",
    "\n",
    "    # Return the list\n",
    "    return f_list\n",
    "\n",
    "\n",
    "# Create the f-list\n",
    "f_list = create_f_list(frequent_one_itemsets)\n",
    "f_list.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Based on the f-list, the initial FP tree can now be created. For this purpose, the items occurring in each transaction are added to a hierarchical tree structure according to their order in the f-list. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Take a look at `add_items_to_tree` within the `FPTree` class and consider how it can be used to create the initial FP-Tree from the dataset and f-list. Then complete the function below accordingly.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function construct_fp_tree used to construct the inital (non-conditional) FP-Tree\n",
    "def construct_fp_tree(dataset, f_list):\n",
    "    # Initialize an FPTree\n",
    "    fp_tree = FPTree()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the FPTree\n",
    "    return fp_tree\n",
    "\n",
    "\n",
    "# Construct the FP-Tree\n",
    "fp_tree = construct_fp_tree(dataset, frequent_one_itemsets)\n",
    "\n",
    "# Print the FP-Tree\n",
    "print(\"FP-Tree:\")\n",
    "fp_tree.print_tree()\n",
    "\n",
    "# Display the header table\n",
    "print(\"\\nHeader table:\")\n",
    "fp_tree.get_header_table().print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function construct_fp_tree used to construct the inital (non-conditional) FP-Tree\n",
    "def construct_fp_tree(dataset, f_list):\n",
    "    # Initialize an FPTree\n",
    "    fp_tree = FPTree()\n",
    "\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Add the transaction to the FPTree\n",
    "        fp_tree.add_items_to_tree(transaction, f_list, 1)\n",
    "\n",
    "    # Return the FPTree\n",
    "    return fp_tree\n",
    "\n",
    "\n",
    "# Construct the FP-Tree\n",
    "fp_tree = construct_fp_tree(dataset, f_list)\n",
    "\n",
    "# Print the FP-Tree\n",
    "print(\"FP-Tree:\")\n",
    "fp_tree.print_tree()\n",
    "\n",
    "# Display the header table\n",
    "print(\"\\nHeader table:\")\n",
    "fp_tree.get_header_table().print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The FP tree can be used to determine the so-called conditional pattern base for any contained frequent 1-itemsets. This conditional pattern base describes which other items occur before the respective one in the FP tree and how often they can be found with the item in the data set.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Extend `get_conditional_pattern_base` so that it can be used to determine the conditional pattern base for any item within a FP-Tree.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Extend get_conditional_pattern_base to be able to determine a conditional pattern base for any itemset\n",
    "def get_conditional_pattern_base(item, fp_tree):\n",
    "    # Initialize an ItemsetList for the conditional pattern base\n",
    "    conditional_pattern_base = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the found conditional_pattern_base\n",
    "    return conditional_pattern_base\n",
    "\n",
    "\n",
    "# Find the conditional pattern base of \"Eggs\"\n",
    "conditional_pattern_base = get_conditional_pattern_base(\"Eggs\", fp_tree)\n",
    "conditional_pattern_base.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Extend get_conditional_pattern_base to be able to determine a conditional pattern base for any itemset\n",
    "def get_conditional_pattern_base(itemset, fp_tree):\n",
    "    # Initialize an ItemsetList for the conditional pattern base\n",
    "    conditional_pattern_base = ItemsetList([])\n",
    "\n",
    "    # Get the header table of the fp_tree\n",
    "    header_table = fp_tree.get_header_table()\n",
    "\n",
    "    # Search for the items element within the header_table\n",
    "    header_table_element = [x for x in header_table.elements if x.item == itemset]\n",
    "\n",
    "    # If there is an element (otherwise there will be no conditional_pattern_base)\n",
    "    if len(header_table_element) == 1:\n",
    "        # In this case we can switch out the list of elements for a single element\n",
    "        header_table_element = header_table_element[0]\n",
    "\n",
    "        # For every linked node (ItemNodes regarding our item)\n",
    "        for linked_node in header_table_element.node_links:\n",
    "            # Get the predecessors\n",
    "            predecessors = linked_node.get_predecessors()\n",
    "\n",
    "            # If there are predecessors\n",
    "            if len(predecessors) > 0:\n",
    "                # Get all the the items part of the predecessors\n",
    "                predecessor_items = {x.item for x in predecessors}\n",
    "\n",
    "                # Create an Itemset with the occurrence_count set to the occurrence_count of the linked_node\n",
    "                # and the items set to the predecessor_items\n",
    "                itemset = Itemset(predecessor_items, linked_node.occurrence_count)\n",
    "\n",
    "                # Add the items to the conditional_pattern_bases\n",
    "                conditional_pattern_base.itemsets.append(itemset)\n",
    "\n",
    "    # Return the found conditional_pattern_base\n",
    "    return conditional_pattern_base\n",
    "\n",
    "\n",
    "# Find the conditional pattern base of \"Eggs\"\n",
    "conditional_pattern_base = get_conditional_pattern_base(\"Eggs\", fp_tree)\n",
    "conditional_pattern_base.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The conditional pattern base can be used to create a new FP tree. This conditional FP tree shows what the initial FP tree would look like if the selected itemset absolutely had to occur.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Implement the funtionality of `construct_conditional_fp_tree` so that it can be used to create a conditional FP-Tree out of any conditional pattern base. Remember what you have learned about the `FPTree`s `add_items_to_tree`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to construct a conditional FP-tree out of a conditional pattern base\n",
    "def construct_conditional_fp_tree(conditional_pattern_base, f_list):\n",
    "    # Initialize an FPTree\n",
    "    fp_tree = FPTree()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the FPTree\n",
    "    return fp_tree\n",
    "\n",
    "\n",
    "# Construct the conditional FP-tree for the conditional pattern base of \"Eggs\"\n",
    "conditional_fp_tree = construct_conditional_fp_tree(\n",
    "    conditional_pattern_base, frequent_one_itemsets\n",
    ")\n",
    "conditional_fp_tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to construct a conditional FP-tree out of a conditional pattern base\n",
    "def construct_conditional_fp_tree(conditional_pattern_base, f_list):\n",
    "    # Initialize an FPTree\n",
    "    fp_tree = FPTree()\n",
    "\n",
    "    # Iterate over all conditional patterns\n",
    "    for conditional_pattern in conditional_pattern_base.itemsets:\n",
    "        # Add the conditional_pattern to the FPTree\n",
    "        fp_tree.add_items_to_tree(\n",
    "            conditional_pattern.items, f_list, conditional_pattern.occurrence_count\n",
    "        )\n",
    "\n",
    "    # Return the FPTree\n",
    "    return fp_tree\n",
    "\n",
    "\n",
    "# Construct the conditional FP-tree for the conditional pattern base of \"Eggs\"\n",
    "conditional_fp_tree = construct_conditional_fp_tree(conditional_pattern_base, f_list)\n",
    "conditional_fp_tree.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As introduced in the lecture, FP-Growth is ultimately a recursive process in which conditional pattern bases for all elements of the header table are generated from an FP-Tree, from which conditional fp trees are then generated, after which a new stage of recursion is performed.\n",
    "\n",
    "In general, the recursion within FP-Growth can be represented as pseudocode as follows:\n",
    "\n",
    "```\n",
    "FP-GROWTH-RECURSION-STEP(FP-TREE, BASE-PATTERN, F-LIST):\n",
    "    Initialize an empty ITEMSETS-FOUND variable\n",
    "\n",
    "    If(BASE-PATTERN is not null):\n",
    "        Add the BASE-PATTERN to the ITEMSETS-FOUND\n",
    "\n",
    "    If(FP-TREE has only one path):\n",
    "        For each possible COMBINATION of the NODES within the FP-TREE:\n",
    "            Combine the items of the BASE-PATTERN with the items of the COMBINATION to build a new ITEMSET\n",
    "            Set the support of the ITEMSET to the support of the rarest item in the COMBINATION\n",
    "            Add the ITEMSET to the ITEMSETS-FOUND\n",
    "    Else:\n",
    "        For each ITEM in the HEADER-TABLE of the FP-TREE:\n",
    "            If(BASE-PATTERN is null):\n",
    "                Set the ITEM to be the NEW-BASE-PATTERN \n",
    "                Set the support of the NEW-BASE-PATTERN to the ITEMs overall occurrence count within the HEADER-TABLE\n",
    "            Else:\n",
    "                Combine the items of the BASE-PATTERN with the ITEM to become the NEW-BASE-PATTERN \n",
    "                Set the support of the NEW-BASE-PATTERN to MIN(BASE-PATTERNs support, ITEMs overall occurrence count)\n",
    "             \n",
    "            Determine the CONDITIONAL-PATTERN-BASE of the NEW-BASE-PATTERN\n",
    "            Use the CONDITIONAL-PATTERN-BASE to build the CONDITIONAL-FP-TREE\n",
    "            Call FP-GROWTH-RECURSION-STEP recursively with CONDITIONAL-FP-TREE and NEW-BASE-PATTERN\n",
    "            Add the result of the recursive call to the ITEMSETS-FOUND\n",
    "            \n",
    "    Return the ITEMSETS-FOUND\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Based on this pseudo_code, complete the function `fp_growth_recursion_step` below.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function fp_growth_recursion_step to implement the FP-Growth recursion step\n",
    "def fp_growth_recursion_step(fp_tree, base_pattern, f_list):\n",
    "    # Initalize an ItemsetList to return all the patterns found during this fp_growth step\n",
    "    patterns = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the found patterns\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Use the method to get all Itemsets based on the base tree\n",
    "fp_growth_recursion_step(fp_tree, None, frequent_one_itemsets).itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function fp_growth_recursion_step to implement the FP-Growth recursion step\n",
    "def fp_growth_recursion_step(fp_tree, base_pattern, f_list):\n",
    "    # Initalize an ItemsetList to return all the patterns found during this fp_growth step\n",
    "    patterns = ItemsetList([])\n",
    "\n",
    "    # If the base path is not none add it to the pattern list\n",
    "    if base_pattern != None:\n",
    "        patterns.itemsets.append(base_pattern)\n",
    "\n",
    "    # If the fp_tree has only one path we can directly generate every pattern by combining each item with each item\n",
    "    if fp_tree.is_single_path():\n",
    "\n",
    "        # Get all nodes within the single path\n",
    "        node_list = fp_tree.get_all_item_nodes()\n",
    "\n",
    "        # There are combinations between length 1 - (incl.) length len(node_list)\n",
    "        for length in range(1, len(node_list) + 1):\n",
    "            # Create every possible combination with this length\n",
    "            for combination in itertools.combinations(node_list, length):\n",
    "                # Create a new Itemset for this pattern by merging the base pattern with the newly found combination\n",
    "                # The occurrence count is set to the lowest occurrence count in that combination\n",
    "                pattern = Itemset(\n",
    "                    {x.item for x in combination}.union(base_pattern.items),\n",
    "                    min([x.occurrence_count for x in combination]),\n",
    "                )\n",
    "\n",
    "                # Add the pattern to the patterns\n",
    "                patterns.itemsets.append(pattern)\n",
    "    # Otherwise we have to perform a recursive search\n",
    "    else:\n",
    "        # We have to generate the conditional pattern base for every item in the header_table\n",
    "        for header_table_element in fp_tree.get_header_table().elements:\n",
    "            if base_pattern == None:\n",
    "                # Set the new_base_pattern to be the header_table_item\n",
    "                new_base_pattern = Itemset(\n",
    "                    {header_table_element.item},\n",
    "                    header_table_element.overall_occurrence_count,\n",
    "                )\n",
    "            else:\n",
    "                # Merge the new item into the base_pattern to become the new_base_pattern\n",
    "                new_base_pattern = Itemset(\n",
    "                    {header_table_element.item}.union(base_pattern.items),\n",
    "                    min(\n",
    "                        base_pattern.occurrence_count,\n",
    "                        header_table_element.overall_occurrence_count,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            # Construct the conditional patterns base\n",
    "            new_conditional_pattern_base = get_conditional_pattern_base(\n",
    "                header_table_element.item, fp_tree\n",
    "            )\n",
    "            new_conditional_fp_tree = construct_conditional_fp_tree(\n",
    "                new_conditional_pattern_base, f_list\n",
    "            )\n",
    "\n",
    "            # Start the recursion\n",
    "            patterns.itemsets.extend(\n",
    "                fp_growth_recursion_step(\n",
    "                    new_conditional_fp_tree, new_base_pattern, f_list\n",
    "                ).itemsets\n",
    "            )\n",
    "\n",
    "    # Return the found patterns\n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Use the method to get all Itemsets based on the base tree\n",
    "fp_growth_recursion_step(fp_tree, None, f_list).itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The last thing that needs to be done with FP-Growth is to merge the individual components into a wrapper function. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write a function `fp_growth` that, for a given dataset and a given minimal support count, performs all steps of the FP-Growth algorithm up to determining all frequent itemsets. Take into account that after execution of `fp_growth_recursion_step` itemsets that do not meet the minimum support count must be pruned.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function fp_growth combining all the functions to get a full FP-Growth implementation\n",
    "def fp_growth(dataset, minimal_support_count):\n",
    "\n",
    "    # ...\n",
    "    return ItemsetList([])\n",
    "\n",
    "\n",
    "# Get all frequent patterns with a minimal support count 2 or higher\n",
    "fp_growth(dataset, 2).itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function fp_growth combining all the functions to get a full FP-Growth implementation\n",
    "def fp_growth(dataset, minimal_support_count):\n",
    "    # Find the frequent 1-itemsets\n",
    "    one_itemsets = generate_one_itemsets(dataset)\n",
    "    frequent_one_itemsets = prune_itemsets(one_itemsets, minimal_support_count)\n",
    "\n",
    "    # Create the f-list\n",
    "    frequent_one_itemsets.itemsets.sort(key=lambda x: x.occurrence_count, reverse=True)\n",
    "    f_list = frequent_one_itemsets\n",
    "\n",
    "    # Construct the initital FP-Tree\n",
    "    fp_tree = construct_fp_tree(dataset, f_list)\n",
    "\n",
    "    # Use the recursive methode to get all n-itemsets (n > 1)\n",
    "    n_itemsets = fp_growth_recursion_step(fp_tree, None, frequent_one_itemsets)\n",
    "\n",
    "    # Prune all non frequent n-itemsets\n",
    "    frequent_n_itemsets = prune_itemsets(n_itemsets, minimal_support_count)\n",
    "\n",
    "    # Return the frequent_n_itemsets\n",
    "    return frequent_n_itemsets\n",
    "\n",
    "\n",
    "# Get all frequent patterns with a minimal support count 2 or higher\n",
    "fp_growth(dataset, 2).itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: Mlxtend\n",
    "\n",
    "Just with like A Priori, there is also a corresponding function for FP-Growth included in `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required packages of mlxtend\n",
    "from mlxtend.frequent_patterns import fpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Again `mlxtend` expects a certain input format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Take a look at the `mlxtend` [documentation](http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/) for information on how dataset must be structured for `fpgrowth` and preprocess our `dataset` accordingly.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(dataset).transform(dataset)\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset, columns=transaction_encoder.columns_\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After this preparation, the determination of the frequent itemset in our dataset is possible by using `fpgrowth`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Using `fpgrowth` from `mlxtend`, determine the frequent itemsets in our dataset. Use a `min_support` comparable to the value we used in the previous section (`minimal_support_count` of 2).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use fpgrowth from mlxtend to determine the frequent itemsets in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use fpgrowth from mlxtend to determine the frequent itemsets in our dataset\n",
    "fpgrowth(preprocessed_dataset, min_support=0.4, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "There are several differences between your own implementation and mlxtend's. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Consider what differences there are between your implementation and `mlxtend`'s implementation of `fpgrowth` for the user of these functions. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Of course, the individual details also depend on your specific implementation. However, based on our specifications, it is to be expected that at least the following things will differ:\n",
    "\n",
    "- <b>The format of the input:</b><br /> \n",
    "Both `mlxtend`s implementation, and your own implementation require a specific format of the dataset on entry. \n",
    "\n",
    "- <b>The format of the output:</b><br /> \n",
    "The format of the output is also different for both variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part Two: Mining Frequent Patterns in the AdventureWorks Database\n",
    "\n",
    "Whereas in Part One you worked on a very small and therefore non-realistic data set, you will now apply your knowledge of Frequent Patterns to a more realistic scenario. Imagine this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "*You are an employee in the fictitious company Adventure Works GmbH. Your job is to find out which of the company's products are frequently bought together. To start with, the management wants you to find the ten most \"relevant\" product pairs bought together.*\n",
    "\n",
    "*You get access to the OLTP database of the company. You are also told by colleagues who are experts in the database that information about individual transactions can be found in the relation `TransactionHistory`. The resolution of ProductIDs into real product names can be done with the help of the relation `Product`.*\n",
    "\n",
    "*From other similar projects of the company you also already know the required libraries and the code to connect to the OLTP database:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Finding the Ten Most \"Relevant\" Frequent Patterns\n",
    "\n",
    "Within this worksheet, you are now given two options: You can first tackle the task independently as you would actually have to in this scenario or you can choose the guided path, in which we accompany you step by step from loading the DataFrames to converting the Frequent Itemsets into Association Rules by spliting up the big problem into small tasks.\n",
    "\n",
    "We recommend that you first try out the \"independent\" version and only switch to the guided version if you encounter problems.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**Note:** In both cases there is an other section at the end of this worksheet. Do not skip it, regardless of your decission in this section. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Option 1: Solve the Assignment Independently\n",
    "\n",
    "In this variant we don't give you anything except for the description of the scenario, the libraries to use, the database connection, and some code cells (just add more if needed, since we only added more than one in the first place to get the tasks of Option 2 out of your view). \n",
    "\n",
    "However, you do get one extra small tip: If you have successfully determined the frequent itemsets, you may take another look at the list of libraries. There you will probably find a function to determine the association rules from the frequent itemsets. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Find the ten most \"relevant\" frequent patterns in the OLTP database of the fictitious Adventure Works GmbH. You have to decide every step from loading the DataFrames to determining the Association Rules from the Frequent Itemsets.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Option 2: Solve the Assignment by Solving Small Tasks \n",
    "\n",
    "Any large assignment can, of course, be broken down into many smaller steps. For a KDD task, where only a database and some relevant relations are given, the first important step is to first get familiar with the given data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Getting to Know `TransactionHistory` and `Product`\n",
    "\n",
    "To become familiar with the data within 'TransactionHistory' and 'Product', records must first be loaded. Since we don't know anything about the relations yet, it would make sense to load all attributes for both datasets first and also not to perform a selection of individual tuples.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Load the relations `TransactionHistory` and `Product` into two individual DataFrames and display the first ten rows of each DataFrame. (Hint: You might want to look at exercise sheet 2 (a-c) to get to know methods of loading relations into a DataFrame)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Load TransactionHistory into a DataFrame and display the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame and display the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load TransactionHistory into a DataFrame and display the first ten rows\n",
    "transaction_history_df = pd.read_sql_query(\n",
    "    \"SELECT * FROM TransactionHistory\", connection\n",
    ")\n",
    "transaction_history_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame and display the first ten rows\n",
    "product_df = pd.read_sql_query(\"SELECT * FROM Product\", connection)\n",
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As our fictitious colleagues told us in the scenario, the `TransactionHistory` seems to contain information about individual transactions. However, it is not yet possible to see how it is possible to determine which products (probably identified via the `ProductID`) are purchased together. \n",
    "\n",
    "With the Product table, the only important information seems to what `ProductID` leads to which `Name`, which is why we can get straight to the problem in the `TransactionHistory`:\n",
    "\n",
    "We might therefore assume that we are looking for products that are purchased in the same transaction. Since the attribute `TransactionID` probably uniquely identifies each transaction, it would make sense to test this hypothesis by determining whether there are `TransactionID`s with more than one linked `ProductID`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Check if there are cases of several different `ProductID`s for the same `TransactionID`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same TransactionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same TransactionID\n",
    "# First group the dataframe by TransactionID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"TransactionID\"]\n",
    ").count()\n",
    "\n",
    "# Then check if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# No results => There are no cases of several different ProductIDs for the same TransactionID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Since there are obviously no cases of multiple different `ProductID`s for the same `TransactionID`, our first hypothesis does not seem to be correct. Apparently, the `TransactionID` is the primary key for the `TransactionHistory` relation: i.e. one and the same `TransactionID` cannot refer to different `ProductID`s. \n",
    "\n",
    "However, if you look at the `TransactionHistory` again, a second attribute stands out. The `ReferenceOrderID`. This could identify the individual order and products that are part of the same order were obviously purchased together.\n",
    "\n",
    "So let's test this new hypothesis as well.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Check if there are cases of several different `ProductID`s for the same `ReferenceOrderID`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same ReferenceOrderID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same ReferenceOrderID\n",
    "# Now group the dataframe by ReferenceOrderID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"ReferenceOrderID\"]\n",
    ").count()\n",
    "\n",
    "# Then check again if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# 23249 results => There are multiple cases of different ProductIDs for the same ReferenceOrderID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Our new hypothesis seems to be correct. I.e. in the next step we want to search for `ProductID`s that regularly occur in the same `ReferenceOrderID`: The frequent itemsets of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Identifing the Frequent Itemsets\n",
    "\n",
    "To be able to determine our frequent itemsets using mlxtend, we first need to do some preprocessing on `TransactionHistory`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Aggregate the `TransactionHistory` so that next to each `ReferenceOrderID`, the associated `ProductID`s are listed in a single cell.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Aggregate the TransactionHistory to have a list of ProductIDs per ReferenceOrderID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Aggregate the TransactionHistory to have a list of ProductIDs per ReferenceOrderID\n",
    "products_per_order_df = (\n",
    "    transaction_history_df.groupby(\"ReferenceOrderID\")[\"ProductID\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ProductIDs\")\n",
    "    .set_index(\"ReferenceOrderID\")\n",
    ")\n",
    "products_per_order_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Prepare the dataset for `mlxtend`s `fpgrowth` by using the `TransactionEncoder` of the library\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply one hot encoding to the prepared dataset by using the TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply one hot encoding to the prepared dataset by using the TransactionEncoder\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(\n",
    "    products_per_order_df[\"ProductIDs\"].tolist()\n",
    ").transform(products_per_order_df[\"ProductIDs\"].tolist())\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset,\n",
    "    columns=transaction_encoder.columns_,\n",
    "    index=products_per_order_df.index,\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After these necessary preprocessing steps, the frequent itemsets can now theoratically be determined. However, we definitely do not know at this point which min_support to choose. \n",
    "\n",
    "Even by trial and error, it is difficult to find a meaningful threshold here, since we have only been told by our fictitious bosses that we should find the ten most \"relevant\" frequent patterns.\n",
    "\n",
    "First of all, it would therefore make sense to determine rather too many itemsets than too few. It is easier to discard frequent itemsets later than to create additional ones.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Use `fpgrowth` to determine the frequent itemsets of our dataset. Select `min_support` so that the approximately 100 most frequent itemsets become frequent itemsets.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the frequent itemsets\n",
    "frequent_itemsets = fpgrowth(preprocessed_dataset, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Determination of the Frequent Patterns\n",
    "\n",
    "Before we use the frequent itemsets to determine frequent patterns, we must first determine how to define \"relevance\" in the context of frequent patterns.\n",
    "\n",
    "If our bosses meant that they wanted to know which ten patterns occur most frequently in our dataset, then support would be the appropriate measure. Did they want to know how certain one can be that Product A will end up in the shopping cart if Product B is already there? Then the calculation of confidence would be more appropriate. In addition, there are of course a large number of interestingness measures.\n",
    "\n",
    "All in all, this question cannot be answered conclusively. In practice, a dialog between management and you would be appropriate in order to narrow down more precisely what is meant by the most \"relevant\" ten patterns.  \n",
    "\n",
    "This ambiguity was intentionally used in the assignment to show that the assignment will often contain inaccuracies in the real world. \n",
    "\n",
    "However, while in the real world dialogue is the best solution, we have no opportunity to consult with our fictitious bosses. For this reason we do what is best for us and choose the simplest measure to apply: The support.\n",
    "\n",
    "It is not important at first that they really generate only the 10 rules with the highest support. If they are part of your list, everything is fine.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Use `mlxtend`s `association_rules` to generate frequent patterns from the frequent itemsets. Set the corresponding threshold so that at least the 10 frequent patterns with the highest support are included.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns\n",
    "frequent_patterns = association_rules(\n",
    "    frequent_itemsets, metric=\"support\", min_threshold=0.02\n",
    ")\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course, it is no problem at all to sort out extra patterns afterwards. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Delete all patterns that do not belong to the ten patterns with the highest support.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Delete the extra patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Delete the extra patterns\n",
    "frequent_patterns = frequent_patterns.nlargest(10, \"support\")\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Getting to Know the Product Names \n",
    "\n",
    "Even though we have now already completed the core task, it will probably do management little good to tell them that `ProductID` 871 is often purchased in addition to `ProductID` 870. After all, these are first and foremost internal database ids. \n",
    "\n",
    "To complete our task satisfactorily for the management, we still need to enrich the `ProductID`s in antecedents and consequents with their actual names.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Enrich the frequent patterns by adding the product names to the list. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge the ProductName into the frequent pattern df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge the ProductName into the frequent pattern df\n",
    "# We have to transform the frozensets within the two colums to strings first\n",
    "# (as we know that there is only one item per set this is pretty simple)\n",
    "frequent_patterns[\"antecedents\"] = frequent_patterns[\"antecedents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "frequent_patterns[\"consequents\"] = frequent_patterns[\"consequents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "\n",
    "# After that we have to merge frequent_patterns with the product df\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"antecedents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"Name\",\n",
    "        \"consequents\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"antecedents name\"})\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"consequents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"antecedents name\",\n",
    "        \"consequents\",\n",
    "        \"Name\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"consequents name\"})\n",
    "\n",
    "# Print the df\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Assignment completed! In the virtual scenario introduced in the beginning of this part you would now be able to report to the management that the `Mountain Bottle Cage` is often purchased in addition to the `Water Bottle - 30 oz.`. The same is true for the other nine requested frequent patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Implementing the Kulczynski Measure and the Imbalance Ratio\n",
    "\n",
    "The library `mlxtend` offers some more measures besides support and confidence for the determination of frequent patterns. While lift, leverage and conviction are offered, the kulczynski metric and imbalance ratio presented in the lecture, for example, are not.\n",
    "\n",
    "Fortunately, the the antecedent support, the consequent support and the support calculated by `mlxtend` can easily be used to calculate these two interestingness measures.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write a function to compute the kulczynski measure, known from the lecture as `Kulc(a, b)`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return (support / 2) * ((1 / antecedent_support) + (1 / consequent_support))\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write a function to compute the imbalance ratio.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return abs(antecedent_support - consequent_support) / (\n",
    "        antecedent_support + consequent_support - support\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course, the question arises again as to how these metrics are to be interpreted.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Interpret the interestingness measures for the association rule `\"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "First both values must be interpreted separately from each other:\n",
    "\n",
    "- **Kulczynski Measure:**<br/>\n",
    "When kulczynski measure is close to 0 or 1 we have an \"interesting\" association rule. Since in this case the value is about 0.6, the kulczynski measure rather suggests that this association rule is uninteresting. \n",
    "- **Imbalance Ratio:**<br/>\n",
    "For the imbalance ratio, a value of 0 indicates a perfectly balanced association rule, while 1 indicates a very unbalanced one. In this case, we are about 0.52, which is about the middle of the spectrum. Thus, we cannot speak of a particularly well balanced rule, but neither can we speak of a completely unbalanced one.\n",
    "\n",
    "In summary, we have not discovered the most interesting rule, but one that is not completely uninteresting (this would the case for kulczynski measure = 0.5 and imbalance ratio = 0.0). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
