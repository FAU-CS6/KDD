{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 3. Frequent Patterns\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Frequent Patterns*. The series itself is based on lecture *6. Mining Frequent Patterns, Associations and Correlations*. \n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing A Priori and FP-Growth\n",
    "- Part Two: Mining Frequent Patterns on a real dataset\n",
    "\n",
    "Recall that we have two exercise groups. Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety. If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part One: Implementing A Priori and FP-Growth\n",
    "\n",
    "In this part we will take a closer look at the methods A Priori and FP-Growth, which are well known from the lecture. In the following, you will first implement both methods yourself step by step and then compare your implementation with the implementation of a common library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "We take a look at a very small data set in this part. It was already used in the lecture and should enable you to validate your code by yourself without knowing a sample solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# A very small data set in the form of a list (transactions) of sets (items)\n",
    "dataset = [\n",
    "    {\"Beer\", \"Nuts\", \"Diapers\"},\n",
    "    {\"Beer\", \"Coffee\", \"Diapers\"},\n",
    "    {\"Beer\", \"Diapers\", \"Eggs\"},\n",
    "    {\"Nuts\", \"Eggs\", \"Milk\"},\n",
    "    {\"Nuts\", \"Coffee\", \"Diapers\", \"Eggs\"},\n",
    "]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### A Priori\n",
    "\n",
    "The first method we consider is A Priori. It is a very basic approach, which requires many accesses to the data set under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "For our implementation, we first define a (data)class `Itemset`, which can be used to store a set of items together with the count of occurrences of these items in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The (data)class Itemset\n",
    "@dataclass\n",
    "class Itemset:\n",
    "    # Attributes\n",
    "    items: set\n",
    "    occurrence_count: int = 0\n",
    "\n",
    "\n",
    "# Example of usage (might be a hint for later tasks)\n",
    "# Create an example Itemset\n",
    "example_itemset = Itemset({\"Beer\", \"Nuts\"})\n",
    "\n",
    "# Increase the occurrence_count\n",
    "example_itemset.occurrence_count += 1\n",
    "\n",
    "# Check whether this itemset is a subset of a bigger set of items\n",
    "example_itemset.items.issubset({\"Beer\", \"Nuts\", \"Diapers\"})\n",
    "example_itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "We also define a class `ItemsetList`, which is a list of `Itemset`s providing some functions you might want to use in later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# The class Itemset\n",
    "class ItemsetList:\n",
    "    # Constructor\n",
    "    def __init__(self, itemsets: list[Itemset]):\n",
    "        self.itemsets = itemsets\n",
    "\n",
    "    # Functions\n",
    "    # Return all Itemsets which are containing exactly the passed items\n",
    "    def get_itemsets_with_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items == items]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing exactly the passed items\n",
    "    def contains_itemset_with_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_items(items)) > 0\n",
    "\n",
    "    # Return all Itemsets which are containing a superset of the passed items\n",
    "    def get_itemsets_with_superset_of_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items.issuperset(items)]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing a superset of the passed items\n",
    "    def contains_itemset_with_superset_of_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_superset_of_items(items)) > 0\n",
    "\n",
    "    # Return all Itemsets which are containing a subset of the passed items\n",
    "    def get_itemsets_with_subset_of_items(self, items: set):\n",
    "        return [x for x in self.itemsets if x.items.issubset(items)]\n",
    "\n",
    "    # Check if a there is at least a Itemset containing a subset of the passed items\n",
    "    def contains_itemset_with_subset_of_items(self, items: set):\n",
    "        return len(self.get_itemsets_with_subset_of_items(items)) > 0\n",
    "\n",
    "\n",
    "# Example of usage (might be a hint for later tasks)\n",
    "# Create an example ItemsetList\n",
    "example_itemset_list = ItemsetList([])\n",
    "\n",
    "# Add our example itemset to the list\n",
    "example_itemset_list.itemsets.append(example_itemset)\n",
    "\n",
    "# Check if there is a itemset with exactly the items Beer and Nuts\n",
    "example_itemset_list.contains_itemset_with_items({\"Beer\", \"Nuts\"})\n",
    "\n",
    "# Get the itemsets with a subset of the items Beer and Nuts\n",
    "example_itemset_list.get_itemsets_with_subset_of_items({\"Beer\", \"Nuts\", \"Diapers\"})\n",
    "example_itemset_list.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The first step in A Priori is to scan the dataset once to get all 1-itemsets. To avoid scanning the dataset multiple times during the search for frequent 1-itemsets the count of occurrences of each item is determined during that step.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function below, which is intended to generate all 1-itemsets and their occurrence count based on a given dataset.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate all 1-itemsets\n",
    "def generate_one_itemsets(dataset):\n",
    "    # Initialize an ItemsetList\n",
    "    itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the itemsets\n",
    "    return itemsets\n",
    "\n",
    "\n",
    "# Get all 1-itemsets (and their occurrence count) within our dataset\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate all 1-itemsets\n",
    "def generate_one_itemsets(dataset):\n",
    "    # Initialize an ItemsetList\n",
    "    itemsets = ItemsetList([])\n",
    "\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Iterate over all items contained in that transaction\n",
    "        for item in transaction:\n",
    "            # Check whether the itemset already exists in itemsets\n",
    "            if itemsets.contains_itemset_with_items({item}):\n",
    "                # If yes just increment the items count\n",
    "                itemsets.get_itemsets_with_items({item})[0].occurrence_count += 1\n",
    "            else:\n",
    "                # If no add the item to itemsets (occurrence_count has to be 1, as it is the first occurrence)\n",
    "                itemsets.itemsets.append(Itemset({item}, 1))\n",
    "\n",
    "    # Return the itemsets\n",
    "    return itemsets\n",
    "\n",
    "\n",
    "# Get all 1-itemsets (and their occurrence count) within our dataset\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Only items that are occurring more often or the same number of times as defined in `minimal_support_count` are frequent 1-itemsets. For this reason, the next necessary step is to prune all itemsets that occur less frequently than this value.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Create the function `prune_itemsets` that removes itemsets that do not satisfy `minimal_support_count` (we use a `minimal_support_count` of 3 in this example).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to prune itemsets that occurred less then minimal_support times\n",
    "def prune_itemsets(itemsets, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Prune every itemset occuring less then three times\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 3)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to prune itemsets that occurred less then minimal_support times\n",
    "def prune_itemsets(itemsets, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # Get all itemsets that occurred at least minimal_support_count times\n",
    "    # This is very similar to the functions given in ItemsetList\n",
    "    # but yet it is not included in ItemsetList to provide a little challenge\n",
    "    frequent_itemsets.itemsets = [\n",
    "        x for x in itemsets.itemsets if x.occurrence_count >= minimal_support_count\n",
    "    ]\n",
    "\n",
    "    # Return the itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Prune every itemset occuring less then three times\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 3)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "One of the most important principles that A Priori uses is that only itemsets that are themselves frequent can lead to supersets that are frequent. So to find possible candidates for frequent 2-itemsets, only the found frequent 1-itemsets have to be combined to 2-itemsets.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Implement `generate_candidates` so that it can be used to generate length-(k+1) candidate itemsets from lenght-k frequent itemsets. You are allowed to use `itertools`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate length-k+1 candidate itemsets from length-k frequent itemsets\n",
    "def generate_candidates(frequent_k_itemsets):\n",
    "    # Initialize an ItemsetList\n",
    "    candidates = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Generate the candidates of the second level\n",
    "two_candidates = generate_candidates(frequent_one_itemsets)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to generate length-k+1 candidate itemsets from length-k frequent itemsets\n",
    "def generate_candidates(frequent_k_itemsets):\n",
    "    # Initialize an ItemsetList\n",
    "    candidates = ItemsetList([])\n",
    "\n",
    "    # Get k\n",
    "    k = len(frequent_k_itemsets.itemsets[0].items)\n",
    "\n",
    "    # Iterate over the frequent_k_itemsets to get all items contained in at least a single frequent_k_itemset\n",
    "    items = set()\n",
    "    for itemset in frequent_k_itemsets.itemsets:\n",
    "        # Add the items of the itemset to items\n",
    "        items = items.union(itemset.items)\n",
    "\n",
    "    # Find all combinations with lenght k+1\n",
    "    for combination in itertools.combinations(items, k + 1):\n",
    "        # Check that all subsets with length k are part of frequent_k_itemsets\n",
    "        all_k_subsets_are_part_of_frequent_k_itemsets = True\n",
    "\n",
    "        for i in range(k + 1):\n",
    "            # Convert combination into a list\n",
    "            # (== copy of the combination)\n",
    "            k_subset = list(combination)\n",
    "\n",
    "            # Remove the i-th element\n",
    "            k_subset.pop(i)\n",
    "\n",
    "            # Convert the list into set\n",
    "            k_subset = set(k_subset)\n",
    "\n",
    "            # Check if k_subset is contained in frequent_k_itemsets\n",
    "            if not frequent_k_itemsets.contains_itemset_with_items(k_subset):\n",
    "                # A k_subset is not part of frequent_k_itemsets\n",
    "                # => The combination is no candidate for k+1\n",
    "                all_k_subsets_are_part_of_frequent_k_itemsets = False\n",
    "\n",
    "                # Of course we can skipping further checking now\n",
    "                break\n",
    "\n",
    "        # If all are part of frequent_k_itemsets the combination is a candidate\n",
    "        if all_k_subsets_are_part_of_frequent_k_itemsets:\n",
    "            candidates.itemsets.append(Itemset(set(combination), 0))\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Generate the candidates of the second level\n",
    "two_candidates = generate_candidates(frequent_one_itemsets)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# You might want to check the list with an other example as well\n",
    "extra_frequent_two_itemsets = ItemsetList(\n",
    "    [\n",
    "        Itemset({\"Football\", \"Shoes\"}),\n",
    "        Itemset({\"Football\", \"Glasses\"}),\n",
    "        Itemset({\"Shoes\", \"Glasses\"}),\n",
    "        Itemset({\"Glasses\", \"Tissues\"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate the candidates of the third level\n",
    "extra_three_candidates = generate_candidates(extra_frequent_two_itemsets)\n",
    "extra_three_candidates.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After generating candidates, the next step is to scan the dataset to find out how often which candidate occurs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Finalize the `scan_candidates` function, which is used to determine how often each candidate Itemset occurs in the dataset.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to determine how often each candidate Itemset occurs in the dataset\n",
    "def scan_candidates(dataset, candidates):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Determine how often each candidate Itemset occurs\n",
    "two_candidates = scan_candidates(dataset, two_candidates)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to determine how often each candidate Itemset occurs in the dataset\n",
    "def scan_candidates(dataset, candidates):\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Get all candidates itemsets that are a subset of the items occurring in the dataset\n",
    "        subset_candidates = candidates.get_itemsets_with_subset_of_items(transaction)\n",
    "\n",
    "        # Increase the occurrence count of each candidate Itemset being a subset of the items occurring in the dataset\n",
    "        for subset_candidate in subset_candidates:\n",
    "            subset_candidate.occurrence_count += 1\n",
    "\n",
    "    # Return the candidates\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Determine how often each candidate Itemset occurs\n",
    "two_candidates = scan_candidates(dataset, two_candidates)\n",
    "two_candidates.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Once the number of occurrences of each candidate has been determined, `prune_itemsets` can be used again to remove the candidates that do not match the `minimal_support_count` of 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Prune all Itemset below the minimal_support_count of 3\n",
    "frequent_two_itemsets = prune_itemsets(two_candidates, 3)\n",
    "frequent_two_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After the initial execution of `generate_one_itemsets`, the functions `prune_itemsets`, `generate_candidates` and `scan_candidates` are executed in a loop until no further candidates or frequent itemsets can be found.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write the function a_priori that uses the functions `generate_one_itemsets`, `prune_itemsets`, `generate_candidates` and `scan_candidates` to perform a complete run of A Priori for an arbitrarily large data set.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement an A Priori wrapper\n",
    "def a_priori(dataset, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the frequent_itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Get all frequent itemsets within our dataset satisfing the minimal_support_count of 3\n",
    "frequent_itemsets = a_priori(dataset, 3)\n",
    "frequent_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement an A Priori wrapper\n",
    "def a_priori(dataset, minimal_support_count):\n",
    "    # Initialize an ItemsetList\n",
    "    frequent_itemsets = ItemsetList([])\n",
    "\n",
    "    # Start by generating all 1-itemsets and make the first candidates for becoming frequent itemsets\n",
    "    candidate_itemsets = generate_one_itemsets(dataset)\n",
    "\n",
    "    # Start the loop that will run as long as there are candidate_itemsets\n",
    "    while len(candidate_itemsets.itemsets) > 0:\n",
    "        # Prune the candidate itemsets not satisfing the minimal_support_count\n",
    "        frequent_k_itemsets = prune_itemsets(candidate_itemsets, minimal_support_count)\n",
    "\n",
    "        # If there are no frequent_k_itemset we might also break the loop (second termination criterion)\n",
    "        if len(frequent_k_itemsets.itemsets) == 0:\n",
    "            break\n",
    "\n",
    "        # Otherwise we should add the found frequent k-itemsets to the main list of frequent_itemsets\n",
    "        frequent_itemsets.itemsets.extend(frequent_k_itemsets.itemsets)\n",
    "\n",
    "        # Prepare the next loop run\n",
    "        # Generate possible candidates\n",
    "        candidate_itemsets = generate_candidates(frequent_k_itemsets)\n",
    "\n",
    "        # Determine how often each candidate occurs\n",
    "        candidate_itemsets = scan_candidates(dataset, candidate_itemsets)\n",
    "\n",
    "    # Return the frequent_itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Get all frequent itemsets within our dataset satisfing the minimal_support_count of 3\n",
    "frequent_itemsets = a_priori(dataset, 3)\n",
    "frequent_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: Mlxtend\n",
    "\n",
    "Of course, it's tedious to program A Priori yourself every time you need it. For this reason, there are already some libraries that contain appropriate methods. On this worksheet we use `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required packages of mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "To be able to use the function `apriori` from `mlxtend` to get the frequent itemsets contained in our dataset, we first have to transform it into a suitable format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Take a look at the `mlxtend` [documentation](http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/) for information on how dataset must be structured for `apriori` and preprocess our `dataset` accordingly.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(dataset).transform(dataset)\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset, columns=transaction_encoder.columns_\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After this preparation, the determination of the frequent itemset in our dataset is possible by using `apriori`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Using `apriori` from `mlxtend`, determine the frequent itemsets in our dataset. Use a `min_support` comparable to the value we used in the previous section (`minimal_support_count` of 3).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset\n",
    "# Min support has to be 0.6 as there are 5 tuples in our dataset\n",
    "# => min_support of 0.6 == minimal_suport_count of 3 for 5 tuples)\n",
    "apriori(preprocessed_dataset, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "There are several differences between your own implementation and mlxtend's. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Consider what differences there are between your implementation and `mlxtend`'s implementation of `apriori` for the user of these functions. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Of course, the individual details also depend on your specific implementation. However, based on our specifications, it is to be expected that at least the following things will differ:\n",
    "\n",
    "- <b>The format of the input:</b><br /> \n",
    "Both `mlxtend`s implementation, and your own implementation require a specific format of the dataset on entry. \n",
    "\n",
    "- <b>The format of the output:</b><br /> \n",
    "The format of the output is also different for both variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definitions for FP-tree\n",
    "\n",
    "# The class Node\n",
    "class Node:\n",
    "    # Constructor\n",
    "    def __init__(self, item: str, parent, occurrence_count: int):\n",
    "        # Save the arguments\n",
    "        self.item = item\n",
    "        self.parent = parent\n",
    "        self.occurrence_count = occurrence_count\n",
    "\n",
    "        # Set the other parameters used later in the lifespan\n",
    "        self.childs = list()\n",
    "        self.node_link = None\n",
    "\n",
    "    # Output the node\n",
    "    def print_node(self, level):\n",
    "        # Print the node itself\n",
    "        print(\n",
    "            (\" \" * (level - 1) * 2)\n",
    "            + \"├── \"\n",
    "            + self.item\n",
    "            + \": \"\n",
    "            + str(self.occurrence_count)\n",
    "            + \" - Node link: \"\n",
    "            + str(self.node_link)\n",
    "        )\n",
    "\n",
    "        # Print the childs\n",
    "        for child in self.childs:\n",
    "            child.print_node(level + 1)\n",
    "\n",
    "\n",
    "# The class Root\n",
    "class Root:\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Set some member variables\n",
    "        self.item = \"Root-Node\"\n",
    "        self.childs = list()\n",
    "\n",
    "    # Print the tree\n",
    "    def print_tree(self):\n",
    "        # Print the root itself\n",
    "        print(self.item)\n",
    "\n",
    "        # Print the childs\n",
    "        for child in self.childs:\n",
    "            child.print_node(1)\n",
    "\n",
    "\n",
    "# Override the (data)class Itemset, as we want to be able to save link to a Node right next to our itemsets\n",
    "@dataclass\n",
    "class Itemset:\n",
    "    # Attributes\n",
    "    items: set\n",
    "    occurrence_count: int = 0\n",
    "    node_link: Node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: Find frequent 1-itemsets (we can reuse the method implemented for A Priori)\n",
    "one_itemsets = generate_one_itemsets(dataset)\n",
    "frequent_one_itemsets = prune_itemsets(one_itemsets, 3)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step: Sort the items in frequency-descending order (create the f-list)\n",
    "frequent_one_itemsets.itemsets.sort(key=lambda x: x.occurrence_count, reverse=True)\n",
    "frequent_one_itemsets.itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third step: Construct the basic FP-tree\n",
    "def construct_fp_tree(dataset, frequent_one_itemsets):\n",
    "    # Initialize the root node of the FP-tree\n",
    "    root = Root()\n",
    "\n",
    "    # Iterate over all transactions\n",
    "    for transaction in dataset:\n",
    "        # Set the root node as current node\n",
    "        current_node = root\n",
    "\n",
    "        # Interate through the sorted frequent_one_itemsets\n",
    "        for itemset in frequent_one_itemsets.itemsets:\n",
    "            # Check if the itemset is part of the transaction\n",
    "            if itemset.items.issubset(transaction):\n",
    "                # Check if the item is already present\n",
    "                if len([x for x in current_node.childs if x.item in itemset.items]) > 0:\n",
    "                    # Set the node with the item to be the current_node\n",
    "                    current_node = [\n",
    "                        x for x in current_node.childs if x.item in itemset.items\n",
    "                    ][0]\n",
    "\n",
    "                    # Increase the occurence count of that node\n",
    "                    current_node.occurrence_count += 1\n",
    "                else:\n",
    "                    # Create a new node\n",
    "                    new_node = Node(list(itemset.items)[0], current_node, 1)\n",
    "\n",
    "                    # Save the node_link to the last element of the node-link chain\n",
    "                    if itemset.node_link == None:\n",
    "                        # If the itemset is not yet linked to a node then set it there\n",
    "                        itemset.node_link = new_node\n",
    "                    else:\n",
    "                        # If it is linked to a node, then follow the node_links until there is no other link\n",
    "                        node = itemset.node_link\n",
    "                        while node.node_link != None:\n",
    "                            node = node.node_link\n",
    "\n",
    "                        # Save the node_link\n",
    "                        node.node_link = new_node\n",
    "\n",
    "                    # Set it as child of the current_node\n",
    "                    current_node.childs.append(new_node)\n",
    "\n",
    "                    # Set the new_node as current_node\n",
    "                    current_node = new_node\n",
    "\n",
    "    # Return the root node and therefore the whole tree\n",
    "    return root\n",
    "\n",
    "\n",
    "# Construct the FP-Tree\n",
    "fp_tree = construct_fp_tree(dataset, frequent_one_itemsets)\n",
    "\n",
    "# Print the FP-Tree\n",
    "print(\"FP-Tree:\")\n",
    "fp_tree.print_tree()\n",
    "\n",
    "# Display the header table\n",
    "print(\"\\nHeader table:\")\n",
    "for itemset in frequent_one_itemsets.itemsets:\n",
    "    print(\n",
    "        str(itemset.items)\n",
    "        + \": \"\n",
    "        + str(itemset.occurrence_count)\n",
    "        + \" - Node link: \"\n",
    "        + str(itemset.node_link)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: Mlxtend\n",
    "\n",
    "Just with like A Priori, there is also a corresponding function for FP-Growth included in `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required packages of mlxtend\n",
    "from mlxtend.frequent_patterns import fpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Again `mlxtend` expects a certain input format.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Take a look at the `mlxtend` [documentation](http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/) for information on how dataset must be structured for `fpgrowth` and preprocess our `dataset` accordingly.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(dataset).transform(dataset)\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset, columns=transaction_encoder.columns_\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After this preparation, the determination of the frequent itemset in our dataset is possible by using `fpgrowth`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Using `fpgrowth` from `mlxtend`, determine the frequent itemsets in our dataset. Use a `min_support` comparable to the value we used in the previous section (`minimal_support_count` of 3).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use fpgrowth from mlxtend to determine the frequent itemsets in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use fpgrowth from mlxtend to determine the frequent itemsets in our dataset\n",
    "fpgrowth(preprocessed_dataset, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "There are several differences between your own implementation and mlxtend's. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Consider what differences there are between your implementation and `mlxtend`'s implementation of `fpgrowth` for the user of these functions. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Of course, the individual details also depend on your specific implementation. However, based on our specifications, it is to be expected that at least the following things will differ:\n",
    "\n",
    "- <b>The format of the input:</b><br /> \n",
    "Both `mlxtend`s implementation, and your own implementation require a specific format of the dataset on entry. \n",
    "\n",
    "- <b>The format of the output:</b><br /> \n",
    "The format of the output is also different for both variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Mining Frequent Patterns on a real dataset\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary notes\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "TODO: This section should be removed before the final publication of the exercise  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the clean DataFrame(s)\n",
    "# Order DataFrame\n",
    "order_df = pd.read_sql_query(\n",
    "    \"SELECT ReferenceOrderID, COUNT(*) \"\n",
    "    \"FROM TransactionHistory h \"\n",
    "    \"GROUP BY ReferenceOrderID \"\n",
    "    \"ORDER BY COUNT(*)\",\n",
    "    connection,\n",
    ")\n",
    "\n",
    "order_df_2 = pd.read_sql_query(\n",
    "    \"SELECT * \" \"FROM TransactionHistory h\",\n",
    "    connection,\n",
    ")\n",
    "\n",
    "order_df_3 = pd.read_sql_query(\n",
    "    \"SELECT ReferenceOrderID, GROUP_CONCAT(ProductID) \"\n",
    "    \"FROM TransactionHistory h \"\n",
    "    \"GROUP BY ReferenceOrderID \",\n",
    "    connection,\n",
    "    index_col=\"ReferenceOrderID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = (\n",
    "    order_df_2.groupby(\"ReferenceOrderID\")[\"ProductID\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"new\")\n",
    ")\n",
    "list_1[\"new\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"Beer, Nuts, Diapers\",\n",
    "    \"Beer, Coffee, Diapers\",\n",
    "    \"Beer, Diapers, Eggs\",\n",
    "    \"Nuts, Eggs, Milk\",\n",
    "    \"Nuts, Coffee, Diapers, Eggs\",\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(list_1[\"new\"].to_list()).transform(list_1[\"new\"].to_list())\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori(df, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = fpgrowth(df, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = order_df_3.values.tolist()\n",
    "list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(list_2).transform(list_2)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    [\"Milk\", \"Onion\", \"Nutmeg\", \"Kidney Beans\", \"Eggs\", \"Yogurt\"],\n",
    "    [\"Dill\", \"Onion\", \"Nutmeg\", \"Kidney Beans\", \"Eggs\", \"Yogurt\"],\n",
    "    [\"Milk\", \"Apple\", \"Eggs\"],\n",
    "    [\"Milk\", \"Unicorn\", \"Corn\", \"Kidney Beans\", \"Yogurt\"],\n",
    "    [\"Corn\", \"Onion\", \"Onion\", \"Kidney Beans\", \"Ice cream\", \"Eggs\"],\n",
    "]\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df\n",
    "\n",
    "apriori(df, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Tree, Node\n",
    "from inspect import getmembers, isfunction\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "node = tree.create_node(\"Test1\", data=1)\n",
    "node3 = tree.create_node(\"Test2\", data=2, parent=node)\n",
    "node2 = tree.create_node(\"Test3\", data=3, parent=node)\n",
    "\n",
    "tree.show()\n",
    "\n",
    "\n",
    "node.successors(tree.identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
