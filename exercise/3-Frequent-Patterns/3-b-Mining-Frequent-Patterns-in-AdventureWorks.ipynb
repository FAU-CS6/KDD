{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 3. Frequent Patterns\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Frequent Patterns*. The series itself is based on lecture *6. Mining Frequent Patterns, Associations and Correlations*. \n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing A Priori and FP-Growth\n",
    "- Part Two: Mining Frequent Patterns in the AdventureWorks Database\n",
    "\n",
    "Recall that we have two exercise groups. Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety. If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part Two: Mining Frequent Patterns in the AdventureWorks Database\n",
    "\n",
    "Whereas in Part One you worked on a very small and therefore non-realistic data set, you will now apply your knowledge of Frequent Patterns to a more realistic scenario. Imagine this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "*You are an employee in the fictitious company Adventure Works GmbH. Your job is to find out which of the company's products are frequently bought together. To start with, the management wants you to find the ten most \"relevant\" product pairs bought together.*\n",
    "\n",
    "*You get access to the OLTP database of the company. You are also told by colleagues who are experts in the database that information about individual transactions can be found in the relation `TransactionHistory`. The resolution of ProductIDs into real product names can be done with the help of the relation `Product`.*\n",
    "\n",
    "*From other similar projects of the company you also already know the required libraries and the code to connect to the OLTP database:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Finding the Ten Most \"Relevant\" Frequent Patterns\n",
    "\n",
    "Within this worksheet, you are now given two options: You can first tackle the task independently as you would actually have to in this scenario or you can choose the guided path, in which we accompany you step by step from loading the DataFrames to converting the Frequent Itemsets into Association Rules by spliting up the big problem into small tasks.\n",
    "\n",
    "We recommend that you first try out the \"independent\" version and only switch to the guided version if you encounter problems.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**Note:** In both cases there is an other section at the end of this worksheet. Do not skip it, regardless of your decission in this section. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Option 1: Solve the Assignment Independently\n",
    "\n",
    "In this variant we don't give you anything except for the description of the scenario, the libraries to use, the database connection, and some code cells (just add more if needed, since we only added more than one in the first place to get the tasks of Option 2 out of your view). \n",
    "\n",
    "However, you do get one extra small tip: If you have successfully determined the frequent itemsets, you may take another look at the list of libraries. There you will probably find a function to determine the association rules from the frequent itemsets. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Find the ten most \"relevant\" frequent patterns in the OLTP database of the fictitious Adventure Works GmbH. You have to decide every step from loading the DataFrames to determining the Association Rules from the Frequent Itemsets.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Option 2: Solve the Assignment by Solving Small Tasks \n",
    "\n",
    "Any large assignment can, of course, be broken down into many smaller steps. For a KDD task, where only a database and some relevant relations are given, the first important step is to first get familiar with the given data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Getting to Know `TransactionHistory` and `Product`\n",
    "\n",
    "To become familiar with the data within 'TransactionHistory' and 'Product', records must first be loaded. Since we don't know anything about the relations yet, it would make sense to load all attributes for both datasets first and also not to perform a selection of individual tuples.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Load the relations `TransactionHistory` and `Product` into two individual DataFrames and display the first ten rows of each DataFrame. (Hint: You might want to look at exercise sheet 2 (a-c) to get to know methods of loading relations into a DataFrame)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Load TransactionHistory into a DataFrame and display the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame and display the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load TransactionHistory into a DataFrame and display the first ten rows\n",
    "transaction_history_df = pd.read_sql_query(\n",
    "    \"SELECT * FROM TransactionHistory\", connection\n",
    ")\n",
    "transaction_history_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame and display the first ten rows\n",
    "product_df = pd.read_sql_query(\"SELECT * FROM Product\", connection)\n",
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "As our fictitious colleagues told us in the scenario, the `TransactionHistory` seems to contain information about individual transactions. However, it is not yet possible to see how it is possible to determine which products (probably identified via the `ProductID`) are purchased together. \n",
    "\n",
    "With the Product table, the only important information seems to what `ProductID` leads to which `Name`, which is why we can get straight to the problem in the `TransactionHistory`:\n",
    "\n",
    "We might therefore assume that we are looking for products that are purchased in the same transaction. Since the attribute `TransactionID` probably uniquely identifies each transaction, it would make sense to test this hypothesis by determining whether there are `TransactionID`s with more than one linked `ProductID`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Check if there are cases of several different `ProductID`s for the same `TransactionID`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same TransactionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same TransactionID\n",
    "# First group the dataframe by TransactionID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"TransactionID\"]\n",
    ").count()\n",
    "\n",
    "# Then check if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# No results => There are no cases of several different ProductIDs for the same TransactionID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Since there are obviously no cases of multiple different `ProductID`s for the same `TransactionID`, our first hypothesis does not seem to be correct. Apparently, the `TransactionID` is the primary key for the `TransactionHistory` relation: i.e. one and the same `TransactionID` cannot refer to different `ProductID`s. \n",
    "\n",
    "However, if you look at the `TransactionHistory` again, a second attribute stands out. The `ReferenceOrderID`. This could identify the individual order and products that are part of the same order were obviously purchased together.\n",
    "\n",
    "So let's test this new hypothesis as well.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Check if there are cases of several different `ProductID`s for the same `ReferenceOrderID`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same ReferenceOrderID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same ReferenceOrderID\n",
    "# Now group the dataframe by ReferenceOrderID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"ReferenceOrderID\"]\n",
    ").count()\n",
    "\n",
    "# Then check again if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# 23249 results => There are multiple cases of different ProductIDs for the same ReferenceOrderID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Our new hypothesis seems to be correct. I.e. in the next step we want to search for `ProductID`s that regularly occur in the same `ReferenceOrderID`: The frequent itemsets of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Identifing the Frequent Itemsets\n",
    "\n",
    "To be able to determine our frequent itemsets using mlxtend, we first need to do some preprocessing on `TransactionHistory`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Aggregate the `TransactionHistory` so that next to each `ReferenceOrderID`, the associated `ProductID`s are listed in a single cell.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Aggregate the TransactionHistory to have a list of ProductIDs per ReferenceOrderID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Aggregate the TransactionHistory to have a list of ProductIDs per ReferenceOrderID\n",
    "products_per_order_df = (\n",
    "    transaction_history_df.groupby(\"ReferenceOrderID\")[\"ProductID\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ProductIDs\")\n",
    "    .set_index(\"ReferenceOrderID\")\n",
    ")\n",
    "products_per_order_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Prepare the dataset for `mlxtend`s `fpgrowth` by using the `TransactionEncoder` of the library\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply one hot encoding to the prepared dataset by using the TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply one hot encoding to the prepared dataset by using the TransactionEncoder\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(\n",
    "    products_per_order_df[\"ProductIDs\"].tolist()\n",
    ").transform(products_per_order_df[\"ProductIDs\"].tolist())\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset,\n",
    "    columns=transaction_encoder.columns_,\n",
    "    index=products_per_order_df.index,\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After these necessary preprocessing steps, the frequent itemsets can now theoratically be determined. However, we definitely do not know at this point which min_support to choose. \n",
    "\n",
    "Even by trial and error, it is difficult to find a meaningful threshold here, since we have only been told by our fictitious bosses that we should find the ten most \"relevant\" frequent patterns.\n",
    "\n",
    "First of all, it would therefore make sense to determine rather too many itemsets than too few. It is easier to discard frequent itemsets later than to create additional ones.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Use `fpgrowth` to determine the frequent itemsets of our dataset. Select `min_support` so that the approximately 100 most frequent itemsets become frequent itemsets.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the frequent itemsets\n",
    "frequent_itemsets = fpgrowth(preprocessed_dataset, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Determination of the Frequent Patterns\n",
    "\n",
    "Before we use the frequent itemsets to determine frequent patterns, we must first determine how to define \"relevance\" in the context of frequent patterns.\n",
    "\n",
    "If our bosses meant that they wanted to know which ten patterns occur most frequently in our dataset, then support would be the appropriate measure. Did they want to know how certain one can be that Product A will end up in the shopping cart if Product B is already there? Then the calculation of confidence would be more appropriate. In addition, there are of course a large number of interestingness measures.\n",
    "\n",
    "All in all, this question cannot be answered conclusively. In practice, a dialog between management and you would be appropriate in order to narrow down more precisely what is meant by the most \"relevant\" ten patterns.  \n",
    "\n",
    "This ambiguity was intentionally used in the assignment to show that the assignment will often contain inaccuracies in the real world. \n",
    "\n",
    "However, while in the real world dialogue is the best solution, we have no opportunity to consult with our fictitious bosses. For this reason we do what is best for us and choose the simplest measure to apply: The support.\n",
    "\n",
    "It is not important at first that they really generate only the 10 rules with the highest support. If they are part of your list, everything is fine.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Use `mlxtend`s `association_rules` to generate frequent patterns from the frequent itemsets. Set the corresponding threshold so that at least the 10 frequent patterns with the highest support are included.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns\n",
    "frequent_patterns = association_rules(\n",
    "    frequent_itemsets, metric=\"support\", min_threshold=0.02\n",
    ")\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course, it is no problem at all to sort out extra patterns afterwards. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Delete all patterns that do not belong to the ten patterns with the highest support.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Delete the extra patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Delete the extra patterns\n",
    "frequent_patterns = frequent_patterns.nlargest(10, \"support\")\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Getting to Know the Product Names \n",
    "\n",
    "Even though we have now already completed the core task, it will probably do management little good to tell them that `ProductID` 871 is often purchased in addition to `ProductID` 870. After all, these are first and foremost internal database ids. \n",
    "\n",
    "To complete our task satisfactorily for the management, we still need to enrich the `ProductID`s in antecedents and consequents with their actual names.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Enrich the frequent patterns by adding the product names to the list. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge the ProductName into the frequent pattern df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge the ProductName into the frequent pattern df\n",
    "# We have to transform the frozensets within the two colums to strings first\n",
    "# (as we know that there is only one item per set this is pretty simple)\n",
    "frequent_patterns[\"antecedents\"] = frequent_patterns[\"antecedents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "frequent_patterns[\"consequents\"] = frequent_patterns[\"consequents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "\n",
    "# After that we have to merge frequent_patterns with the product df\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"antecedents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"Name\",\n",
    "        \"consequents\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"antecedents name\"})\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"consequents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"antecedents name\",\n",
    "        \"consequents\",\n",
    "        \"Name\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"consequents name\"})\n",
    "\n",
    "# Print the df\n",
    "frequent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Assignment completed! In the virtual scenario introduced in the beginning of this part you would now be able to report to the management that the `Mountain Bottle Cage` is often purchased in addition to the `Water Bottle - 30 oz.`. The same is true for the other nine requested frequent patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### Implementing the Kulczynski Measure and the Imbalance Ratio\n",
    "\n",
    "The library `mlxtend` offers some more measures besides support and confidence for the determination of frequent patterns. While lift, leverage and conviction are offered, the kulczynski metric and imbalance ratio presented in the lecture, for example, are not.\n",
    "\n",
    "Fortunately, the the antecedent support, the consequent support and the support calculated by `mlxtend` can easily be used to calculate these two interestingness measures.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write a function to compute the kulczynski measure, known from the lecture as `Kulc(a, b)`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return (support / 2) * ((1 / antecedent_support) + (1 / consequent_support))\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Write a function to compute the imbalance ratio.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return abs(antecedent_support - consequent_support) / (\n",
    "        antecedent_support + consequent_support - support\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course, the question arises again as to how these metrics are to be interpreted.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** Interpret the interestingness measures for the association rule `\"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "First both values must be interpreted separately from each other:\n",
    "\n",
    "- **Kulczynski Measure:**<br/>\n",
    "When kulczynski measure is close to 0 or 1 we have an \"interesting\" association rule. Since in this case the value is about 0.6, the kulczynski measure rather suggests that this association rule is uninteresting. \n",
    "- **Imbalance Ratio:**<br/>\n",
    "For the imbalance ratio, a value of 0 indicates a perfectly balanced association rule, while 1 indicates a very unbalanced one. In this case, we are about 0.52, which is about the middle of the spectrum. Thus, we cannot speak of a particularly well balanced rule, but neither can we speak of a completely unbalanced one.\n",
    "\n",
    "In summary, we have not discovered the most interesting rule, but one that is not completely uninteresting (this would the case for kulczynski measure = 0.5 and imbalance ratio = 0.0). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
