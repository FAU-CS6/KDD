{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# Exercise 1: Mining Frequent Patterns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "We start with a small example in the form of a transactional data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# A very small data set in the form of a list (transactions) of sets (items)\n",
    "dataset = [\n",
    "    {\"Beer\", \"Nuts\", \"Diapers\"},\n",
    "    {\"Beer\", \"Coffee\", \"Diapers\"},\n",
    "    {\"Beer\", \"Diapers\", \"Eggs\"},\n",
    "    {\"Nuts\", \"Eggs\", \"Milk\"},\n",
    "    {\"Nuts\", \"Coffee\", \"Diapers\", \"Eggs\"},\n",
    "]\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "From this data set, we want to extract the frequent itemsets. \n",
    "\n",
    "We use the library `mlxtend`, which contains an implementation for the two methods Apriori and FP-growth known from the lecture. In doing so, it is necessary to prepare the data set for the use of `apriori` or `fpgrowth`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task 1:** \n",
    "    \n",
    "Take a look at the `mlxtend` [documentation](http://rasbt.github.io/mlxtend/USER_GUIDE_INDEX/) for information on how dataset must be structured for `apriori`/`fpgrowth` and preprocess our `dataset` accordingly.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(dataset).transform(dataset)\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset, columns=transaction_encoder.columns_\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "After this preparation, the determination of the frequent itemset in our dataset is possible by using `apriori` or `fpgrowth`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Using `apriori` or `fpgrowth` from `mlxtend`, determine the frequent itemsets in our dataset.\n",
    "Use a `min_support` of 0.4.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use apriori from mlxtend to determine the frequent itemsets in our dataset\n",
    "frequent_itemsets = apriori(preprocessed_dataset, min_support=0.4, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Use fpgrowth from mlxtend to determine the frequent itemsets in our dataset\n",
    "frequent_itemsets = fpgrowth(preprocessed_dataset, min_support=0.4, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In order to obtain frequent patterns from the frequent itemsets, the `mlxtend` function `association_rules` can be used.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 3:**\n",
    "    \n",
    "Generate all association rules with a support of at least 0.4 from the frequent itemsets using `association_rules` from `mlxtend`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns\n",
    "frequent_patterns = association_rules(\n",
    "    frequent_itemsets, metric=\"support\", min_threshold=0.4\n",
    ")\n",
    "frequent_patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "What was quite easy in the small example is usually a bit more difficult in reality. For example, data is often not available in transactional form and must first be adapted accordingly.\n",
    "\n",
    "Imagine the following scenario:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "*You are an employee in the fictitious company Adventure Works GmbH.*\n",
    "*Your job is to find out which of the company's products are frequently bought together.*\n",
    "*To start with, the management wants you to find the ten most \"relevant\" product pairs bought together.*\n",
    "\n",
    "*You get access to the OLTP database of the company.*\n",
    "*The information about individual transactions can be found in the relation `TransactionHistory`.*\n",
    "*The translation of ProductIDs into real product names can be done with the help of the relation `Product`.*\n",
    "\n",
    "*Given are some helpful libraries and the code to connect to the OLTP database:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import helpful libraries\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 4:**\n",
    "    \n",
    "Find the ten most \"relevant\" frequent patterns in the OLTP database of the fictitious Adventure Works GmbH.\n",
    "You have to implement every step from loading the DataFrames to determining the Association Rules from the Frequent Itemsets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Find the ten most \"relevant\" frequent patterns (Code placeholder 10/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "First the records of `TransactionHistory` and `Product` have to be loaded.\n",
    "Since we don't know anything about the relations we load all attributes and tuples for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load TransactionHistory into a DataFrame and display the first ten rows\n",
    "transaction_history_df = pd.read_sql_query(\n",
    "    \"SELECT * FROM TransactionHistory\", connection\n",
    ")\n",
    "transaction_history_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame and display the first ten rows\n",
    "product_df = pd.read_sql_query(\"SELECT * FROM Product\", connection)\n",
    "product_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The `TransactionHistory` seems to contain information about individual transactions.\n",
    "But it is not yet possible to see how it is possible to determine which products (probably identified via the `ProductID`) are purchased together.\n",
    "\n",
    "The Product table can be used to map from `ProductID` to `Name`.\n",
    "\n",
    "We might therefore assume that we are looking for products that are purchased in the same transaction.\n",
    "The attribute `TransactionID` seems to uniquely identify each transaction.\n",
    "We test this hypothesis by determining whether there are `TransactionID`s with more than one linked `ProductID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same TransactionID\n",
    "# First group the dataframe by TransactionID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"TransactionID\"]\n",
    ").count()\n",
    "\n",
    "# Then check if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# No results => There are no cases of several different ProductIDs for the same TransactionID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We find no cases of multiple different `ProductID`s for the same `TransactionID` so our first hypothesis not seem to be correct.\n",
    "Apparently the `TransactionID` is the primary key for the `TransactionHistory` relation and one `TransactionID` cannot refer to different `ProductID`s.\n",
    "\n",
    "When looking at `TransactionHistory` a second attribute stands out.\n",
    "The `ReferenceOrderID` could identify the individual order and products that are part of the same order are obviously purchased together.\n",
    "\n",
    "So let's test this new hypothesis as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the TransactionHistory DataFrame for cases of several different ProductIDs for the same ReferenceOrderID\n",
    "# Now group the dataframe by ReferenceOrderID and aggregate the other columns by counting different values\n",
    "transaction_history_df_grouped = transaction_history_df.groupby(\n",
    "    [\"ReferenceOrderID\"]\n",
    ").count()\n",
    "\n",
    "# Then check again if there are results there cells inb ProductID there the count of different values is greater than one\n",
    "transaction_history_df_grouped[transaction_history_df_grouped[\"ProductID\"] > 1]\n",
    "\n",
    "# 23249 results => There are multiple cases of different ProductIDs for the same ReferenceOrderID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Our new hypothesis seems to be correct.\n",
    "In the next step we want to search for `ProductID`s that regularly occur in the same `ReferenceOrderID`, the frequent itemsets of our problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "To determine our frequent itemsets using mlxtend we first need to do some preprocessing on `TransactionHistory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Aggregate the TransactionHistory to have a list of ProductIDs per ReferenceOrderID\n",
    "products_per_order_df = (\n",
    "    transaction_history_df.groupby(\"ReferenceOrderID\")[\"ProductID\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ProductIDs\")\n",
    "    .set_index(\"ReferenceOrderID\")\n",
    ")\n",
    "products_per_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Apply one hot encoding to the prepared dataset by using the TransactionEncoder\n",
    "# Create a TransactionEncoder\n",
    "transaction_encoder = TransactionEncoder()\n",
    "\n",
    "# Use the TransactionEncoder to transform the dataset into a one-hot encoded NumPy boolean array\n",
    "one_hot_encoded_dataset = transaction_encoder.fit(\n",
    "    products_per_order_df[\"ProductIDs\"].tolist()\n",
    ").transform(products_per_order_df[\"ProductIDs\"].tolist())\n",
    "\n",
    "# Transform the one-hot encoded array into a pandas DataFrame\n",
    "preprocessed_dataset = pd.DataFrame(\n",
    "    one_hot_encoded_dataset,\n",
    "    columns=transaction_encoder.columns_,\n",
    "    index=products_per_order_df.index,\n",
    ")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "After the preprocessing steps the frequent itemsets can now theoretically be determined.\n",
    "But we definitely do not know which min_support to choose. \n",
    "\n",
    "Even by trial and error it is difficult to find a meaningful threshold here and we have only been told by our bosses to find the ten most \"relevant\" frequent patterns.\n",
    "\n",
    "First of all we should determine rather too many itemsets than too few because it is easier to discard frequent itemsets later than to create additional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Determine the frequent itemsets\n",
    "frequent_itemsets = fpgrowth(preprocessed_dataset, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Before using the frequent itemsets to determine frequent patterns we have to determine how to define \"relevance\" in the context of frequent patterns.\n",
    "\n",
    "If our bosses wanted to know which ten patterns occur most frequently in our dataset the *support* would be the appropriate measure.\n",
    "Did they wanted to know how certain one can be that Product A will end up in the shopping cart if Product B is already there?\n",
    "Then the calculation of *confidence* would be more appropriate.\n",
    "In addition there is a large number of other interestingness measures.\n",
    "\n",
    "All in all, this question cannot be answered conclusively.\n",
    "In practice a dialog with the management would be appropriate in order to narrow down more precisely what is meant by the most \"relevant\" ten patterns.\n",
    "\n",
    "This ambiguity was intentionally used in the assignment to show that the assignment will often contain inaccuracies in the real world. \n",
    "\n",
    "While in the real world dialogue is the best solution we have no opportunity to consult with our fictitious bosses.\n",
    "Therefore we do what is best for us and choose the simplest measure to apply: the support.\n",
    "\n",
    "It is not important that we generate only the 10 rules with the highest support.\n",
    "As long as they are part of your list, everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the association rules/frequent patterns\n",
    "frequent_patterns = association_rules(\n",
    "    frequent_itemsets, metric=\"support\", min_threshold=0.02\n",
    ")\n",
    "frequent_patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Of course it is no problem at all to sort out extra patterns afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Delete the extra patterns\n",
    "frequent_patterns = frequent_patterns.nlargest(10, \"support\")\n",
    "frequent_patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "After completing the core task we tidy up the results, because management can not use the Information that `ProductID` 871 is often purchased in combination with `ProductID` 870 because these are internal database ids. \n",
    "\n",
    "To complete our task satisfactorily for the management we still need to enrich the `ProductID`s with their actual names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge the ProductName into the frequent pattern df\n",
    "# We have to transform the ids within the two columns to strings first\n",
    "# (as we know that there is only one item per set this is pretty simple)\n",
    "frequent_patterns[\"antecedents\"] = frequent_patterns[\"antecedents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "frequent_patterns[\"consequents\"] = frequent_patterns[\"consequents\"].apply(\n",
    "    lambda x: list(x)[0]\n",
    ")\n",
    "\n",
    "# After that we have to merge frequent_patterns with the product df\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"antecedents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"Name\",\n",
    "        \"consequents\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"antecedents name\"})\n",
    "frequent_patterns = pd.merge(\n",
    "    frequent_patterns, product_df, left_on=\"consequents\", right_on=\"ProductID\"\n",
    ")[\n",
    "    [\n",
    "        \"antecedents\",\n",
    "        \"antecedents name\",\n",
    "        \"consequents\",\n",
    "        \"Name\",\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\",\n",
    "        \"support\",\n",
    "        \"confidence\",\n",
    "        \"lift\",\n",
    "        \"leverage\",\n",
    "        \"conviction\",\n",
    "    ]\n",
    "]\n",
    "frequent_patterns = frequent_patterns.rename(columns={\"Name\": \"consequents name\"})\n",
    "\n",
    "# Print the df\n",
    "frequent_patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Now the assignment is fully completed!\n",
    "In the virtual scenario introduced in the beginning of this part you would now be able to report to the management that the `Mountain Bottle Cage` is often purchased in combination with the `Water Bottle - 30 oz.`.\n",
    "The same is true for the other nine requested frequent patterns.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "\n",
    "The library `mlxtend` offers some more measures besides support and confidence for the determination of frequent patterns.\n",
    "While lift, leverage and conviction are offered, the kulczynski metric and imbalance ratio presented in the lecture are not.\n",
    "\n",
    "We can use the antecedent support, the consequent support and the support calculated by `mlxtend` to easily calculate these interestingness measures.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 5:** \n",
    "    \n",
    "Write a function to compute the kulczynski measure known from the lecture as `Kulc(a, b)`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function kulczynski_measure to compute the kulczynski measure\n",
    "def kulczynski_measure(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return (support / 2) * ((1 / antecedent_support) + (1 / consequent_support))\n",
    "\n",
    "\n",
    "# Compute the kulczynski measure for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "kulczynski_measure(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 6:**\n",
    "    \n",
    "Write a function to compute the imbalance ratio.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # ...\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Complete the function imbalance_ratio to compute the imbalance ratio\n",
    "def imbalance_ratio(antecedent_support, consequent_support, support):\n",
    "    # Simply use the formula introduced in the lecture\n",
    "    return abs(antecedent_support - consequent_support) / (\n",
    "        antecedent_support + consequent_support - support\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the imbalance ratio for \"Water Bottle - 30 oz.\" -> \"Mountain Bottle Cage\"\n",
    "imbalance_ratio(0.112802, 0.049356, 0.041139)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Now the question arises as to how these metrics have to be interpreted.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 7:**\n",
    "    \n",
    "Interpret the interestingness measures for the association rule `\"Water Bottle - 30 oz.\"` $\\rightarrow$ `\"Mountain Bottle Cage\"`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "First both values must be interpreted separately from each other:\n",
    "\n",
    "- **Kulczynski Measure:**  \n",
    "When kulczynski measure is close to 0 or 1 we have an \"interesting\" association rule.\n",
    "Since in this case the value is about 0.6, the kulczynski measure rather suggests that this association rule is uninteresting. \n",
    "- **Imbalance Ratio:**  \n",
    "For the imbalance ratio, a value of 0 indicates a perfectly balanced association rule while 1 indicates a very unbalanced one.\n",
    "In this case we are about 0.52, which is about the middle of the spectrum.\n",
    "Thus we cannot speak of a particularly well balanced rule, but neither can we speak of a completely unbalanced one.\n",
    "\n",
    "In summary we have not discovered the most interesting rule, but one that is not completely uninteresting (this would the case for kulczynski measure = 0.5 and imbalance ratio = 0.0). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
