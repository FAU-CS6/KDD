{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 5. Clustering\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Clustering*.\n",
    "The series itself is based on lecture *8. Cluster Analysis*.\n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing k-means and DBScan\n",
    "- Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "Recall that we have two exercise groups.\n",
    "Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety.\n",
    "If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Implementing k-means and DBScan\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "TODO\n",
    "\n",
    "</div>\n",
    "\n",
    "- [ ] Rework the cluster/partition naming scheme\n",
    "- [ ] Think of more pythonic solutions/whether they make sense in that case or confuse the students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(\n",
    "    [\n",
    "        [6, 5],\n",
    "        [8, 5],\n",
    "        [4, 3],\n",
    "        [5, 6],\n",
    "        [6, 2],\n",
    "        [6, 3],\n",
    "        [2, 2],\n",
    "        [3, 3],\n",
    "        [4, 4],\n",
    "        [5, 5],\n",
    "        [6, 6],\n",
    "        [7, 7],\n",
    "        [8, 7],\n",
    "        [8, 4],\n",
    "    ],\n",
    "    columns=[\"x\", \"y\"],\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=dataset[\"x\"], y=dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Step: Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(dataset, k):\n",
    "    \"\"\"\n",
    "    Arbitrarily partition the dataset into k non-empty partitions.\n",
    "    Adds a third column to indicate the affiliation to a specific cluster.\n",
    "    \"\"\"\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster affiliation later\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # The method that was used in the lecture example is to sort the samples regarding to their y values\n",
    "    dataset_copy = dataset_copy.sort_values(by=[\"y\"]).reset_index(drop=True)\n",
    "\n",
    "    # Then to define the size of each cluster\n",
    "    cluster_size = round(dataset_copy.shape[0] / k)\n",
    "\n",
    "    # And then to assing the samples to the clusters\n",
    "    for i in range(0, dataset_copy.shape[0], cluster_size):\n",
    "        # Start of the slice\n",
    "        start = i\n",
    "\n",
    "        # End of the slice\n",
    "        end = min(i + cluster_size, dataset_copy.shape[0])\n",
    "\n",
    "        # Cluster id\n",
    "        cluster_id = i / cluster_size\n",
    "\n",
    "        # Assign the cluster value\n",
    "        dataset_copy.loc[start : end - 1, \"cluster\"] = cluster_id\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "partitioned_dataset = partition_dataset(dataset, 2)\n",
    "partitioned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x=partioned_dataset[\"x\"], y=partioned_dataset[\"y\"], hue=partioned_dataset[\"cluster\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Step: Compute the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(partitioned_dataset, k):\n",
    "    \"\"\"\n",
    "    Compute the centroids of each of the k partitions\n",
    "    \"\"\"\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # Compute the centroid of each partition\n",
    "    for i in range(0, k):\n",
    "        # Compute the mean of the x values within that single partition\n",
    "        x_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"x\"].mean()\n",
    "\n",
    "        # Compute the mean of the y values within that single partition\n",
    "        y_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"y\"].mean()\n",
    "\n",
    "        # Add the centroid of this single partition\n",
    "        centroids.loc[i, [\"x\", \"y\"]] = [x_mean, y_mean]\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "centroids = compute_centroids(partitioned_dataset, 2)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Step: Assign each sample to the cluster of the nearest centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two points a and b.\n",
    "    \"\"\"\n",
    "    return (abs(a - b) ** 2).sum() ** 0.5\n",
    "\n",
    "\n",
    "a = pd.Series([1, 9])\n",
    "b = pd.Series([9, 5])\n",
    "euclidean_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassign(partitioned_dataset, centroids, k):\n",
    "    \"\"\"\n",
    "    (Re)assigns each sample to the cluster of its nearest centroid.\n",
    "    Also returns a boolean that signals whether there was at least one sample reassigned.\n",
    "    \"\"\"\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # Check for each sample whether it has to be reassigned\n",
    "    for i in range(0, dataset_copy.shape[0]):\n",
    "        # Get the value of the the dataset for easier access\n",
    "        sample = dataset_copy.loc[i, [\"x\", \"y\"]]\n",
    "\n",
    "        # Set the current cluster id and centroid values\n",
    "        current_cluster = dataset_copy.loc[i, \"cluster\"]\n",
    "        current_centroid = centroids.loc[current_cluster]\n",
    "        current_distance = euclidean_distance(sample, current_centroid)\n",
    "\n",
    "        # print(sample)\n",
    "        # print(current_cluster)\n",
    "        # print(current_centroid)\n",
    "        # print(current_distance)\n",
    "        # print()\n",
    "\n",
    "        # Iterate through the centroids and check whether the distance is lower than the current distance\n",
    "        # NOTE: We do not skip the current centroid, as this would complicate the code and isn't a big performance problem\n",
    "        for j in range(0, k):\n",
    "            # Compute the distance\n",
    "            distance = euclidean_distance(sample, centroids.loc[j])\n",
    "\n",
    "            # If the distance is lower than the current_distance we have to reassign\n",
    "            if distance < current_distance:\n",
    "\n",
    "                # Set the cluster\n",
    "                dataset_copy.loc[i, \"cluster\"] = j\n",
    "                current_cluster = j\n",
    "\n",
    "                # Set the current_centroid\n",
    "                current_centroid = centroids.loc[j]\n",
    "\n",
    "                # Set the current_distance\n",
    "                current_distance = distance\n",
    "\n",
    "                # Set the reassign_indicator\n",
    "                reassign_indicator = True\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "reassign_indicator, reassigned_dataset = reassign(partitioned_dataset, centroids, 2)\n",
    "\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "reassigned_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Step: Merge the functions into a wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(dataset, k):\n",
    "    \"\"\"\n",
    "    Wrapper function to implement the k_means clustering\n",
    "    \"\"\"\n",
    "\n",
    "    # Partition the dataset\n",
    "    dataset = partition_dataset(dataset, k)\n",
    "\n",
    "    # Set the reassign_indicator to True (as the intial partitioning was as reassingment in itself)\n",
    "    reassign_indicator = True\n",
    "\n",
    "    # As long as there are reassingment the following two steps are repeated\n",
    "    while reassign_indicator:\n",
    "        # Compute the centroids\n",
    "        centroids = compute_centroids(dataset, k)\n",
    "\n",
    "        # Reassign each sample to the cluster of the nearest centroid\n",
    "        reassign_indicator, dataset = reassign(dataset, centroids, k)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster our dataset\n",
    "clustered_dataset = k_means(dataset, 2)\n",
    "clustered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x=clustered_dataset[\"x\"], y=clustered_dataset[\"y\"], hue=clustered_dataset[\"cluster\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution found in the Internet (copied for comparison reasons - will not be part of the final exercise)\n",
    "\n",
    "Source: https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "centers = 5\n",
    "X_train, true_labels = make_blobs(n_samples=100000, centers=centers, random_state=2306)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "sns.scatterplot(\n",
    "    x=[X[0] for X in X_train],\n",
    "    y=[X[1] for X in X_train],\n",
    "    hue=true_labels,\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(point, data):\n",
    "    \"\"\"\n",
    "    Euclidean distance between point & data.\n",
    "    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((point - data) ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=8, max_iter=300):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X_train):\n",
    "        # Randomly select centroid start points, uniformly distributed across the domain of the dataset\n",
    "        min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)\n",
    "        self.centroids = [np.random.uniform(min_, max_) for _ in range(self.n_clusters)]\n",
    "        # Iterate, adjusting centroids until converged or until passed max_iter\n",
    "        iteration = 0\n",
    "        prev_centroids = None\n",
    "        while (\n",
    "            np.not_equal(self.centroids, prev_centroids).any()\n",
    "            and iteration < self.max_iter\n",
    "        ):\n",
    "            # Sort each datapoint, assigning to nearest centroid\n",
    "            sorted_points = [[] for _ in range(self.n_clusters)]\n",
    "            for x in X_train:\n",
    "                dists = euclidean(x, self.centroids)\n",
    "                centroid_idx = np.argmin(dists)\n",
    "                sorted_points[centroid_idx].append(x)\n",
    "            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n",
    "            prev_centroids = self.centroids\n",
    "            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                if np.isnan(\n",
    "                    centroid\n",
    "                ).any():  # Catch any np.nans, resulting from a centroid having no points\n",
    "                    self.centroids[i] = prev_centroids[i]\n",
    "            iteration += 1\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        centroids = []\n",
    "        centroid_idxs = []\n",
    "        for x in X:\n",
    "            dists = euclidean(x, self.centroids)\n",
    "            centroid_idx = np.argmin(dists)\n",
    "            centroids.append(self.centroids[centroid_idx])\n",
    "            centroid_idxs.append(centroid_idx)\n",
    "        return centroids, centroid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=centers)\n",
    "kmeans.fit(X_train)\n",
    "# View results\n",
    "class_centers, classification = kmeans.evaluate(X_train)\n",
    "sns.scatterplot(\n",
    "    x=[X[0] for X in X_train],\n",
    "    y=[X[1] for X in X_train],\n",
    "    hue=true_labels,\n",
    "    style=classification,\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.plot(\n",
    "    [x for x, _ in kmeans.centroids],\n",
    "    [y for _, y in kmeans.centroids],\n",
    "    \"+\",\n",
    "    markersize=10,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "TODO\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
