{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 5. Clustering\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Clustering*.\n",
    "The series itself is based on lecture *8. Cluster Analysis*.\n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing k-means and DBScan\n",
    "- Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "Again we would like to remind you that we have multiple exercise groups.\n",
    "Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety.\n",
    "If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part One: Implementing k-means and DBScan\n",
    "\n",
    "In the first part of this exercise sheet we will take a closer look at two clustering methods known from the lecture: k-means and DBScan. You will be asked to implement both methods from scratch. In both cases you get the option to implement the algortithms totally on your self or with the help of a step-by-step task series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The datasets to cluster are the same for both methods. On the one hand, there is the `small_dataset`, which is used as an example for k-means in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the small_dataset\n",
    "small_dataset = pd.DataFrame(\n",
    "    [\n",
    "        [6, 5],\n",
    "        [8, 5],\n",
    "        [4, 3],\n",
    "        [5, 6],\n",
    "        [6, 2],\n",
    "        [6, 3],\n",
    "        [2, 2],\n",
    "        [3, 3],\n",
    "        [4, 4],\n",
    "        [5, 5],\n",
    "        [6, 6],\n",
    "        [7, 7],\n",
    "        [8, 7],\n",
    "        [8, 4],\n",
    "    ],\n",
    "    columns=[\"x\", \"y\"],\n",
    ")\n",
    "\n",
    "# Output the small_dataset in a scatterplot diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=small_dataset[\"x\"], y=small_dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "On the other hand, there is the `big_dataset`, a larger dataset that was generated using `sklearn` and for which a ground trouth (column `true_labels`) exists to validate the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the big_dataset\n",
    "big_data, big_data_true_labels = make_blobs(\n",
    "    n_samples=1000, centers=10, random_state=2306\n",
    ")\n",
    "big_dataset = pd.DataFrame(big_data, columns=[\"x\", \"y\"])\n",
    "big_dataset[\"true_labels\"] = big_data_true_labels\n",
    "\n",
    "# Print a scatterplot diagram of the big_dataset while showing the actual class affiliations with colors\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=big_dataset[\"x\"],\n",
    "    y=big_dataset[\"y\"],\n",
    "    hue=big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "If you decide to take a standalone approach to implementation, we recommend that you first use the `small_dataset`. A clustering run on the `big_dataset` takes its time and is thus not so well suited for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### K-means\n",
    "\n",
    "The first approach you are asked to implement is k-means. It is part of the so called partitioning methods and is the first approach we took a look at in the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "As announced, there are two options for you regarding the implementation of k-means. You may implement the method without extra help or you can choose option two: A guided step-by-step implementation of the k-means algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Implement K-means on Your Own\n",
    "\n",
    "Some of you may prefer to implement k-means on your own. In this case refer to the lecture for a comprehensive explanation of the method. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use your knowlegde of k-means to implement a method `k_means` that can be used to cluster the two datasets `small_dataset` and `big_dataset` into `k` clusters.\n",
    "If you are in need of more code cells than provided, feel free to add more.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample k_means sceleton\n",
    "# NOTE: You are allowed to use this sceleton but don't have to\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the small_dataset (just as in the lecture we use k=2)\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the big_dataset (as we know that there are 10 classes, we use k=10)\n",
    "clustered_big_dataset = k_means(big_dataset, 10)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 2: Implement K-means by Solving Small Tasks\n",
    "\n",
    "When someone tries to implement k-means step-by-step, the initial step is always to make an initial partition of the existing data into `k` non-empty partitions. This division can be random or according to an arbitrary scheme. However it is important that the result are exactly `k` partitions, that none of these partitions is empty and that each sample is represented in exactly one of the partitions. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `partition_dataset` that splits a `dataset` into `k` initial partitions. It doesn`t matter what kind of partitioning you decide on, as long as it complies with the rules mentioned. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # The method that was used in the lecture example is to sort the samples regarding to their y values\n",
    "    dataset_copy = dataset_copy.sort_values(by=[\"y\"]).reset_index(drop=True)\n",
    "\n",
    "    # Then to define an equal size for each cluster/partition\n",
    "    cluster_size = round(dataset_copy.shape[0] / k)\n",
    "\n",
    "    # And then to assign the samples to the cluster/partition\n",
    "    for i in range(0, dataset_copy.shape[0], cluster_size):\n",
    "        # Start of the slice\n",
    "        start = i\n",
    "\n",
    "        # End of the slice\n",
    "        end = min(i + cluster_size, dataset_copy.shape[0])\n",
    "\n",
    "        # Cluster id\n",
    "        cluster_id = i / cluster_size\n",
    "\n",
    "        # Assign the cluster value\n",
    "        dataset_copy.loc[start : end - 1, \"cluster\"] = cluster_id\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The first repetitive step in k-means is to calculate for the so-called centroids (mean points) for each partition/cluster. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement the function `compute_centroids` that computes the centroid for each of the `k` partitions. The return value should be a pandas DataFrame with the cluster identifier as an index and two columns `x` and `y` indicating the coordinates of the corresponding centroid.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 2)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # Compute the centroid of each partition\n",
    "    for i in range(0, k):\n",
    "        # Compute the mean of the x values within that single partition\n",
    "        x_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"x\"].mean()\n",
    "\n",
    "        # Compute the mean of the y values within that single partition\n",
    "        y_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"y\"].mean()\n",
    "\n",
    "        # Add the centroid of this single partition\n",
    "        centroids.loc[i, [\"x\", \"y\"]] = [x_mean, y_mean]\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 2)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "To reassign points to their nearest centroid, distance measure must be defined. Here, for example, the Euclidean distance comes in handy, which we have already implemented ourselves in an earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# \"Pythonic\" implementation of the euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return (abs(a - b) ** 2).sum() ** 0.5\n",
    "\n",
    "\n",
    "# Compute the euclidean distance for two random points a and b\n",
    "a = pd.Series([1, 9])\n",
    "b = pd.Series([9, 5])\n",
    "euclidean_distance(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Reassignment is also the next step within k-means. Samples are always reassigned to the cluster/partition whose centroid is closest to themselves.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function `reassign_samples` that reassigns samples to the cluster/partition whose centroid is closest to themselves. Return the dataset and an indictator to communicate whether at least tuple was reassigned within the function or not.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 2\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # Check for each sample whether it has to be reassigned\n",
    "    for i in range(0, dataset_copy.shape[0]):\n",
    "        # Get the value of the the dataset for easier access\n",
    "        sample = dataset_copy.loc[i, [\"x\", \"y\"]]\n",
    "\n",
    "        # Set the current cluster id and centroid values\n",
    "        current_cluster = dataset_copy.loc[i, \"cluster\"]\n",
    "        current_centroid = centroids.loc[current_cluster]\n",
    "        current_distance = euclidean_distance(sample, current_centroid)\n",
    "\n",
    "        # Iterate through the centroids and check whether the distance is lower than the current distance\n",
    "        # NOTE: We do not skip the current centroid, as this would complicate the code and isn't a big performance problem\n",
    "        for j in range(0, k):\n",
    "            # Compute the distance\n",
    "            distance = euclidean_distance(sample, centroids.loc[j])\n",
    "\n",
    "            # If the distance is lower than the current_distance we have to reassign\n",
    "            if distance < current_distance:\n",
    "\n",
    "                # Set the cluster\n",
    "                dataset_copy.loc[i, \"cluster\"] = j\n",
    "                current_cluster = j\n",
    "\n",
    "                # Set the current_centroid\n",
    "                current_centroid = centroids.loc[j]\n",
    "\n",
    "                # Set the current_distance\n",
    "                current_distance = distance\n",
    "\n",
    "                # Set the reassign_indicator\n",
    "                reassign_indicator = True\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 2\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In the iterative k-means algorithm it would now be checked whether samples were reassigned or not. If yes, we have to go back to calculating the centroids for this new assignment. If not, then the corresponding clusters have been found. \n",
    "\n",
    "This decision can of course be passed to a wrapper function `k_means` which summarizes the whole algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Merge the previously implemented function `k_means` to achieve a complete implementation of the algorithm.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Partition the dataset\n",
    "    dataset = partition_dataset(dataset, k)\n",
    "\n",
    "    # Set the reassign_indicator to True (as the intial partitioning was as reassingment in itself)\n",
    "    reassign_indicator = True\n",
    "\n",
    "    # As long as there are reassingment the following two steps are repeated\n",
    "    while reassign_indicator:\n",
    "        # Compute the centroids\n",
    "        centroids = compute_centroids(dataset, k)\n",
    "\n",
    "        # Reassign each sample to the cluster of the nearest centroid\n",
    "        reassign_indicator, dataset = reassign_samples(dataset, centroids, k)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Validation\n",
    "\n",
    "If we take a look at the `big_dataset` with its `true_labels`, we can identify some areas that might lead to problems with k-means due to overlapping of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Print a scatterplot diagram of the big_dataset while showing the actual class affiliations with colors\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=big_dataset[\"x\"],\n",
    "    y=big_dataset[\"y\"],\n",
    "    hue=big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "If you now run your k-means implementation on this `big_dataset`, you will probably also see some areas where the classes were not well recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the big_dataset\n",
    "clustered_big_dataset = k_means(big_dataset, 10)\n",
    "\n",
    "# Output the clustered dataset including information on the true classes\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    style=clustered_big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "It is important to mention that these problems may well be caused by k-means itself and not necessarily by faulty programming on your part. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Compare the actual class partitioning and the partitioning determined by your k-means implementation. Describe the most prominent misidentifications.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Especially due to the fact that the initial assignment can be done arbitrarily, the results of the student implementation and this sample solution are probably very different. For this reason, we focus in this description explicitly only on the differences within the sample solution:\n",
    "\n",
    "- Our k-means implementation created a mega cluster of original two classes around the point -10, 5. These are not correctly recognized by our k-means implementation.\n",
    "- Another mega cluster also exists around point 7, 7.5: again, two classes were detected as only one cluster.\n",
    "- To compensate for the two incorrectly recognized classes, our k-means implementation created four clusters around the point 0, 3. Originally, this was a region of two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Especially with strongly mixed classes k-means has problems. However, this is not unusual for a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: scikit-learn\n",
    "\n",
    "Even with the clustering algorithms from this task sheet, it is of course not normally necessary to create your own implementations for the procedures. In the case of k-means, for example, there is a good implementation in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's implementation of k-means to find ten clusters in the `big_dataset`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns k-means clustering on the big_dataset\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns k-means clustering on the big_dataset\n",
    "kmeans = KMeans(n_clusters=10).fit(big_dataset[[\"x\", \"y\"]])\n",
    "\n",
    "# Save the labels to a copy of the big_dataset to generate the equivalent of our clustered_big_dataset\n",
    "clustered_big_dataset_2 = big_dataset.copy()\n",
    "clustered_big_dataset_2[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# Print the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset_2[\"x\"],\n",
    "    y=clustered_big_dataset_2[\"y\"],\n",
    "    hue=clustered_big_dataset_2[\"cluster\"],\n",
    "    style=clustered_big_dataset_2[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "This shows how useful it can be to optimize the selection of the start value. By default, Scikit-learn uses a method called k-means++, which leads to a significantly better recognition of the true classes contained in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "TODO\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to the dataset to save the status of the dataset\n",
    "def prepare_dataset(dataset):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation\n",
    "    # Special codings for ...\n",
    "    # ... points that are not set yet: -1\n",
    "    # ... points that are noise: -2\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Create a new empty column to save the visited status\n",
    "    dataset_copy[\"visited\"] = False\n",
    "\n",
    "    # Return the dataset_copy\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Prepare the small dataset\n",
    "prepared_small_dataset = prepare_dataset(small_dataset)\n",
    "prepared_small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random point that is unvisited\n",
    "def pick_random_unvisited_point(dataset):\n",
    "    # Select all points that are unvisited\n",
    "    unvisited_points = dataset[dataset[\"visited\"] == False]\n",
    "\n",
    "    # If there are no unvisited points return None\n",
    "    if len(unvisited_points) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        # Select one random point and return it\n",
    "        return unvisited_points.sample().iloc[0]\n",
    "\n",
    "\n",
    "# Pick a random point\n",
    "random_point = pick_random_unvisited_point(prepared_small_dataset)\n",
    "random_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Pythonic\" implementation of the euclidean distance (repetition)\n",
    "def euclidean_distance(a, b):\n",
    "    return (abs(a - b) ** 2).sum() ** 0.5\n",
    "\n",
    "\n",
    "# Compute the euclidean distance for two points\n",
    "a = pd.Series([1, 9])\n",
    "b = pd.Series([9, 5])\n",
    "euclidean_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all points within a distance of eps next to a specific point\n",
    "def get_all_points_within_eps_distance(point, dataset, eps):\n",
    "    # Select all unvisited points within eps distance\n",
    "    return dataset[\n",
    "        dataset.apply(\n",
    "            lambda a: euclidean_distance([a[\"x\"], a[\"y\"]], point[[\"x\", \"y\"]].values)\n",
    "            <= eps,\n",
    "            axis=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# Get all points within distance of 1 regarding to the point (6,5)\n",
    "points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "    pd.Series(data=[6, 5, -1, False], index=[\"x\", \"y\", \"cluster\", \"visited\"]),\n",
    "    prepared_small_dataset,\n",
    "    1,\n",
    ")\n",
    "points_within_eps_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function to expand the cluster by checking whether a point is a core point\n",
    "# and then adding the neighbor points to the cluster\n",
    "def expand_cluster(dataset, eps, min_pts, point, cluster_id):\n",
    "    # Add the point to the cluster\n",
    "    dataset.loc[point.name, \"cluster\"] = cluster_id\n",
    "\n",
    "    # If point was not visited, we have to visit it now\n",
    "    if dataset.loc[point.name, \"visited\"] == False:\n",
    "        # Mark the point as visited\n",
    "        dataset.loc[point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        # => We have to go deeper into the recursion\n",
    "        if len(points_within_eps_distance) >= min_pts:\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Prepare the dataset\n",
    "    dataset = prepare_dataset(dataset)\n",
    "\n",
    "    # While there are unvisited points pick a random one\n",
    "    while len(dataset[dataset[\"visited\"] == False]) > 0:\n",
    "        # Select a random unvisited point\n",
    "        random_point = pick_random_unvisited_point(dataset)\n",
    "\n",
    "        # Mark the random point as visited\n",
    "        dataset.loc[random_point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            random_point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        if len(points_within_eps_distance) < min_pts:\n",
    "            # Not a core point => mark as noise\n",
    "            dataset.loc[random_point.name, \"cluster\"] = -2\n",
    "        else:\n",
    "            # Get the last used cluster id\n",
    "            last_cluster_id = dataset[\"cluster\"].max()\n",
    "\n",
    "            # Increment the id to get an new id for the new cluster\n",
    "            new_cluster_id = last_cluster_id + 1\n",
    "\n",
    "            # Add the random point to the cluster\n",
    "            dataset.loc[random_point.name, \"visited\"] = True\n",
    "\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, new_cluster_id)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 1, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Prepare the dataset\n",
    "    dataset = prepare_dataset(dataset)\n",
    "\n",
    "    # Select a random unvisited point\n",
    "    random_point = pick_random_unvisited_point(dataset)\n",
    "\n",
    "    # Mark the point as visited\n",
    "    random_point[\"visited\"] = True\n",
    "    dataset.loc[random_point.index[0], \"visited\"] = True\n",
    "\n",
    "    # Get the last used ID\n",
    "    last_cluster = dataset[\"cluster\"].max()\n",
    "\n",
    "    # Increment the id to get an new id for the new cluster\n",
    "    new_cluster = last_cluster + 1\n",
    "\n",
    "    # print(random_point.index)\n",
    "    # print(random_point)\n",
    "    # print(\"\")\n",
    "    # print(dataset.loc[0])\n",
    "    # print(dataset[0])\n",
    "    # print(random_point.loc(random_point.index[0]))\n",
    "    print(dataset)\n",
    "\n",
    "    # Get all points within distance\n",
    "    points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "        random_point, dataset, eps\n",
    "    )\n",
    "\n",
    "    # Check if count of points is higher than min_pts => is a core point\n",
    "    if len(points_within_eps_distance) < min_pts:\n",
    "        print(\"Test2\")\n",
    "\n",
    "        # Not a core point => mark as noise\n",
    "        random_point[\"cluster\"] = -2\n",
    "        dataset.loc[random_point.index[0], \"cluster\"] = -2\n",
    "    else:\n",
    "        print(\"Test\")\n",
    "        print(points_within_eps_distance)\n",
    "\n",
    "        # Iterate through the points in eps distance\n",
    "        for index, row in points_within_eps_distance.iterrows():\n",
    "            # Check whether the point is part of a cluster\n",
    "            # (Note that a point marked as noise is not part of a cluster, too)\n",
    "            if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                continue\n",
    "\n",
    "            # If it is not yet part of a cluster add it to the new_cluster\n",
    "            dataset.loc[index, \"cluster\"] = new_cluster\n",
    "\n",
    "            # Set the point to visited\n",
    "            dataset.loc[index, \"visited\"] = True\n",
    "\n",
    "            print(index)\n",
    "            print(\"\")\n",
    "            print(dataset.loc[index, \"cluster\"])\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 4, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "TODO\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
