{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 5. Clustering\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Clustering*.\n",
    "The series itself is based on lecture *8. Cluster Analysis*.\n",
    "\n",
    "There are two parts:\n",
    "\n",
    "- Part One: Implementing k-means and DBScan\n",
    "- Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "Again we would like to remind you that we have multiple exercise groups.\n",
    "Depending on how each group progresses, some parts of these exercises may not be discussed in its entirety.\n",
    "If questions arise, ask them in your study group or in our StudOn forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## Part One: Implementing k-means and DBScan\n",
    "\n",
    "In the first part of this exercise sheet we will take a closer look at two clustering methods known from the lecture: k-means and DBScan. You will be asked to implement both methods from scratch. In both cases you get the option to implement the algortithms totally on your self or with the help of a step-by-step task series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The datasets to cluster are the same for both methods. On the one hand, there is the `small_dataset`, which is used as an example for k-means in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the small_dataset\n",
    "small_dataset = pd.DataFrame(\n",
    "    [\n",
    "        [6, 5],\n",
    "        [8, 5],\n",
    "        [4, 3],\n",
    "        [5, 6],\n",
    "        [6, 2],\n",
    "        [6, 3],\n",
    "        [2, 2],\n",
    "        [3, 3],\n",
    "        [4, 4],\n",
    "        [5, 5],\n",
    "        [6, 6],\n",
    "        [7, 7],\n",
    "        [8, 7],\n",
    "        [8, 4],\n",
    "    ],\n",
    "    columns=[\"x\", \"y\"],\n",
    ")\n",
    "\n",
    "# Output the small_dataset in a scatterplot diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=small_dataset[\"x\"], y=small_dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "On the other hand, there is the `big_dataset`, a larger dataset that was generated using `sklearn` and for which a ground trouth (column `true_labels`) exists to validate the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the big_dataset\n",
    "big_data, big_data_true_labels = make_blobs(\n",
    "    n_samples=1000, centers=10, random_state=2306\n",
    ")\n",
    "big_dataset = pd.DataFrame(big_data, columns=[\"x\", \"y\"])\n",
    "big_dataset[\"true_labels\"] = big_data_true_labels\n",
    "\n",
    "# Print a scatterplot diagram of the big_dataset while showing the actual class affiliations with colors\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=big_dataset[\"x\"],\n",
    "    y=big_dataset[\"y\"],\n",
    "    hue=big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "If you decide to take a standalone approach to implementation, we recommend that you first use the `small_dataset`. A clustering run on the `big_dataset` takes its time and is thus not so well suited for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### K-means\n",
    "\n",
    "The first approach you are asked to implement is k-means. It is part of the so called partitioning methods and is the first approach we took a look at in the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "As announced, there are two options for you regarding the implementation of k-means. You may implement the method without extra help or you can choose option two: A guided step-by-step implementation of the k-means algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Implement K-means on Your Own\n",
    "\n",
    "Some of you may prefer to implement k-means on your own. In this case refer to the lecture for a comprehensive explanation of the method. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use your knowledge of k-means to implement a method `k_means` that can be used to cluster the two datasets `small_dataset` and `big_dataset` into `k` clusters using the euclidean distance to measure the distance between two points.\n",
    "If you are in need of more code cells than provided, feel free to add more.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample k_means sceleton\n",
    "# NOTE: You are allowed to use this sceleton but don't have to\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the small_dataset (just as in the lecture we use k=2)\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the big_dataset (as we know that there are 10 classes, we use k=10)\n",
    "clustered_big_dataset = k_means(big_dataset, 10)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 2: Implement K-means by Solving Small Tasks\n",
    "\n",
    "When someone tries to implement k-means step-by-step, the initial step is always to make an initial partition of the existing data into `k` non-empty partitions. This division can be random or according to an arbitrary scheme. However it is important that the result are exactly `k` partitions, that none of these partitions is empty and that each sample is represented in exactly one of the partitions. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `partition_dataset` that splits a `dataset` into `k` initial partitions. It doesn`t matter what kind of partitioning you decide on, as long as it complies with the rules mentioned. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # The method that was used in the lecture example is to sort the samples regarding to their y values\n",
    "    dataset_copy = dataset_copy.sort_values(by=[\"y\"]).reset_index(drop=True)\n",
    "\n",
    "    # Then to define an equal size for each cluster/partition\n",
    "    cluster_size = round(dataset_copy.shape[0] / k)\n",
    "\n",
    "    # And then to assign the samples to the cluster/partition\n",
    "    for i in range(0, dataset_copy.shape[0], cluster_size):\n",
    "        # Start of the slice\n",
    "        start = i\n",
    "\n",
    "        # End of the slice\n",
    "        end = min(i + cluster_size, dataset_copy.shape[0])\n",
    "\n",
    "        # Cluster id\n",
    "        cluster_id = i / cluster_size\n",
    "\n",
    "        # Assign the cluster value\n",
    "        dataset_copy.loc[start : end - 1, \"cluster\"] = cluster_id\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 2)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The first repetitive step in k-means is to calculate for the so-called centroids (mean points) for each partition/cluster. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement the function `compute_centroids` that computes the centroid for each of the `k` partitions. The return value should be a pandas DataFrame with the cluster identifier as an index and two columns `x` and `y` indicating the coordinates of the corresponding centroid.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 2)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # Compute the centroid of each partition\n",
    "    for i in range(0, k):\n",
    "        # Compute the mean of the x values within that single partition\n",
    "        x_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"x\"].mean()\n",
    "\n",
    "        # Compute the mean of the y values within that single partition\n",
    "        y_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"y\"].mean()\n",
    "\n",
    "        # Add the centroid of this single partition\n",
    "        centroids.loc[i, [\"x\", \"y\"]] = [x_mean, y_mean]\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 2)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "To reassign points to their nearest centroid, distance measure must be defined. Here, for example, the Euclidean distance comes in handy, which we have already implemented ourselves in an earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# \"Pythonic\" implementation of the euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return (abs(a - b) ** 2).sum() ** 0.5\n",
    "\n",
    "\n",
    "# Compute the euclidean distance for two random points a and b\n",
    "a = pd.Series([1, 9])\n",
    "b = pd.Series([9, 5])\n",
    "euclidean_distance(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Reassignment is also the next step within k-means. Samples are always reassigned to the cluster/partition whose centroid is closest to themselves.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function `reassign_samples` that reassigns samples to the cluster/partition whose centroid is closest to themselves. Return the dataset and an indictator to communicate whether at least tuple was reassigned within the function or not.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 2\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # Check for each sample whether it has to be reassigned\n",
    "    for i in range(0, dataset_copy.shape[0]):\n",
    "        # Get the value of the the dataset for easier access\n",
    "        sample = dataset_copy.loc[i, [\"x\", \"y\"]]\n",
    "\n",
    "        # Set the current cluster id and centroid values\n",
    "        current_cluster = dataset_copy.loc[i, \"cluster\"]\n",
    "        current_centroid = centroids.loc[current_cluster]\n",
    "        current_distance = euclidean_distance(sample, current_centroid)\n",
    "\n",
    "        # Iterate through the centroids and check whether the distance is lower than the current distance\n",
    "        # NOTE: We do not skip the current centroid, as this would complicate the code and isn't a big performance problem\n",
    "        for j in range(0, k):\n",
    "            # Compute the distance\n",
    "            distance = euclidean_distance(sample, centroids.loc[j])\n",
    "\n",
    "            # If the distance is lower than the current_distance we have to reassign\n",
    "            if distance < current_distance:\n",
    "\n",
    "                # Set the cluster\n",
    "                dataset_copy.loc[i, \"cluster\"] = j\n",
    "                current_cluster = j\n",
    "\n",
    "                # Set the current_centroid\n",
    "                current_centroid = centroids.loc[j]\n",
    "\n",
    "                # Set the current_distance\n",
    "                current_distance = distance\n",
    "\n",
    "                # Set the reassign_indicator\n",
    "                reassign_indicator = True\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 2\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In the iterative k-means algorithm it would now be checked whether samples were reassigned or not. If yes, we have to go back to calculating the centroids for this new assignment. If not, then the corresponding clusters have been found. \n",
    "\n",
    "This decision can of course be passed to a wrapper function `k_means` which summarizes the whole algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Merge the previously implemented function `k_means` to achieve a complete implementation of the algorithm.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Partition the dataset\n",
    "    dataset = partition_dataset(dataset, k)\n",
    "\n",
    "    # Set the reassign_indicator to True (as the intial partitioning was as reassingment in itself)\n",
    "    reassign_indicator = True\n",
    "\n",
    "    # As long as there are reassingment the following two steps are repeated\n",
    "    while reassign_indicator:\n",
    "        # Compute the centroids\n",
    "        centroids = compute_centroids(dataset, k)\n",
    "\n",
    "        # Reassign each sample to the cluster of the nearest centroid\n",
    "        reassign_indicator, dataset = reassign_samples(dataset, centroids, k)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Validation\n",
    "\n",
    "If we take a look at the `big_dataset` with its `true_labels`, we can identify some areas that might lead to problems with k-means due to overlapping of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Print a scatterplot diagram of the big_dataset while showing the actual class affiliations with colors\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=big_dataset[\"x\"],\n",
    "    y=big_dataset[\"y\"],\n",
    "    hue=big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "If you now run your k-means implementation on this `big_dataset`, you will probably also see some areas where the classes were not well recognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the big_dataset\n",
    "clustered_big_dataset = k_means(big_dataset, 10)\n",
    "\n",
    "# Output the clustered dataset including information on the true classes\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    style=clustered_big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "It is important to mention that these problems may well be caused by k-means itself and not necessarily by faulty programming on your part. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Compare the actual class partitioning and the partitioning determined by your k-means implementation. Describe the most prominent misidentifications.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Especially due to the fact that the initial assignment can be done arbitrarily, the results of the student implementation and this sample solution are probably very different. For this reason, we focus in this description explicitly only on the differences within the sample solution:\n",
    "\n",
    "- Our k-means implementation created a mega cluster of original two classes around the point -10, 5. These are not correctly recognized by our k-means implementation.\n",
    "- Another mega cluster also exists around point 7, 7.5: again, two classes were detected as only one cluster.\n",
    "- To compensate for the two incorrectly recognized classes, our k-means implementation created four clusters around the point 0, 3. Originally, this was a region of two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Especially with strongly mixed classes k-means has problems. However, this is not unusual for a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: scikit-learn\n",
    "\n",
    "Even with the clustering algorithms from this task sheet, it is of course not normally necessary to create your own implementations for the procedures. In the case of k-means, for example, there is a good implementation in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's implementation of k-means to find ten clusters in the `big_dataset`. Print the result in a diagram.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns k-means clustering on the big_dataset\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns k-means clustering on the big_dataset\n",
    "kmeans = KMeans(n_clusters=10).fit(big_dataset[[\"x\", \"y\"]])\n",
    "\n",
    "# Save the labels to a copy of the big_dataset to generate the equivalent of our clustered_big_dataset\n",
    "clustered_big_dataset_2 = big_dataset.copy()\n",
    "clustered_big_dataset_2[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# Print the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset_2[\"x\"],\n",
    "    y=clustered_big_dataset_2[\"y\"],\n",
    "    hue=clustered_big_dataset_2[\"cluster\"],\n",
    "    style=clustered_big_dataset_2[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "This shows how useful it can be to optimize the selection of the start value. By default, Scikit-learn uses a method called k-means++, which leads to a significantly better recognition of the true classes contained in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### DBSCAN\n",
    "\n",
    "In addition to the partitioning methods, density-based methods were also presented in the lecture. As an example of these methods, you will asked to implement DBSCAN in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "Also during this implementation you have two options: On the one hand, you may implement DBSCAN completely on your own, on the other hand, you may use the task series divided into smaller tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Implement DBSCAN on Your Own\n",
    "\n",
    "If you decided to implement DBSCAN on your own refer to the lecture for a comprehensive explanation of the method. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement a method `dbscan` that can be used to cluster the two datasets `small_dataset` and `big_dataset` into multiple clusters. You shall use the euclidean distance to measure the distance between two points during the clustering.\n",
    "If you are in need of more code cells than provided, feel free to add more.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample dbscan sceleton\n",
    "# NOTE: You are allowed to use this sceleton but don't have to\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation\n",
    "    # Special codings for ...\n",
    "    # ... points that are not set yet: -1\n",
    "    # ... points that are noise: -2\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Create a new empty column to save the visited status\n",
    "    dataset_copy[\"visited\"] = False\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the small_dataset\n",
    "# (the parameters eps=1 and min_pts=2 should result in five different clusters and\n",
    "# one \"noisy\" point for this dataset)\n",
    "clustered_small_dataset = dbscan(small_dataset, 1, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the big_dataset\n",
    "# (the parameters eps=1 and min_pts=5 should result in five different clusters and\n",
    "# multiple \"noisy\" points for this dataset)\n",
    "clustered_big_dataset = dbscan(big_dataset, 1, 5)\n",
    "\n",
    "# Output the clustered dataset including information on the true classes\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    style=clustered_big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 2: Implement DBSCAN by Solving Small Tasks\n",
    "\n",
    "For DBSCAN, you need not only the cluster membership as meta information, but also the status \"visited\". Before we start with the step-by-step implementation of DBSCAN, it is useful to write a small function for preparing the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Add columns to the dataset to save the status of the dataset\n",
    "def prepare_dataset(dataset):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation\n",
    "    # Special codings for ...\n",
    "    # ... points that are not set yet: -1\n",
    "    # ... points that are noise: -2\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Create a new empty column to save the visited status\n",
    "    dataset_copy[\"visited\"] = False\n",
    "\n",
    "    # Return the dataset_copy\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Prepare the small dataset\n",
    "prepared_small_dataset = prepare_dataset(small_dataset)\n",
    "prepared_small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Besides this preparatory helper function, there are two things that it would make sense to outsource to separate functions before the actual DBSCAN implementation. \n",
    "\n",
    "First, a function is needed in DBSCAN to randomly select a single unvisited point from a prepared data set. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `pick_random_unvisited_point` that randomly selects an unvisited point out of the `dataset` and returns it.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick a random point that is unvisited\n",
    "def pick_random_unvisited_point(dataset):\n",
    "    # ...\n",
    "    return None\n",
    "\n",
    "\n",
    "# Pick a random point\n",
    "random_point = pick_random_unvisited_point(prepared_small_dataset)\n",
    "random_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick a random point that is unvisited\n",
    "def pick_random_unvisited_point(dataset):\n",
    "    # Select all points that are unvisited\n",
    "    unvisited_points = dataset[dataset[\"visited\"] == False]\n",
    "\n",
    "    # If there are no unvisited points return None\n",
    "    if len(unvisited_points) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        # Select one random point and return it\n",
    "        return unvisited_points.sample().iloc[0]\n",
    "\n",
    "\n",
    "# Pick a random point\n",
    "random_point = pick_random_unvisited_point(prepared_small_dataset)\n",
    "random_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "A second helper function that helps implementing DBSCAN is a function that returns all point within eps distance of a selected point.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `get_all_points_within_eps_distance` that returns all points within distance of `eps`to the passed `point`. Use the euclidean distance function introduced during the k-means part of this exercise to determine the distance between two points. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Get all points within a distance of eps next to a specific point\n",
    "def get_all_points_within_eps_distance(point, dataset, eps):\n",
    "    # ...\n",
    "    return None\n",
    "\n",
    "\n",
    "# Get all points within distance of 1 regarding to the point (6,5)\n",
    "points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "    pd.Series(data=[6, 5, -1, False], index=[\"x\", \"y\", \"cluster\", \"visited\"]),\n",
    "    prepared_small_dataset,\n",
    "    1,\n",
    ")\n",
    "points_within_eps_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Get all points within a distance of eps next to a specific point\n",
    "def get_all_points_within_eps_distance(point, dataset, eps):\n",
    "    # Select all unvisited points within eps distance\n",
    "    return dataset[\n",
    "        dataset.apply(\n",
    "            lambda a: euclidean_distance([a[\"x\"], a[\"y\"]], point[[\"x\", \"y\"]].values)\n",
    "            <= eps,\n",
    "            axis=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# Get all points within distance of 1 regarding to the point (6,5)\n",
    "points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "    pd.Series(data=[6, 5, -1, False], index=[\"x\", \"y\", \"cluster\", \"visited\"]),\n",
    "    prepared_small_dataset,\n",
    "    1,\n",
    ")\n",
    "points_within_eps_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The pseudocode from the lecture on DBSCAN is quite general. Thus, the substep `If p′ is core point, add all objects in its ϵ-neighborhood to N` is ultimately something that can be implemented both by merging multiple sets of points, and by recursion. \n",
    "Since the recursive variant of DBSCAN is easier to implement, we focus on this variant in this step-by-step implementation. \n",
    "\n",
    "Finally, it makes sense to outsource the entire step `For each p′ in N that does not yet belong to a cluster` to a seperate recursive function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function sceleton of the function `expand_cluster` below. Remember that you can use the previously defined helper functions. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# This function is used to expand a specific cluster by one point\n",
    "# If the point is a core point (at least min_pts in eps distance) by itself\n",
    "# expand_cluster is called for each neighbor.\n",
    "def expand_cluster(dataset, eps, min_pts, point, cluster_id):\n",
    "    # ...\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# This function is used to expand a specific cluster by one point\n",
    "# If the point is a core point (at least min_pts in eps distance) by itself\n",
    "# expand_cluster is called for each neighbor.\n",
    "def expand_cluster(dataset, eps, min_pts, point, cluster_id):\n",
    "    # Add the point to the cluster\n",
    "    dataset.loc[point.name, \"cluster\"] = cluster_id\n",
    "\n",
    "    # If point was not visited, we have to visit it now\n",
    "    if dataset.loc[point.name, \"visited\"] == False:\n",
    "        # Mark the point as visited\n",
    "        dataset.loc[point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        # => We have to go deeper into the recursion\n",
    "        if len(points_within_eps_distance.index) >= min_pts:\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course it is useful to have a test case to test your implementation against. However this test case is somewhat more difficult to understand, as the function depends on input of an undefined function. Therefore lets desribe the test scenario first:\n",
    "\n",
    "*Lets say that the point with id `7` (Coordinates are `(3, 3)`) is selected as random unvisited point out of the prepared_small_dataset by the main dbscan function. As in this example eps is `1` in this case and min_pts is `2` the selected random unvisited point is a core point, as there is one other point (Id `2` and coordinates `(4,3)`) within eps distance. Therefore a new cluster with id `0` is created, the point with id `7` is added and `expand_cluster` gets called for all neighboring points that are not part of a cluster yet. In our example call we take a look call `expand_cluster` for the point with id `2`*\n",
    "\n",
    "If your function works fine, it should add the point with id `2` into the cluster and should check whether it is a core point itself. As there is one still univisited point to descend to (Id `8` with coordingates `(4,4)`) recursion is started. In the end there should be three visited points withing cluster `0`(Ids `2`, `7` and `8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "prepared_small_dataset = prepare_dataset(small_dataset)\n",
    "\n",
    "# Mark the point with id 7 as visited and add it to the cluster with id 0\n",
    "prepared_small_dataset.loc[7, \"visited\"] = True\n",
    "prepared_small_dataset.loc[7, \"cluster\"] = 0\n",
    "\n",
    "# Select the point with id 2\n",
    "point_with_id_2 = prepared_small_dataset.iloc[2]\n",
    "\n",
    "# Call expand_cluster\n",
    "expand_cluster(prepared_small_dataset, 1, 2, point_with_id_2, 0)\n",
    "\n",
    "# Take a look at the dataset (should now contain three points within cluster 0)\n",
    "prepared_small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "With the help of the recursive function `expand_cluster` it is now not difficult to implement the function `dbscan`, which in principle takes over the remaining steps of the pseudocode and uses `expand_cluster` whenever neighboring items have to be added to the cluster.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the `dbscan`. Again it es recommend to use the prviously defined functions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 1, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Prepare the dataset\n",
    "    dataset = prepare_dataset(dataset)\n",
    "\n",
    "    # While there are unvisited points pick a random one\n",
    "    while len(dataset[dataset[\"visited\"] == False]) > 0:\n",
    "        # Select a random unvisited point\n",
    "        random_point = pick_random_unvisited_point(dataset)\n",
    "\n",
    "        # Mark the random point as visited\n",
    "        dataset.loc[random_point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            random_point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        if len(points_within_eps_distance.index) < min_pts:\n",
    "            # Not a core point => mark as noise\n",
    "            dataset.loc[random_point.name, \"cluster\"] = -2\n",
    "        else:\n",
    "            # Get the last used cluster id\n",
    "            last_cluster_id = dataset[\"cluster\"].max()\n",
    "\n",
    "            # Increment the id to get an new id for the new cluster\n",
    "            new_cluster_id = last_cluster_id + 1\n",
    "\n",
    "            # Add the random point to the cluster\n",
    "            dataset.loc[random_point.name, \"visited\"] = True\n",
    "\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, new_cluster_id)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 1, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Validation\n",
    "\n",
    "Again we can take a look at the `big_dataset` with its `true_labels`. We will again see strengths and disadvantages of a clustering methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the big_dataset\n",
    "clustered_big_dataset = dbscan(big_dataset, 1, 5)\n",
    "\n",
    "# Output the clustered dataset including information on the true classes\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    style=clustered_big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Describe the most prominent problems you can identificate for the DBSCAN algorithm in this example.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "In this case, there are some things that stand out in particular. However it has to be noted, that some of these might be less significant with other `eps` and `min_pts` configurations while others might be more significant:\n",
    "    \n",
    "- Our implementation of DBSCAN leads to less classes than there actually are. (However it might also be an advantage that someone does not have to know the actual count of classes in advance.)\n",
    "- There are some \"noisy\" points that are not part of any cluster.\n",
    "- DBSCAN can not differate overlapping classes at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: scikit-learn\n",
    "\n",
    "Just as for k-means, scikit-learn also offers an extensive implementation for DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's implementation of DBSCAN to find clusters in the `big_dataset`. Use the same parameters we used in the above in the own implementation. Print the result in a diagram.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns DBSCAN clustering on the big_dataset\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns DBSCAN clustering on the big_dataset\n",
    "dbscan = DBSCAN(eps=1, min_samples=5).fit(big_dataset[[\"x\", \"y\"]])\n",
    "\n",
    "# Save the labels to a copy of the big_dataset to generate the equivalent of our clustered_big_dataset\n",
    "clustered_big_dataset_3 = big_dataset.copy()\n",
    "clustered_big_dataset_3[\"cluster\"] = dbscan.labels_\n",
    "\n",
    "# Print the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset_3[\"x\"],\n",
    "    y=clustered_big_dataset_3[\"y\"],\n",
    "    hue=clustered_big_dataset_3[\"cluster\"],\n",
    "    style=clustered_big_dataset_3[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In this case, the results of our function and that of scikit-learn are identical. This shows that DBSCAN is more deterministic than k-means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Clustering in the AdventureWorks Database\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "TODO\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
