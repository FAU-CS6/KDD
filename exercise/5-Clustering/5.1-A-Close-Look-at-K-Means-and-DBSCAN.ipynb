{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# 5. Clustering\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Clustering*. <br/>\n",
    "The series itself is based on lecture *8. Cluster Analysis*.\n",
    "\n",
    "This exercise series is divided into two parts. There will be one exercise session per part (= one part per week):\n",
    "\n",
    "- **5.1.** A Close Look at K-Means and DBSCAN (*this notebook*)\n",
    "    - **5.1.1.** [K-Means](#5.1.1.-K-Means)\n",
    "        - **5.1.1.1.** [Application by Hand](#5.1.1.1.-Application-by-Hand)\n",
    "        - **5.1.1.2.** [Implementation](#5.1.1.2.-Implementation)\n",
    "- **5.2.** [Clustering in Python](./5.2-Clustering-in-Python.ipynb) (*next weeks notebook*)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "Work on the respective part yourself <u>BEFORE</u> each exercise session. The exercise session is <u>NOT</u> intended to take a first look at the exercise sheet, but to solve problems students had while preparing the exercise sheet beforehand.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "## 5.1. A Close Look at K-Means and DBSCAN\n",
    "\n",
    "In this part we will take a closer look at two clustering methods known from the lecture: K-means and DBSCAN. \n",
    "\n",
    "In the following, you will first apply both methods step by step by hand to a data set and then write your own implementation for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The data set to cluster is the same for both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = pd.DataFrame(\n",
    "    [\n",
    "        [1, 1],\n",
    "        [1, 2],\n",
    "        [1, 4],\n",
    "        [2, 1],\n",
    "        [2, 3],\n",
    "        [3, 2],\n",
    "        [3, 4],\n",
    "        [4, 1],\n",
    "        [4, 3],\n",
    "        [4, 4],\n",
    "    ],\n",
    "    columns=[\"x\", \"y\"],\n",
    ")\n",
    "\n",
    "# Output the dataset in a scatterplot diagram\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(x=dataset[\"x\"], y=dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### 5.1.1. K-Means\n",
    "\n",
    "The first clustering method we are taking a close look at is K-means. It is part of the partitioning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### 5.1.1.1. Application by Hand\n",
    "\n",
    "In order to familiarise yourself with K-means, you should first apply K-means by hand.\n",
    "\n",
    "Given is the small data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "| x | y |\n",
    "|:-:|:-:|\n",
    "| 1 | 1 |\n",
    "| 1 | 2 |\n",
    "| 1 | 4 |\n",
    "| 2 | 1 |\n",
    "| 2 | 3 |\n",
    "| 3 | 2 |\n",
    "| 3 | 4 |\n",
    "| 4 | 1 |\n",
    "| 4 | 3 |\n",
    "| 4 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "You are now to apply K-Means to this data set by hand. We will use the Euclidean distance and a k of three in both cases:\n",
    "\n",
    "- **Option 1:** [Apply K-Means on your own](#Option-1:-Apply-K-Means-on-your-own)\n",
    "- **Option 2:** [Apply K-Means step by step](#Option-2:-Apply-K-Means-step-by-step)\n",
    "\n",
    "It is recommended that you first try it on your own and only resort to the guided step-by-step variant if you have problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Apply K-Means on your own\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1.1:** \n",
    "    \n",
    "Use K-Means to cluster the data points into three clusters. Write down all intermediate steps.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 2: Apply K-Means step by step\n",
    "\n",
    "The first step of K-Means is to arbitrarily distribute all data points into k partitions. This can be done in many different ways (e.g. randomly or by dividing the points into partitions of equal size).\n",
    "\n",
    "In this case we distribute the points into (approximately) equal-sized partitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | y | Partition |\n",
    "|:-:|:-:|:---------:|\n",
    "| 1 | 1 |     1     |\n",
    "| 1 | 2 |     1     |\n",
    "| 1 | 4 |     1     |\n",
    "| 2 | 1 |     1     |\n",
    "| 2 | 3 |     2     |\n",
    "| 3 | 2 |     2     |\n",
    "| 3 | 4 |     2     |\n",
    "| 4 | 1 |     3     |\n",
    "| 4 | 3 |     3     |\n",
    "| 4 | 4 |     3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the centroids of the partitions.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1.2.1:** \n",
    "    \n",
    "Determine the coordinates of the centroids of the partitions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Centroid of Partition 1:**\n",
    "\n",
    "- $x = \\frac{1 + 1 + 1 + 2}{4} = 1,25$\n",
    "- $y = \\frac{1 + 2 + 4 + 1}{4} = 2$\n",
    "\n",
    "**Centroid of Partition 2:**\n",
    "\n",
    "- $x = \\frac{2 + 2 + 3}{3} = \\frac{7}{3} \\approx 2,333$\n",
    "- $y = \\frac{3 + 2 + 4}{3} = 3$ \n",
    "\n",
    "**Centroid of Partition 3:**\n",
    "\n",
    "- $x = \\frac{4 + 4 + 4}{3} = 4$\n",
    "- $y = \\frac{1 + 3 + 4}{3} = \\frac{8}{3} \\approx 2,667$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the nearest centroid is calculated for each of the original data points. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1.2.2:** \n",
    "    \n",
    "For each data point, determine which centroid has the smallest Euclidean distance to that point.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Point at (1,1):**\n",
    "- $Distance_{(1,1)\\leftrightarrow(1.25,2.0)} = \\sqrt{(1-1.25)^2+(1-2.0)^2} \\approx 1.03$\n",
    "- $Distance_{(1,1)\\leftrightarrow(2.33,3.0)} = \\sqrt{(1-2.33)^2+(1-3.0)^2} \\approx 2.4$\n",
    "- $Distance_{(1,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(1-2.67)^2} \\approx 3.43$\n",
    " \n",
    "Nearest centroid: $(1.25,2.0)$ (Partition 1)\n",
    " \n",
    "**Data Point at (1,2):**\n",
    "- $Distance_{(1,2)\\leftrightarrow(1.25,2.0)} = \\sqrt{(1-1.25)^2+(2-2.0)^2} \\approx 0.25$\n",
    "- $Distance_{(1,2)\\leftrightarrow(2.33,3.0)} = \\sqrt{(1-2.33)^2+(2-3.0)^2} \\approx 1.67$\n",
    "- $Distance_{(1,2)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(2-2.67)^2} \\approx 3.07$\n",
    " \n",
    "Nearest centroid: $(1.25,2.0)$ (Partition 1)\n",
    " \n",
    "**Data Point at (1,4):**\n",
    "- $Distance_{(1,4)\\leftrightarrow(1.25,2.0)} = \\sqrt{(1-1.25)^2+(4-2.0)^2} \\approx 2.02$\n",
    "- $Distance_{(1,4)\\leftrightarrow(2.33,3.0)} = \\sqrt{(1-2.33)^2+(4-3.0)^2} \\approx 1.67$\n",
    "- $Distance_{(1,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(4-2.67)^2} \\approx 3.28$\n",
    " \n",
    "Nearest centroid: $(2.33,3.0)$ (Partition 2)\n",
    " \n",
    "**Data Point at (2,1):**\n",
    "- $Distance_{(2,1)\\leftrightarrow(1.25,2.0)} = \\sqrt{(2-1.25)^2+(1-2.0)^2} \\approx 1.25$\n",
    "- $Distance_{(2,1)\\leftrightarrow(2.33,3.0)} = \\sqrt{(2-2.33)^2+(1-3.0)^2} \\approx 2.03$\n",
    "- $Distance_{(2,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(2-4.0)^2+(1-2.67)^2} \\approx 2.6$\n",
    " \n",
    "Nearest centroid: $(1.25,2.0)$ (Partition 1)\n",
    " \n",
    "**Data Point at (2,3):**\n",
    "- $Distance_{(2,3)\\leftrightarrow(1.25,2.0)} = \\sqrt{(2-1.25)^2+(3-2.0)^2} \\approx 1.25$\n",
    "- $Distance_{(2,3)\\leftrightarrow(2.33,3.0)} = \\sqrt{(2-2.33)^2+(3-3.0)^2} \\approx 0.33$\n",
    "- $Distance_{(2,3)\\leftrightarrow(4.0,2.67)} = \\sqrt{(2-4.0)^2+(3-2.67)^2} \\approx 2.03$\n",
    " \n",
    "Nearest centroid: $(2.33,3.0)$ (Partition 2)\n",
    " \n",
    "**Data Point at (3,2):**\n",
    "- $Distance_{(3,2)\\leftrightarrow(1.25,2.0)} = \\sqrt{(3-1.25)^2+(2-2.0)^2} \\approx 1.75$\n",
    "- $Distance_{(3,2)\\leftrightarrow(2.33,3.0)} = \\sqrt{(3-2.33)^2+(2-3.0)^2} \\approx 1.2$\n",
    "- $Distance_{(3,2)\\leftrightarrow(4.0,2.67)} = \\sqrt{(3-4.0)^2+(2-2.67)^2} \\approx 1.2$\n",
    " \n",
    "Nearest centroid: $(2.33,3.0)$ (Partition 2)\n",
    " \n",
    "**Data Point at (3,4):**\n",
    "- $Distance_{(3,4)\\leftrightarrow(1.25,2.0)} = \\sqrt{(3-1.25)^2+(4-2.0)^2} \\approx 2.66$\n",
    "- $Distance_{(3,4)\\leftrightarrow(2.33,3.0)} = \\sqrt{(3-2.33)^2+(4-3.0)^2} \\approx 1.2$\n",
    "- $Distance_{(3,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(3-4.0)^2+(4-2.67)^2} \\approx 1.67$\n",
    " \n",
    "Nearest centroid: $(2.33,3.0)$ (Partition 2)\n",
    " \n",
    "**Data Point at (4,1):**\n",
    "- $Distance_{(4,1)\\leftrightarrow(1.25,2.0)} = \\sqrt{(4-1.25)^2+(1-2.0)^2} \\approx 2.93$\n",
    "- $Distance_{(4,1)\\leftrightarrow(2.33,3.0)} = \\sqrt{(4-2.33)^2+(1-3.0)^2} \\approx 2.6$\n",
    "- $Distance_{(4,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(1-2.67)^2} \\approx 1.67$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    " \n",
    "**Data Point at (4,3):**\n",
    "- $Distance_{(4,3)\\leftrightarrow(1.25,2.0)} = \\sqrt{(4-1.25)^2+(3-2.0)^2} \\approx 2.93$\n",
    "- $Distance_{(4,3)\\leftrightarrow(2.33,3.0)} = \\sqrt{(4-2.33)^2+(3-3.0)^2} \\approx 1.67$\n",
    "- $Distance_{(4,3)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(3-2.67)^2} \\approx 0.33$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    " \n",
    "**Data Point at (4,4):**\n",
    "- $Distance_{(4,4)\\leftrightarrow(1.25,2.0)} = \\sqrt{(4-1.25)^2+(4-2.0)^2} \\approx 3.4$\n",
    "- $Distance_{(4,4)\\leftrightarrow(2.33,3.0)} = \\sqrt{(4-2.33)^2+(4-3.0)^2} \\approx 1.94$\n",
    "- $Distance_{(4,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(4-2.67)^2} \\approx 1.33$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, the data points are assigned to the partition to which the respective centroid belongs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1.2.3:** \n",
    "    \n",
    "Assign the points to the respective new partition.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | y | Old Partition | New Partition |\n",
    "|:-:|:-:|:-------------:|:-------------:|\n",
    "| 1 | 1 |       1       |       ?       |\n",
    "| 1 | 2 |       1       |       ?       |\n",
    "| 1 | 4 |       1       |       ?       |\n",
    "| 2 | 1 |       1       |       ?       |\n",
    "| 2 | 3 |       2       |       ?       |\n",
    "| 3 | 2 |       2       |       ?       |\n",
    "| 3 | 4 |       2       |       ?       |\n",
    "| 4 | 1 |       3       |       ?       |\n",
    "| 4 | 3 |       3       |       ?       |\n",
    "| 4 | 4 |       3       |       ?       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | y | Old Partition | New Partition |\n",
    "|:-:|:-:|:-------------:|:-------------:|\n",
    "| 1 | 1 |       1       |       1       |\n",
    "| 1 | 2 |       1       |       1       |\n",
    "| 1 | 4 |       1       |       2       |\n",
    "| 2 | 1 |       1       |       1       |\n",
    "| 2 | 3 |       2       |       2       |\n",
    "| 3 | 2 |       2       |       2       |\n",
    "| 3 | 4 |       2       |       2       |\n",
    "| 4 | 1 |       3       |       3       |\n",
    "| 4 | 3 |       3       |       3       |\n",
    "| 4 | 4 |       3       |       3       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a point has been assigned into a new partition, a new run begins in which centroids are determined, the distances of the points to the new centroids are measured and a reallocation of points takes place. This takes place until there are no more partition changes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1.2.4:**\n",
    "    \n",
    "Continue K-means until no points are reassigned.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Step: Compute the new centroids**\n",
    "\n",
    "**Centroid of Partition 1:**\n",
    "\n",
    "- $x = \\frac{1 + 1 + 2}{3} = 4/3 \\approx 1,333$\n",
    "- $y = \\frac{1 + 2 + 1}{3} = 4/3 \\approx 1,333$\n",
    "\n",
    "**Centroid of Partition 2:**\n",
    "\n",
    "- $x = \\frac{1 + 2 + 2 + 3}{4} = 2$\n",
    "- $y = \\frac{4 + 3 + 2 + 4}{4} = 3,25$ \n",
    "\n",
    "**Centroid of Partition 3:**\n",
    "\n",
    "- $x = \\frac{4 + 4 + 4}{3} = 4$\n",
    "- $y = \\frac{1 + 3 + 4}{3} = \\frac{8}{3} \\approx 2,667$ \n",
    "\n",
    "\n",
    "\n",
    "**5. Step: Compute the new distances**\n",
    "\n",
    "**Data Point at (1,1):**\n",
    "- $Distance_{(1,1)\\leftrightarrow(1.33,1.33)} = \\sqrt{(1-1.33)^2+(1-1.33)^2} \\approx 0.47$\n",
    "- $Distance_{(1,1)\\leftrightarrow(2.0,3.25)} = \\sqrt{(1-2.0)^2+(1-3.25)^2} \\approx 2.46$\n",
    "- $Distance_{(1,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(1-2.67)^2} \\approx 3.43$\n",
    " \n",
    "Nearest centroid: $(1.33,1.33)$ (Partition 1)\n",
    " \n",
    "**Data Point at (1,2):**\n",
    "- $Distance_{(1,2)\\leftrightarrow(1.33,1.33)} = \\sqrt{(1-1.33)^2+(2-1.33)^2} \\approx 0.75$\n",
    "- $Distance_{(1,2)\\leftrightarrow(2.0,3.25)} = \\sqrt{(1-2.0)^2+(2-3.25)^2} \\approx 1.6$\n",
    "- $Distance_{(1,2)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(2-2.67)^2} \\approx 3.07$\n",
    " \n",
    "Nearest centroid: $(1.33,1.33)$ (Partition 1)\n",
    " \n",
    "**Data Point at (1,4):**\n",
    "- $Distance_{(1,4)\\leftrightarrow(1.33,1.33)} = \\sqrt{(1-1.33)^2+(4-1.33)^2} \\approx 2.69$\n",
    "- $Distance_{(1,4)\\leftrightarrow(2.0,3.25)} = \\sqrt{(1-2.0)^2+(4-3.25)^2} \\approx 1.25$\n",
    "- $Distance_{(1,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(1-4.0)^2+(4-2.67)^2} \\approx 3.28$\n",
    " \n",
    "Nearest centroid: $(2.0,3.25)$ (Partition 2)\n",
    " \n",
    "**Data Point at (2,1):**\n",
    "- $Distance_{(2,1)\\leftrightarrow(1.33,1.33)} = \\sqrt{(2-1.33)^2+(1-1.33)^2} \\approx 0.75$\n",
    "- $Distance_{(2,1)\\leftrightarrow(2.0,3.25)} = \\sqrt{(2-2.0)^2+(1-3.25)^2} \\approx 2.25$\n",
    "- $Distance_{(2,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(2-4.0)^2+(1-2.67)^2} \\approx 2.6$\n",
    " \n",
    "Nearest centroid: $(1.33,1.33)$ (Partition 1)\n",
    " \n",
    "**Data Point at (2,3):**\n",
    "- $Distance_{(2,3)\\leftrightarrow(1.33,1.33)} = \\sqrt{(2-1.33)^2+(3-1.33)^2} \\approx 1.8$\n",
    "- $Distance_{(2,3)\\leftrightarrow(2.0,3.25)} = \\sqrt{(2-2.0)^2+(3-3.25)^2} \\approx 0.25$\n",
    "- $Distance_{(2,3)\\leftrightarrow(4.0,2.67)} = \\sqrt{(2-4.0)^2+(3-2.67)^2} \\approx 2.03$\n",
    " \n",
    "Nearest centroid: $(2.0,3.25)$ (Partition 2)\n",
    " \n",
    "**Data Point at (3,2):**\n",
    "- $Distance_{(3,2)\\leftrightarrow(1.33,1.33)} = \\sqrt{(3-1.33)^2+(2-1.33)^2} \\approx 1.8$\n",
    "- $Distance_{(3,2)\\leftrightarrow(2.0,3.25)} = \\sqrt{(3-2.0)^2+(2-3.25)^2} \\approx 1.6$\n",
    "- $Distance_{(3,2)\\leftrightarrow(4.0,2.67)} = \\sqrt{(3-4.0)^2+(2-2.67)^2} \\approx 1.2$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    " \n",
    "**Data Point at (3,4):**\n",
    "- $Distance_{(3,4)\\leftrightarrow(1.33,1.33)} = \\sqrt{(3-1.33)^2+(4-1.33)^2} \\approx 3.14$\n",
    "- $Distance_{(3,4)\\leftrightarrow(2.0,3.25)} = \\sqrt{(3-2.0)^2+(4-3.25)^2} \\approx 1.25$\n",
    "- $Distance_{(3,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(3-4.0)^2+(4-2.67)^2} \\approx 1.67$\n",
    " \n",
    "Nearest centroid: $(2.0,3.25)$ (Partition 2)\n",
    " \n",
    "**Data Point at (4,1):**\n",
    "- $Distance_{(4,1)\\leftrightarrow(1.33,1.33)} = \\sqrt{(4-1.33)^2+(1-1.33)^2} \\approx 2.69$\n",
    "- $Distance_{(4,1)\\leftrightarrow(2.0,3.25)} = \\sqrt{(4-2.0)^2+(1-3.25)^2} \\approx 3.01$\n",
    "- $Distance_{(4,1)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(1-2.67)^2} \\approx 1.67$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    " \n",
    "**Data Point at (4,3):**\n",
    "- $Distance_{(4,3)\\leftrightarrow(1.33,1.33)} = \\sqrt{(4-1.33)^2+(3-1.33)^2} \\approx 3.14$\n",
    "- $Distance_{(4,3)\\leftrightarrow(2.0,3.25)} = \\sqrt{(4-2.0)^2+(3-3.25)^2} \\approx 2.02$\n",
    "- $Distance_{(4,3)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(3-2.67)^2} \\approx 0.33$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    " \n",
    "**Data Point at (4,4):**\n",
    "- $Distance_{(4,4)\\leftrightarrow(1.33,1.33)} = \\sqrt{(4-1.33)^2+(4-1.33)^2} \\approx 3.77$\n",
    "- $Distance_{(4,4)\\leftrightarrow(2.0,3.25)} = \\sqrt{(4-2.0)^2+(4-3.25)^2} \\approx 2.14$\n",
    "- $Distance_{(4,4)\\leftrightarrow(4.0,2.67)} = \\sqrt{(4-4.0)^2+(4-2.67)^2} \\approx 1.33$\n",
    " \n",
    "Nearest centroid: $(4.0,2.67)$ (Partition 3)\n",
    "\n",
    "\n",
    "**6. Step: Reassign the Data Points**\n",
    "\n",
    "| x | y | Old Partition | New Partition |\n",
    "|:-:|:-:|:-------------:|:-------------:|\n",
    "| 1 | 1 |       1       |       1       |\n",
    "| 1 | 2 |       1       |       1       |\n",
    "| 1 | 4 |       2       |       2       |\n",
    "| 2 | 1 |       1       |       1       |\n",
    "| 2 | 3 |       2       |       2       |\n",
    "| 3 | 2 |       2       |       3       |\n",
    "| 3 | 4 |       2       |       2       |\n",
    "| 4 | 1 |       3       |       3       |\n",
    "| 4 | 3 |       3       |       3       |\n",
    "| 4 | 4 |       3       |       3       |\n",
    "\n",
    "\n",
    "**7. Step: Compute the new centroids**\n",
    "\n",
    "**Centroid of Partition 1:**\n",
    "\n",
    "- $x = \\frac{1 + 1 + 2}{3} = 4/3 \\approx 1,333$\n",
    "- $y = \\frac{1 + 2 + 1}{3} = 4/3 \\approx 1,333$\n",
    "\n",
    "**Centroid of Partition 2:**\n",
    "\n",
    "- $x = \\frac{1 + 2 + 2 + 3}{4} = 2$\n",
    "- $y = \\frac{4 + 3 + 2 + 4}{4} = 3,25$ \n",
    "\n",
    "**Centroid of Partition 3:**\n",
    "\n",
    "- $x = \\frac{4 + 4 + 4}{3} = 4$\n",
    "- $y = \\frac{1 + 3 + 4}{3} = \\frac{8}{3} \\approx 2,667$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(\n",
    "    [[4 / 3, 4 / 3], [2, 3.25], [4, 8 / 3]],\n",
    "    columns=[\"x\", \"y\"],\n",
    ")\n",
    "\n",
    "euclidean_distance(dataset.iloc[0], centroids.iloc[0])\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    best_dist = 5000\n",
    "    best_cent = 0\n",
    "    print(\"**Data Point at (\" + str(row[\"x\"]) + \",\" + str(row[\"y\"]) + \"):**\")\n",
    "    for index_cent, row_cent in centroids.iterrows():\n",
    "        distance = euclidean_distance(row, row_cent)\n",
    "        if distance < best_dist:\n",
    "            best_cent = index_cent\n",
    "            best_dist = distance\n",
    "        print(\n",
    "            \"- $Distance_{(\"\n",
    "            + str(row[\"x\"])\n",
    "            + \",\"\n",
    "            + str(row[\"y\"])\n",
    "            + \")\\leftrightarrow(\"\n",
    "            + str(round(row_cent[\"x\"], 2))\n",
    "            + \",\"\n",
    "            + str(round(row_cent[\"y\"], 2))\n",
    "            + \")} = \\sqrt{(\"\n",
    "            + str(row[\"x\"])\n",
    "            + \"-\"\n",
    "            + str(round(row_cent[\"x\"], 2))\n",
    "            + \")^2+(\"\n",
    "            + str(row[\"y\"])\n",
    "            + \"-\"\n",
    "            + str(round(row_cent[\"y\"], 2))\n",
    "            + \")^2} \\\\approx \"\n",
    "            + str(round(distance, 2))\n",
    "            + \"$\"\n",
    "        )\n",
    "    print(\" \")\n",
    "    print(\n",
    "        \"Nearest centroid: $(\"\n",
    "        + str(round(centroids.iloc[best_cent][\"x\"], 2))\n",
    "        + \",\"\n",
    "        + str(round(centroids.iloc[best_cent][\"y\"], 2))\n",
    "        + \")$ (Partition \"\n",
    "        + str(best_cent + 1)\n",
    "        + \")\"\n",
    "    )\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### 5.1.1.2. Implementation\n",
    "\n",
    "As announced, there are two options for you regarding the implementation of K-means. You may implement the method without extra help or you can choose option two: A guided step-by-step implementation of the K-means algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Implement K-means on Your Own\n",
    "\n",
    "Some of you may prefer to implement K-means on your own. In this case refer to the lecture for a comprehensive explanation of the method. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use your knowledge of K-means to implement a method `k_means` that can be used to cluster the `small_dataset` and into `k` clusters using the euclidean distance to measure the distance between two points.\n",
    "If you are in need of more code cells than provided, feel free to add more.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a k_means function (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample k_means sceleton\n",
    "# NOTE: You are allowed to use this sceleton but don't have to\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the small_dataset (We use k=3)\n",
    "clustered_small_dataset = k_means(small_dataset, 3)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 2: Implement K-means by Solving Small Tasks\n",
    "\n",
    "When someone tries to implement K-means step-by-step, the initial step is always to make an initial partition of the existing data into `k` non-empty partitions. This division can be random or according to an arbitrary scheme. However it is important that the result are exactly `k` partitions, that none of these partitions is empty and that each sample is represented in exactly one of the partitions. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `partition_dataset` that splits a `dataset` into `k` initial partitions. It doesn`t matter what kind of partitioning you decide on, as long as it complies with the rules mentioned. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 3)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a funtion to arbitrarily partition the dataset into k parts\n",
    "def partition_dataset(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Compute quotient and the remainder if spliting the dataset into k parts\n",
    "    quotient, remainder = divmod(dataset_copy.shape[0], k)\n",
    "\n",
    "    # And then to assign the samples to the cluster/partition\n",
    "    for i in range(0, k):\n",
    "        # Assign the cluster value\n",
    "        dataset_copy.loc[\n",
    "            i * quotient\n",
    "            + min(i, remainder) : (i + 1) * quotient\n",
    "            + min(i + 1, remainder),\n",
    "            \"cluster\",\n",
    "        ] = i\n",
    "\n",
    "    # Return the dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Partition the small_dataset\n",
    "partitioned_small_dataset = partition_dataset(small_dataset, 3)\n",
    "\n",
    "# Print a scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The first repetitive step in K-means is to calculate for the so-called centroids (mean points) for each partition/cluster. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement the function `compute_centroids` that computes the centroid for each of the `k` partitions. The return value should be a pandas DataFrame with the cluster identifier as an index and two columns `x` and `y` indicating the coordinates of the corresponding centroid.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 3)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to compute the centroids for a partitioned dataset\n",
    "def compute_centroids(partitioned_dataset, k):\n",
    "    # Init a DataFrame to hold the centroids\n",
    "    centroids = pd.DataFrame(\n",
    "        [[np.nan, np.nan] for i in range(0, k)], columns=[\"x\", \"y\"]\n",
    "    )\n",
    "\n",
    "    # Compute the centroid of each partition\n",
    "    for i in range(0, k):\n",
    "        # Compute the mean of the x values within that single partition\n",
    "        x_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"x\"].mean()\n",
    "\n",
    "        # Compute the mean of the y values within that single partition\n",
    "        y_mean = partitioned_dataset[partitioned_dataset[\"cluster\"] == i][\"y\"].mean()\n",
    "\n",
    "        # Add the centroid of this single partition\n",
    "        centroids.loc[i, [\"x\", \"y\"]] = [x_mean, y_mean]\n",
    "\n",
    "    # Return the centroids\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# Compute the centroids of the intitial partitioning\n",
    "centroids = compute_centroids(partitioned_small_dataset, 3)\n",
    "\n",
    "# Print the centroids into the scatterplot (black)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=partitioned_small_dataset[\"x\"],\n",
    "    y=partitioned_small_dataset[\"y\"],\n",
    "    hue=partitioned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "sns.scatterplot(x=centroids[\"x\"], y=centroids[\"y\"], c=[\"black\"])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "To reassign points to their nearest centroid, distance measure must be defined. Here, for example, the Euclidean distance comes in handy, which we have already implemented ourselves in an earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# \"Pythonic\" implementation of the euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return (abs(a - b) ** 2).sum() ** 0.5\n",
    "\n",
    "\n",
    "# Compute the euclidean distance for two random points a and b\n",
    "a = pd.Series([1, 1])\n",
    "b = pd.Series([2, 2])\n",
    "euclidean_distance(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Reassignment is also the next step within K-means. Samples are always reassigned to the cluster/partition whose centroid is closest to themselves.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function `reassign_samples` that reassigns samples to the cluster/partition whose centroid is closest to themselves. Return the dataset and an indictator to communicate whether at least tuple was reassigned within the function or not.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # ...\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 3\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a function to reassign each sample to its nearest centroid\n",
    "def reassign_samples(partitioned_dataset, centroids, k):\n",
    "    # Indicator to show whether there was at least one tuple reassigned\n",
    "    reassign_indicator = False\n",
    "\n",
    "    # Copy the original partitioned_dataset\n",
    "    dataset_copy = partitioned_dataset.copy()\n",
    "\n",
    "    # Check for each sample whether it has to be reassigned\n",
    "    for i in range(0, dataset_copy.shape[0]):\n",
    "        # Get the value of the the dataset for easier access\n",
    "        sample = dataset_copy.loc[i, [\"x\", \"y\"]]\n",
    "\n",
    "        # Set the current cluster id and centroid values\n",
    "        current_cluster = dataset_copy.loc[i, \"cluster\"]\n",
    "        current_centroid = centroids.loc[current_cluster]\n",
    "        current_distance = euclidean_distance(sample, current_centroid)\n",
    "\n",
    "        # Iterate through the centroids and check whether the distance is lower than the current distance\n",
    "        # NOTE: We do not skip the current centroid, as this would complicate the code and isn't a big performance problem\n",
    "        for j in range(0, k):\n",
    "            # Compute the distance\n",
    "            distance = euclidean_distance(sample, centroids.loc[j])\n",
    "\n",
    "            # If the distance is lower than the current_distance we have to reassign\n",
    "            if distance < current_distance:\n",
    "                # Set the cluster\n",
    "                dataset_copy.loc[i, \"cluster\"] = j\n",
    "                current_cluster = j\n",
    "\n",
    "                # Set the current_centroid\n",
    "                current_centroid = centroids.loc[j]\n",
    "\n",
    "                # Set the current_distance\n",
    "                current_distance = distance\n",
    "\n",
    "                # Set the reassign_indicator\n",
    "                reassign_indicator = True\n",
    "\n",
    "    return reassign_indicator, dataset_copy\n",
    "\n",
    "\n",
    "# Reassign the samples of our partitioned_small_dataset to their nearest centroid\n",
    "reassign_indicator, reassigned_small_dataset = reassign_samples(\n",
    "    partitioned_small_dataset, centroids, 3\n",
    ")\n",
    "\n",
    "# Output the indicator\n",
    "print(\"Was there at least one sample reassigned? - \" + str(reassign_indicator))\n",
    "\n",
    "# Print a scatterplot showing the new class assignments\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=reassigned_small_dataset[\"x\"],\n",
    "    y=reassigned_small_dataset[\"y\"],\n",
    "    hue=reassigned_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In the iterative K-means algorithm it would now be checked whether samples were reassigned or not. If yes, we have to go back to calculating the centroids for this new assignment. If not, then the corresponding clusters have been found. \n",
    "\n",
    "This decision can of course be passed to a wrapper function `k_means` which summarizes the whole algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Merge the previously implemented function `k_means` to achieve a complete implementation of the algorithm.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation (-1 is representing no cluster/partition)\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 3)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement the wrapper function k_means\n",
    "def k_means(dataset, k):\n",
    "    # Partition the dataset\n",
    "    dataset = partition_dataset(dataset, k)\n",
    "\n",
    "    # Set the reassign_indicator to True (as the intial partitioning was as reassingment in itself)\n",
    "    reassign_indicator = True\n",
    "\n",
    "    # As long as there are reassingment the following two steps are repeated\n",
    "    while reassign_indicator:\n",
    "        # Compute the centroids\n",
    "        centroids = compute_centroids(dataset, k)\n",
    "\n",
    "        # Reassign each sample to the cluster of the nearest centroid\n",
    "        reassign_indicator, dataset = reassign_samples(dataset, centroids, k)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = k_means(small_dataset, 3)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: scikit-learn\n",
    "\n",
    "Even with the clustering algorithms from this task sheet, it is of course not normally necessary to create your own implementations for the procedures. In the case of K-means, for example, there is a good implementation in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's implementation of K-means to find three clusters in the `small_dataset`. Print the result in a diagram.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns K-means clustering on the small_dataset\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns K-means clustering on the small_dataset\n",
    "kmeans = KMeans(n_clusters=3, n_init=\"auto\").fit(small_dataset[[\"x\", \"y\"]])\n",
    "\n",
    "# Save the labels to a copy of the small_dataset to generate the equivalent of our clustered_big_dataset\n",
    "clustered_small_dataset_2 = small_dataset.copy()\n",
    "clustered_small_dataset_2[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# Print the result\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset_2[\"x\"],\n",
    "    y=clustered_small_dataset_2[\"y\"],\n",
    "    hue=clustered_small_dataset_2[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "### DBSCAN\n",
    "\n",
    "In addition to the partitioning methods, density-based methods were also presented in the lecture. As an example of these methods, you will asked to implement DBSCAN in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "Also during this implementation you have two options: On the one hand, you may implement DBSCAN completely on your own, on the other hand, you may use the task series divided into smaller tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 1: Implement DBSCAN on Your Own\n",
    "\n",
    "If you decided to implement DBSCAN on your own refer to the lecture for a comprehensive explanation of the method. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Implement a method `dbscan` that can be used to cluster the two datasets `small_dataset` and `big_dataset` into multiple clusters. You shall use the euclidean distance to measure the distance between two points during the clustering.\n",
    "If you are in need of more code cells than provided, feel free to add more.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement a dbscan function (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample dbscan sceleton\n",
    "# NOTE: You are allowed to use this sceleton but don't have to\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation\n",
    "    # Special codings for ...\n",
    "    # ... points that are not set yet: -1\n",
    "    # ... points that are noise: -2\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Create a new empty column to save the visited status\n",
    "    dataset_copy[\"visited\"] = False\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the small_dataset\n",
    "# (the parameters eps=1 and min_pts=2 should result in five different clusters and\n",
    "# one \"noisy\" point for this dataset)\n",
    "clustered_small_dataset = dbscan(small_dataset, 1, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Cluster the big_dataset\n",
    "# (the parameters eps=1 and min_pts=5 should result in five different clusters and\n",
    "# multiple \"noisy\" points for this dataset)\n",
    "clustered_big_dataset = dbscan(big_dataset, 1, 5)\n",
    "\n",
    "# Output the clustered dataset including information on the true classes\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset[\"x\"],\n",
    "    y=clustered_big_dataset[\"y\"],\n",
    "    hue=clustered_big_dataset[\"cluster\"],\n",
    "    style=clustered_big_dataset[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Sample solution => See Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "##### Option 2: Implement DBSCAN by Solving Small Tasks\n",
    "\n",
    "For DBSCAN, you need not only the cluster membership as meta information, but also the status \"visited\". Before we start with the step-by-step implementation of DBSCAN, it is useful to write a small function for preparing the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Add columns to the dataset to save the status of the dataset\n",
    "def prepare_dataset(dataset):\n",
    "    # Copy the original dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "\n",
    "    # Create a new empty column to save the cluster/partition affiliation\n",
    "    # Special codings for ...\n",
    "    # ... points that are not set yet: -1\n",
    "    # ... points that are noise: -2\n",
    "    dataset_copy[\"cluster\"] = -1\n",
    "\n",
    "    # Create a new empty column to save the visited status\n",
    "    dataset_copy[\"visited\"] = False\n",
    "\n",
    "    # Return the dataset_copy\n",
    "    return dataset_copy\n",
    "\n",
    "\n",
    "# Prepare the small dataset\n",
    "prepared_small_dataset = prepare_dataset(small_dataset)\n",
    "prepared_small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Besides this preparatory helper function, there are two things that it would make sense to outsource to separate functions before the actual DBSCAN implementation. \n",
    "\n",
    "First, a function is needed in DBSCAN to randomly select a single unvisited point from a prepared data set. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `pick_random_unvisited_point` that randomly selects an unvisited point out of the `dataset` and returns it.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick a random point that is unvisited\n",
    "def pick_random_unvisited_point(dataset):\n",
    "    # ...\n",
    "    return None\n",
    "\n",
    "\n",
    "# Pick a random point\n",
    "random_point = pick_random_unvisited_point(prepared_small_dataset)\n",
    "random_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Pick a random point that is unvisited\n",
    "def pick_random_unvisited_point(dataset):\n",
    "    # Select all points that are unvisited\n",
    "    unvisited_points = dataset[dataset[\"visited\"] == False]\n",
    "\n",
    "    # If there are no unvisited points return None\n",
    "    if len(unvisited_points) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        # Select one random point and return it\n",
    "        return unvisited_points.sample().iloc[0]\n",
    "\n",
    "\n",
    "# Pick a random point\n",
    "random_point = pick_random_unvisited_point(prepared_small_dataset)\n",
    "random_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "A second helper function that helps implementing DBSCAN is a function that returns all point within eps distance of a selected point.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Write a function `get_all_points_within_eps_distance` that returns all points within distance of `eps`to the passed `point`. Use the euclidean distance function introduced during the K-means part of this exercise to determine the distance between two points. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Get all points within a distance of eps next to a specific point\n",
    "def get_all_points_within_eps_distance(point, dataset, eps):\n",
    "    # ...\n",
    "    return None\n",
    "\n",
    "\n",
    "# Get all points within distance of 1 regarding to the point (6,5)\n",
    "points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "    pd.Series(data=[6, 5, -1, False], index=[\"x\", \"y\", \"cluster\", \"visited\"]),\n",
    "    prepared_small_dataset,\n",
    "    1,\n",
    ")\n",
    "points_within_eps_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Get all points within a distance of eps next to a specific point\n",
    "def get_all_points_within_eps_distance(point, dataset, eps):\n",
    "    # Select all unvisited points within eps distance\n",
    "    return dataset[\n",
    "        dataset.apply(\n",
    "            lambda a: euclidean_distance([a[\"x\"], a[\"y\"]], point[[\"x\", \"y\"]].values)\n",
    "            <= eps,\n",
    "            axis=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# Get all points within distance of 1 regarding to the point (6,5)\n",
    "points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "    pd.Series(data=[6, 5, -1, False], index=[\"x\", \"y\", \"cluster\", \"visited\"]),\n",
    "    prepared_small_dataset,\n",
    "    1,\n",
    ")\n",
    "points_within_eps_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "The pseudocode from the lecture on DBSCAN is quite general. Thus, the substep `If p is core point, add all objects in its -neighborhood to N` is ultimately something that can be implemented both by merging multiple sets of points, and by recursion. \n",
    "Since the recursive variant of DBSCAN is easier to implement, we focus on this variant in this step-by-step implementation. \n",
    "\n",
    "Finally, it makes sense to outsource the entire step `For each p in N that does not yet belong to a cluster` to a seperate recursive function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the function sceleton of the function `expand_cluster` below. Remember that you can use the previously defined helper functions. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# This function is used to expand a specific cluster by one point\n",
    "# If the point is a core point (at least min_pts in eps distance) by itself\n",
    "# expand_cluster is called for each neighbor.\n",
    "def expand_cluster(dataset, eps, min_pts, point, cluster_id):\n",
    "    # ...\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# This function is used to expand a specific cluster by one point\n",
    "# If the point is a core point (at least min_pts in eps distance) by itself\n",
    "# expand_cluster is called for each neighbor.\n",
    "def expand_cluster(dataset, eps, min_pts, point, cluster_id):\n",
    "    # Add the point to the cluster\n",
    "    dataset.loc[point.name, \"cluster\"] = cluster_id\n",
    "\n",
    "    # If point was not visited, we have to visit it now\n",
    "    if dataset.loc[point.name, \"visited\"] == False:\n",
    "        # Mark the point as visited\n",
    "        dataset.loc[point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        # => We have to go deeper into the recursion\n",
    "        if len(points_within_eps_distance.index) >= min_pts:\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Of course it is useful to have a test case to test your implementation against. However this test case is somewhat more difficult to understand, as the function depends on input of an undefined function. Therefore lets describe the test scenario first:\n",
    "\n",
    "*Lets say that the point with id `7` (Coordinates are `(3, 3)`) is selected as random unvisited point out of the prepared_small_dataset by the main dbscan function. As in this example eps is `1` in this case and min_pts is `2` the selected random unvisited point is a core point, as there is one other point (Id `2` and coordinates `(4,3)`) within eps distance. Therefore a new cluster with id `0` is created, the point with id `7` is added and `expand_cluster` gets called for all neighboring points that are not part of a cluster yet. In our example call we take a look call `expand_cluster` for the point with id `2`*\n",
    "\n",
    "If your function works fine, it should add the point with id `2` into the cluster and should check whether it is a core point itself. As there is one still unvisited point to descend to (Id `8` with coordinates `(4,4)`) recursion is started. In the end there should be three visited points withing cluster `0`(Ids `2`, `7` and `8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "prepared_small_dataset = prepare_dataset(small_dataset)\n",
    "\n",
    "# Mark the point with id 7 as visited and add it to the cluster with id 0\n",
    "prepared_small_dataset.loc[7, \"visited\"] = True\n",
    "prepared_small_dataset.loc[7, \"cluster\"] = 0\n",
    "\n",
    "# Select the point with id 2\n",
    "point_with_id_2 = prepared_small_dataset.iloc[2]\n",
    "\n",
    "# Call expand_cluster\n",
    "expand_cluster(prepared_small_dataset, 1, 2, point_with_id_2, 0)\n",
    "\n",
    "# Take a look at the dataset (should now contain three points within cluster 0)\n",
    "prepared_small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "With the help of the recursive function `expand_cluster` it is now not difficult to implement the function `dbscan`, which in principle takes over the remaining steps of the pseudocode and uses `expand_cluster` whenever neighboring items have to be added to the cluster.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Complete the `dbscan`. Again it is recommended to use the previously defined functions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # ...\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 1.5, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Implement dbscan\n",
    "def dbscan(dataset, eps, min_pts):\n",
    "    # Prepare the dataset\n",
    "    dataset = prepare_dataset(dataset)\n",
    "\n",
    "    # While there are unvisited points pick a random one\n",
    "    while len(dataset[dataset[\"visited\"] == False]) > 0:\n",
    "        # Select a random unvisited point\n",
    "        random_point = pick_random_unvisited_point(dataset)\n",
    "\n",
    "        # Mark the random point as visited\n",
    "        dataset.loc[random_point.name, \"visited\"] = True\n",
    "\n",
    "        # Get all points within eps distance\n",
    "        points_within_eps_distance = get_all_points_within_eps_distance(\n",
    "            random_point, dataset, eps\n",
    "        )\n",
    "\n",
    "        # Check if count of points is higher than min_pts => is a core point\n",
    "        if len(points_within_eps_distance.index) < min_pts:\n",
    "            # Not a core point => mark as noise\n",
    "            dataset.loc[random_point.name, \"cluster\"] = -2\n",
    "        else:\n",
    "            # Get the last used cluster id\n",
    "            last_cluster_id = dataset[\"cluster\"].max()\n",
    "\n",
    "            # Increment the id to get an new id for the new cluster\n",
    "            new_cluster_id = last_cluster_id + 1\n",
    "\n",
    "            # Add the random point to the cluster\n",
    "            dataset.loc[random_point.name, \"cluster\"] = new_cluster_id\n",
    "\n",
    "            # Iterate through the points in eps distance\n",
    "            for index, row in points_within_eps_distance.iterrows():\n",
    "                # Check whether the neighbor is already member of a cluster\n",
    "                # (Note that a point marked as noise is not part of a cluster, too)\n",
    "                if dataset.loc[index, \"cluster\"] >= 0:\n",
    "                    # Skip that point\n",
    "                    continue\n",
    "                else:\n",
    "                    # Expand the cluster with that point\n",
    "                    expand_cluster(dataset, eps, min_pts, row, new_cluster_id)\n",
    "\n",
    "    # Return the clustered dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Cluster the small_dataset\n",
    "clustered_small_dataset = dbscan(small_dataset, 1.5, 2)\n",
    "\n",
    "# Output the corresponding scatterplot\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_small_dataset[\"x\"],\n",
    "    y=clustered_small_dataset[\"y\"],\n",
    "    hue=clustered_small_dataset[\"cluster\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "#### Libary: scikit-learn\n",
    "\n",
    "Just as for K-means, scikit-learn also offers an extensive implementation for DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task:** Use scikit-learn's implementation of DBSCAN to find clusters in the `big_dataset`. Use the same parameters we used in the above in the own implementation. Print the result in a diagram.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns DBSCAN clustering on the big_dataset\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sklearns DBSCAN clustering on the big_dataset\n",
    "dbscan = DBSCAN(eps=1, min_samples=5).fit(big_dataset[[\"x\", \"y\"]])\n",
    "\n",
    "# Save the labels to a copy of the big_dataset to generate the equivalent of our clustered_big_dataset\n",
    "clustered_big_dataset_3 = big_dataset.copy()\n",
    "clustered_big_dataset_3[\"cluster\"] = dbscan.labels_\n",
    "\n",
    "# Print the result\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=clustered_big_dataset_3[\"x\"],\n",
    "    y=clustered_big_dataset_3[\"y\"],\n",
    "    hue=clustered_big_dataset_3[\"cluster\"],\n",
    "    style=clustered_big_dataset_3[\"true_labels\"],\n",
    "    palette=\"deep\",\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "In this case, the results of our function and that of scikit-learn are identical. This shows that DBSCAN is more deterministic than K-means. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
