{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Classification* based on the lecture of the same title.\n",
    "\n",
    "This exercise series is divided into three parts. There will be one exercise session per part (= one part per week):\n",
    "\n",
    "- **4.1.** [Decision Tree](./4.1.-Decision-Tree.ipynb) (*last weeks notebook*)\n",
    "- **4.2.** Naive Bayes (*this notebook*)\n",
    "    - **4.2.1.** [Dataset](#4.2.1.-Dataset) \n",
    "    - **4.2.2.** [Train Your Categorical Naive Bayes](#4.2.2.-Train-Your-Categorical-Naive-Bayes)\n",
    "        - **4.2.2.1.** [Prior Probability](#4.2.2.1.-Prior-Probability)\n",
    "        - **4.2.2.2.** [Likelihood](#4.2.2.2.-Likelihood)\n",
    "    - **4.2.3.** [Test Your Categorical Naive Bayes](#4.2.3.-Test-Your-Categorical-Naive-Bayes)  \n",
    "- **4.3.** AdaBoost (*next weeks notebook*) - *Will be uploaded at a later date as a separate zip-file*\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "Work on the respective part yourself **BEFORE** each exercise session. The exercise session is **NOT** intended to take a first look at the exercise sheet, but to solve problems students had while preparing the exercise sheet beforehand.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**\n",
    "\n",
    "Feel free to import more libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. (Categorical) Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this second exercise part will be to implement (categorical) naive Bayes from scratch. \n",
    "\n",
    "Recall Baye's Theorem from slides 45 - 48 and the example on slides 49 - 51 of our lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Dataset \n",
    "We will use the following dataset in this JupyterNotebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.buys_computer import train_buys_computer\n",
    "\n",
    "# view dataset\n",
    "train_buys_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 1:**\n",
    "    \n",
    "Implement Categorical Naive Bayes \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task is to implement some functions in the following `CategociralNaiveBayes` object: `_calculate_prior_probability_per_class`, `_calculate_likelihood`, and `predict`.**\n",
    "\n",
    "- `_calculate_prior_probability_per_class` calculates the probability that any given tuple has a specific class. For instance in the buys computer example dataset the likelihood will return the probability that any customer will buy a PC.\n",
    "- `_calculate_likelihood` calculates the probability that a specific tuple with specific properties is associated to a specific class. In the buys computer example the likelihood returns the probability that a specific customer buys a PC.\n",
    "- `predict` will use the likelihood and prior probability per class to calculate the probability per class label and return the class that is most probable.\n",
    "\n",
    "We alredy implemented the `fit` function for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "**Note these additional requirements:**\n",
    "- For the time being, we restrict our naive Bayes to work solely with **categorical values**.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class CategoricalNaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        self.n: int = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.prior: dict = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.likelihood: dict = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.target_attribute: str = None\n",
    "\n",
    "    def fit(self, dataset: pd.DataFrame, target_attribute: str) -> None:\n",
    "        \"\"\"Fits naive Bayes for categorical data, that is: it calculates all necessary probabilities.\"\"\"\n",
    "        # Store number of tuples for later\n",
    "        self.n = dataset.shape[0]\n",
    "        # Store target attribute for later\n",
    "        self.target_attribute = target_attribute\n",
    "        # Calculate prior probability per class\n",
    "        self._calculate_prior_probability_per_class(dataset=dataset)\n",
    "        # Calculate likelihood\n",
    "        self._calculate_likelihood(dataset=dataset)\n",
    "\n",
    "    def _calculate_prior_probability_per_class(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Private method to calculate prior probability for each class for a given dataset.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Implement this function and store the result in self.prior.\"\n",
    "        )\n",
    "\n",
    "    def _calculate_likelihood(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Private method to calculate likelihood for a given dataset.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Implement this function and store the result in self.likelihood.\"\n",
    "        )\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Returns predicted values for a given dataset.\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "class CategoricalNaiveBayes:\n",
    "    def __init__(self) -> None:\n",
    "        self.n: int = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.prior: dict = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.likelihood: dict = None\n",
    "        # Function fit will later populate this variable\n",
    "        self.target_attribute: str = None\n",
    "\n",
    "    def fit(self, dataset: pd.DataFrame, target_attribute: str) -> None:\n",
    "        \"\"\"Fits naive Bayes for categorical data, that is: it calculates all necessary probabilities.\"\"\"\n",
    "        # Store number of tuples for later\n",
    "        self.n = dataset.shape[0]\n",
    "        # Store target attribute for later\n",
    "        self.target_attribute = target_attribute\n",
    "        # Calculate prior probability per class\n",
    "        self._calculate_prior_probability_per_class(dataset=dataset)\n",
    "        # Calculate likelihood\n",
    "        self._calculate_likelihood(dataset=dataset)\n",
    "\n",
    "    def _calculate_prior_probability_per_class(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Private method to calculate prior probability for each class for a given dataset.\"\"\"\n",
    "        # Here, we build a dictionary that holds the class labels as keys\n",
    "        # and the corresponding prior probability as values.\n",
    "        self.prior = {\n",
    "            class_label: dataset[dataset[self.target_attribute] == class_label].shape[0]\n",
    "            / self.n\n",
    "            for class_label in dataset[self.target_attribute].unique()\n",
    "        }\n",
    "        # Above dictionary comprehension could also be written as:\n",
    "        # self.prior = dict()\n",
    "        # for class_label in dataset[self.target_attribute].unique():\n",
    "        #     current_n = dataset[dataset[self.target_attribute] == class_label].shape[0]\n",
    "        #     self.prior[class_label] = current_n / self.n\n",
    "\n",
    "    def _calculate_likelihood(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Private method to calculate likelihood for a given dataset.\"\"\"\n",
    "        # Here we build a nested dictionary.\n",
    "        # The outer most dictionary contains the class labels as keys.\n",
    "        # For each class_label key, we store a dictionary for each attribute\n",
    "        # (the attribute name is the key here).\n",
    "        # Essentially, for each class label, attribute, and unique discrete value,\n",
    "        # we store the likelihood.\n",
    "        # Example of the resulting dictionary:\n",
    "        # {\n",
    "        #     \"no\": {\n",
    "        #          age\": {\"<=30\": 0.6, \"31-40\": 0.0, \">40\": 0.4},\n",
    "        #          <other attributes here>\n",
    "        #     },\n",
    "        #     \"yes\": {\n",
    "        #         \"age\": {\n",
    "        #             \"<=30\": 0.2222222222222222,\n",
    "        #             \"31-40\": 0.4444444444444444,\n",
    "        #             \">40\": 0.3333333333333333,\n",
    "        #         },\n",
    "        #         <other attributes here>\n",
    "        #     },\n",
    "        # }\n",
    "        self.likelihood = {\n",
    "            class_label: {\n",
    "                attribute_name: {\n",
    "                    attribute_value: dataset.loc[\n",
    "                        (dataset[self.target_attribute] == class_label)\n",
    "                        & (dataset[attribute_name] == attribute_value)\n",
    "                    ].shape[0]\n",
    "                    / dataset[dataset[self.target_attribute] == class_label].shape[0]\n",
    "                    for attribute_value in dataset[attribute_name].unique()\n",
    "                }\n",
    "                for attribute_name in dataset.columns\n",
    "                if attribute_name != self.target_attribute\n",
    "            }\n",
    "            for class_label in dataset[self.target_attribute].unique()\n",
    "        }\n",
    "        # Above dictionary dictionary could also be written as follows: (You may want to uncomment the lines)\n",
    "        # self.likelihood = dict()\n",
    "        # # Iterate over all class labels\n",
    "        # for class_label in dataset[target_attribute].unique():\n",
    "        #     # Create an empty dictionary\n",
    "        #     current_class_label_dictionary = dict()\n",
    "        #     # Calculate number of tuples in this class\n",
    "        #     current_number_of_tuples_per_class = dataset[\n",
    "        #         dataset[self.target_attribute] == class_label\n",
    "        #     ].shape[0]\n",
    "\n",
    "        #     # Iterate over all attribute names in our dataset.\n",
    "        #     for attribute_name in dataset.columns:\n",
    "        #         if attribute_name != self.target_attribute:\n",
    "        #             # If current attribute name is not equal to the target attribute.\n",
    "        #             # Create an empty dictionary\n",
    "        #             current_unique_attribute_values = dict()\n",
    "        #             # Iterate over all unique attribute values of the current attribute.\n",
    "        #             for attribute_value in dataset[attribute_name].unique():\n",
    "        #                 # Calculate the number of tuples that have the current class label and attribute value.\n",
    "        #                 current_number_of_tuples = dataset.loc[\n",
    "        #                     (dataset[self.target_attribute] == class_label)\n",
    "        #                     & (dataset[attribute_name] == attribute_value)\n",
    "        #                 ].shape[0]\n",
    "        #                 # Calculate the likelihood and\n",
    "        #                 # add this number to the dictionary with the attribute value as the key.\n",
    "        #                 current_unique_attribute_values[attribute_value] = (\n",
    "        #                     current_number_of_tuples\n",
    "        #                     / current_number_of_tuples_per_class\n",
    "        #                 )\n",
    "        #             # Add the dictionary that contains the likelihoods for each unique attribute value\n",
    "        #             # and current class label to the dictionary with the attribute name as key.\n",
    "        #             current_class_label_dictionary[\n",
    "        #                 attribute_name\n",
    "        #             ] = current_unique_attribute_values\n",
    "        #     # Add the dictionary that contains the likelihoods for each attribute name and\n",
    "        #     # their corresponding unique discrete value's likelihood to another dictionary with the class label as key.\n",
    "        #     self.likelihood[class_label] = current_class_label_dictionary\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Returns predicted values for a given dataset.\"\"\"\n",
    "        # Predicting works by computing the likelihood for each attribute for\n",
    "        prediction = [\n",
    "            {\n",
    "                class_label: (\n",
    "                    self.prior[class_label]\n",
    "                    * reduce(\n",
    "                        (lambda x, y: x * y),\n",
    "                        [\n",
    "                            attribute_likelihoods[row[attribute]]\n",
    "                            for attribute, attribute_likelihoods in likelihoods.items()\n",
    "                        ],\n",
    "                    )\n",
    "                )\n",
    "                for class_label, likelihoods in self.likelihood.items()\n",
    "            }\n",
    "            for _, row in dataset.iterrows()\n",
    "        ]\n",
    "        # Above list comprehension could be written as:\n",
    "        # prediction = []\n",
    "        # # Iterate over each row in our test dataset\n",
    "        # for _, row in dataset.iterrows():\n",
    "        #     # Create an empty dictionary that later stores the class probabilities for this current tuple/row\n",
    "        #     current_tuple_probabilities = dict()\n",
    "\n",
    "        #     # Iterate over all class labels and corresponding likelihood dictionaries in our likelihood class variable\n",
    "        #     for class_label, likelihoods in self.likelihood.items():\n",
    "        #         current_prior = self.prior[class_label]\n",
    "\n",
    "        #         # Create an empty list to store the likelihood for each attribute\n",
    "        #         current_likelihoods = []\n",
    "\n",
    "        #         # Alternatively to the list:\n",
    "        #         # Creating a list forces us to multiply all elements, we could instead calculate the likelihood as we iterate\n",
    "        #         # over the elements. Thus, we could initialize a variable with 1.\n",
    "        #         current_likelihood = 1\n",
    "\n",
    "        #         # Iterate over each attribute and its corresponding likelihood\n",
    "        #         for attribute, attribute_likelihoods in likelihoods.items():\n",
    "        #             # Add only the likelihood of the corresponding value we observe in our test dataset to our likelihood list\n",
    "        #             current_likelihoods.append(attribute_likelihoods[row[attribute]])\n",
    "\n",
    "        #             # Alternatively, calculate on the fly:\n",
    "        #             current_likelihood *= attribute_likelihoods[row[attribute]]\n",
    "\n",
    "        #         # When using a list to store likelihoods we need to multiply each element.\n",
    "        #         # To achieve this we can use the function reduce with an anonymous function (lambda function) that multiplies two elements.\n",
    "        #         # The result will be a single number.\n",
    "        #         current_likelihood = reduce((lambda x, y: x * y), current_likelihoods)\n",
    "\n",
    "        #         # Calculate the probability and add the result to our tuple probability dictionary\n",
    "        #         current_tuple_probabilities[class_label] = current_prior * current_likelihood\n",
    "\n",
    "        #     # Add the current tuple probabilities for each class to the list of predictions\n",
    "        #     prediction.append(current_tuple_probabilities)\n",
    "\n",
    "        # We now have a list that contains for each tuple a dictionary. Each dictionary holds the probabilities\n",
    "        # for each class of the tuple at this list position. Yet we want to return only the class label for each\n",
    "        # tuple in our test dataset.\n",
    "        # Therefore, we build a list that stores exactly this and return this list.\n",
    "        # Building this list can be done with a simple list comprehension.\n",
    "        # For every dictionary in our prediction list, we want to select the dictionary key (our class label) that\n",
    "        # holds the highest class probability. Selecting the dictionary key with the highes class probability can\n",
    "        # be done by using the function max in combination with its parameter key.\n",
    "        # This simply sorts the dictionary by its values and then returns the dictionary's key of the maximum value.\n",
    "        return [max(elem, key=elem.get) for elem in prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Train Your Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute = \"buys_computer\"\n",
    "\n",
    "naive_bayes = CategoricalNaiveBayes()\n",
    "naive_bayes.fit(dataset=train_buys_computer, target_attribute=target_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1. Prior Probability\n",
    "Take a look at the pior probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prior:\", naive_bayes.prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assert that it contains the correct values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert naive_bayes.prior == {\"no\": 0.35714285714285715, \"yes\": 0.6428571428571429}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2. Likelihood\n",
    "Take a look at the likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"likelihood:\", naive_bayes.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert naive_bayes.likelihood == {\n",
    "    \"no\": {\n",
    "        \"age\": {\"<=30\": 0.6, \"31-40\": 0.0, \">40\": 0.4},\n",
    "        \"income\": {\"high\": 0.4, \"medium\": 0.4, \"low\": 0.2},\n",
    "        \"student\": {\"no\": 0.8, \"yes\": 0.2},\n",
    "        \"credit_rating\": {\"fair\": 0.4, \"excellent\": 0.6},\n",
    "    },\n",
    "    \"yes\": {\n",
    "        \"age\": {\n",
    "            \"<=30\": 0.2222222222222222,\n",
    "            \"31-40\": 0.4444444444444444,\n",
    "            \">40\": 0.3333333333333333,\n",
    "        },\n",
    "        \"income\": {\n",
    "            \"high\": 0.2222222222222222,\n",
    "            \"medium\": 0.4444444444444444,\n",
    "            \"low\": 0.3333333333333333,\n",
    "        },\n",
    "        \"student\": {\"no\": 0.3333333333333333, \"yes\": 0.6666666666666666},\n",
    "        \"credit_rating\": {\"fair\": 0.6666666666666666, \"excellent\": 0.3333333333333333},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Test Your Categorical Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.buys_computer import test_buys_computer\n",
    "\n",
    "\n",
    "test_buys_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"predicted:\",\n",
    "    naive_bayes.predict(test_buys_computer.iloc[:, :-1]),\n",
    "    \"true:\",\n",
    "    test_buys_computer.iloc[0, -1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 2:**\n",
    "    \n",
    "Train and Test Your Naive Bayes Implementation With the Play Tennis Dataset\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.play_tennis import train_play_tennis\n",
    "\n",
    "\n",
    "train_play_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# train your naive Bayes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# train your naive Bayes here\n",
    "naive_bayes_tennis = CategoricalNaiveBayes()\n",
    "naive_bayes_tennis.fit(dataset=train_play_tennis, target_attribute=\"Play Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your newly trained decision tree with the following test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.play_tennis import test_play_tennis\n",
    "\n",
    "\n",
    "test_play_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions with your decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# get predictions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# get predictions here\n",
    "print(\n",
    "    \"predicted:\",\n",
    "    naive_bayes_tennis.predict(test_play_tennis.iloc[:, :-1]),\n",
    "    \"true:\",\n",
    "    test_play_tennis.iloc[:, -1],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
