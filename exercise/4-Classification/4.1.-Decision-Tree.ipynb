{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Classification* based on the lecture of the same title.\n",
    "\n",
    "This exercise series is divided into three parts. There will be one exercise session per part (= one part per week):\n",
    "\n",
    "- **4.1.** Decision Tree (*this notebook*)\n",
    "    - **4.1.1.** [Dataset](#4.1.1.-Dataset) \n",
    "    - **4.1.2.** [Tree Helper Objects](#4.1.2.-Tree-Helper-Objects)\n",
    "    - **4.1.3.** [Train Your Decision Tree](#4.1.3.-Train-Your-Decision-Tree)\n",
    "    - **4.1.4.** [Obtain Predictions with Your Decision Tree](#4.1.4.-Obtain-Predictions-with-Your-Decision-Tree)\n",
    "    - **4.1.5.** [Evaluate Your Decision Tree](#4.1.5.-Evaluate-Your-Decision-Tree)\n",
    "    - **4.1.6.** [Use Another Dataset to Test your Decision Tree Implementation](#4.1.6.-Use-Another-Dataset-to-Test-your-Decision-Tree-Implementation)\n",
    "    - **4.1.7.** [Another Attribute Selection Method](#4.1.7.-Another-Attribute-Selection-Method)\n",
    "        - **4.1.7.1.** [Gain Ratio](#4.1.7.1.-Gain-Ratio)\n",
    "        - **4.1.7.2.** [Gini Index](#4.1.7.2.-Gini-Index)\n",
    "- **4.2.** [Naive Bayes](./4.2.-Naive-Bayes.ipynb) (*next weeks notebook*)\n",
    "- **4.3.** AdaBoost (*notebook of the week after next*) - *Will be uploaded at a later date as a separate zip-file*\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "Work on the respective part yourself **BEFORE** each exercise session. The exercise session is **NOT** intended to take a first look at the exercise sheet, but to solve problems students had while preparing the exercise sheet beforehand.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**\n",
    "\n",
    "Feel free to import more libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Any, Callable, Tuple\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first exercise you find yourself implementing a basic decision tree algorithm from scratch. Yet before you get to implement the decision tree algorithm itself, you need an attribute selection measure. Recall that we discussed three in our lecture: information gain, gain ratio, and Gini index. All three have their advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 1:**\n",
    "    \n",
    "What Are the Key Differences Between the Three Discussed Attribute Selection Measures? Bullet Points are Sufficient.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "Information gain:\n",
    "\n",
    "Gain ratio:\n",
    "\n",
    "Gini index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Information gain:\n",
    "- supports multiway split\n",
    "- used by ID3\n",
    "- guarantees a simple (but not the simplest) tree\n",
    "- select attribute with highest information gain\n",
    "- favours attributes with large amount of (distinct) values\n",
    "\n",
    "Gain ratio:\n",
    "- extension to information gain\n",
    "- used by C4.5 (which is an improved version of ID3)\n",
    "- select attribute with highest gain ratio\n",
    "- becomes unstable when SplitInfo approaches zero (possible solution: contrain it by using information gain then instead of SplitInfo)\n",
    "- prefers unbalanced splits\n",
    "\n",
    "Gini index:\n",
    "- enforces binary split\n",
    "- used by CART\n",
    "- select attribute with lowest Gini index\n",
    "- biased towards multivalued attributes\n",
    "- difficulties with large number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Dataset \n",
    "We will use the following dataset in this JupyterNotebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.buys_computer import train_buys_computer\n",
    "\n",
    "# view dataset\n",
    "train_buys_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Implement Information Gain\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def information(dataset: pd.DataFrame, target_attribute: str) -> float:\n",
    "    \"\"\"Calculate encoded information in a dataset based on its target label distribution.\"\"\"\n",
    "    raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "\n",
    "def information_partitioned(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculate encoded information in a dataset partitioned by a\n",
    "    specific attribute and based on its target label distribution.\"\"\"\n",
    "    raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "\n",
    "def information_gain(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculating information gain of a given dataset and its partitioning attribute.\"\"\"\n",
    "    raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def information(dataset: pd.DataFrame, target_attribute: str) -> float:\n",
    "    \"\"\"Calculate encoded information in a dataset based on its target label distribution.\"\"\"\n",
    "    class_probability = dataset[target_attribute].value_counts() / dataset.shape[0]\n",
    "    return sum([p * log(p, 2) for p in class_probability]) * -1\n",
    "\n",
    "\n",
    "def information_partitioned(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculate encoded information in a dataset partitioned by a\n",
    "    specific attribute and based on its target label distribution.\"\"\"\n",
    "    weights = dataset[partition_attribute].value_counts() / dataset.shape[0]\n",
    "    return sum(\n",
    "        [\n",
    "            weight\n",
    "            * information(\n",
    "                dataset=dataset[dataset[partition_attribute] == index],\n",
    "                target_attribute=target_attribute,\n",
    "            )\n",
    "            for index, weight in weights.items()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def information_gain(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculating information gain of a given dataset and its partitioned version.\"\"\"\n",
    "    return information(dataset, target_attribute) - information_partitioned(\n",
    "        dataset, target_attribute, partition_attribute\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target we want to predict\n",
    "target_attribute = \"buys_computer\"\n",
    "\n",
    "info = information(train_buys_computer, target_attribute)\n",
    "print(info)\n",
    "assert info == 0.9402859586706309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_partitioned = information_partitioned(train_buys_computer, target_attribute, \"age\")\n",
    "print(info_partitioned)\n",
    "assert info_partitioned == 0.6935361388961918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain = information_gain(\n",
    "    dataset=train_buys_computer,\n",
    "    target_attribute=target_attribute,\n",
    "    partition_attribute=\"age\",\n",
    ")\n",
    "print(info_gain)\n",
    "assert info_gain == 0.2467498197744391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Tree Helper Objects\n",
    "\n",
    "Implementing a decision tree from scratch requires storing information in a specific structure. For this, we provide you with two classes, namely `Node` and `Branch`.\n",
    "\n",
    "Recall the components of a decision tree on slide 8:\n",
    "![Components of a decision tree.](decision-tree-components.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Node` object refers to the root node, an internal node, or could also be a leaf node. The difference between a (root/internal) node and a leaf node is the existance or absence of branches and children. Meaning, a node with an empty branches list is a leaf node. Its label holds the class label.\n",
    "\n",
    "As depicted in above's figure, a node can have branches that either lead to internal nodes or a leaf node. Additionally, a (decision) tree may support multiway split (as pictured above) or only support binary splits. To support multiway splits and to enable to branches hold labels with the attribute's corresponding value, we created a `Branch` object. Such an object essentially contains a label and a node. A `Node` object then holds several branches in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Node of a tree. Can consist of multiple branches,\n",
    "    if no branches exist then node is not an internal node but a leaf node.\"\"\"\n",
    "\n",
    "    def __init__(self, label: str, branches: List = None) -> None:\n",
    "        self.label = label\n",
    "        # Our decision trees may support multiway splits.\n",
    "        # Therefore, we store our branches or children as a list.\n",
    "        # Should be of type List[Branch], but in this JupyterNotebook cell\n",
    "        # it is not possible to reference the object Branch before it is defined.\n",
    "        # We refrained form creating a package for these two objects.\n",
    "        self.branches = branches\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Special method to return a string containing a printable\n",
    "        representation of this custom object. This representation can\n",
    "        be used to create this very same object when the value is passed\n",
    "        to eval().\"\"\"\n",
    "        return \"Node(%r, %r)\" % (self.label, self.branches)\n",
    "\n",
    "\n",
    "class Branch:\n",
    "    \"\"\"Branch of a tree containing a label and a (internal/leaf) node.\"\"\"\n",
    "\n",
    "    def __init__(self, label: str, node: Node = None) -> None:\n",
    "        self.label = label\n",
    "        # Actual child of a tree Node.\n",
    "        self.node = node\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Special method to return a string containing a printable\n",
    "        representation of this custom object. This representation can\n",
    "        be used to create this very same object when the value is passed\n",
    "        to eval().\"\"\"\n",
    "        return \"Branch(%r, %r)\" % (self.label, self.node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 3:**\n",
    "    \n",
    "Implement the Basic Algorithm for Decision Trees That Uses Your Implemented Information Gain Function.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now you implemented one attribute selection measure, namely information gain. Additionally, you have two Python objects to aid your decision tree implementation. In this task you will implement a decision tree from scratch. \n",
    "\n",
    "For implementation details refer to lecture slide 9 for the algorithm sketch and to slide 10 for the stopping criteria. The full decision tree algorithm is also in the appendix of this lecture.\n",
    "\n",
    "Our reference book, on which our lecture is based on, contains a detailed explanation on each step (pp. 332) and a pseudo code on p. 333, figure 8.3. Note that this book is available as hard copy in our library and also available online (just google it and it will be among the first results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task is to implement some functions in the following `DecisionTree` object: `_build_tree`, `_find_best_splitting_attribute`, and `predict`.**\n",
    "\n",
    "- `_build_tree` is the heart of this object as it is responsible in constructing a decision tree. This function is called in `fit`.\n",
    "- `_find_best_splitting_attribute` should be called in `_build_tree` to determine which attribute is the best to split and grow a subtree. This function will use your implemented `information_gain` function. How will it use it? By simply instantiating a `DecisionTree` object with a reference to your function. This is the reason why the `__init__` method has one parameter `attribute_selection_method` that is of type `Callable`.\n",
    "- `predict` will use your constructed decision tree. Your task here is to implement this function to walk down your constructed tree to retrieve a class label for a given test data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "**Note these additional requirements:**\n",
    "- For the time being, we restrict our decision tree to work with **categorical data** only.\n",
    "- We allow multiway splits.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "*Short Excursus:*\n",
    "\n",
    "You are wondering why are there methods begining with a single underscore? And why are these methods refered to as \"private\" even though no such keyword exist in Python?\n",
    "\n",
    "True private methods or variables begin with two leading underscores (similar to special methods like `__init__`). It is not possible to access these methods or variables from outside this particular object. Why? Because Python internally adds the class name to this method or variable. When inherit from such an object with a private method/variable comes with its own quirks (you are invited to play around with this yourself). For easier use, it is common to create so called \"private\" methods that have only one leading underscore. These signal that you should not call them from outside the class object or call them directly.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now back to your task at hand: Implementing a basic decision tree from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"Basic Decision Tree algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        attribute_selection_method: Callable,\n",
    "    ) -> None:\n",
    "        self.attribute_selection_method = attribute_selection_method\n",
    "\n",
    "        # Function fit will later populate this variable\n",
    "        self.target_attribute = None\n",
    "\n",
    "        # Function fit will later produce a decision tree\n",
    "        self.tree: Node = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        target_attribute: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit decision tree on a given dataset and target attribute.\"\"\"\n",
    "        # Store target_attribute in this object\n",
    "        self.target_attribute = target_attribute\n",
    "        # Get the attribute list\n",
    "        attribute_list = [col for col in dataset.columns if col != target_attribute]\n",
    "        # Construct the actual decision tree\n",
    "        self.tree = self._build_tree(dataset, attribute_list)\n",
    "\n",
    "    def _build_tree(self, data: pd.DataFrame, attribute_list: List[str]) -> Node:\n",
    "        \"\"\"'Private' method to build decision tree recursively. Returns current (sub-)tree at point.\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    def _find_best_splitting_attribute(\n",
    "        self, data: pd.DataFrame, attribute_list: List[str]\n",
    "    ) -> tuple[str, Any]:\n",
    "        \"\"\"'Private' method to find the best splitting attribute in a list of all available attributes.\"\"\"\n",
    "        # This function should be used in _build_tree. Of course, you can implement\n",
    "        # this functionality directly in _build_tree if you prefer.\n",
    "        raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Returns predicted values for a given dataset.\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"Basic Decision Tree algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        attribute_selection_method: Callable,\n",
    "    ) -> None:\n",
    "        self.attribute_selection_method = attribute_selection_method\n",
    "\n",
    "        # Function fit will later populate this variable\n",
    "        self.target_attribute = None\n",
    "\n",
    "        # Function fit will later produce a decision tree\n",
    "        self.tree: Node = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        target_attribute: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit decision tree on a given dataset and target attribute.\"\"\"\n",
    "        # Store target_attribute in this object\n",
    "        self.target_attribute = target_attribute\n",
    "        # Get the attribute list\n",
    "        attribute_list = [col for col in dataset.columns if col != target_attribute]\n",
    "        # Construct the actual decision tree\n",
    "        self.tree = self._build_tree(dataset, attribute_list)\n",
    "\n",
    "    def _build_tree(self, data: pd.DataFrame, attribute_list: List[str]) -> Node:\n",
    "        \"\"\"'Private' method to build decision tree recursively. Returns current (sub-)tree at point.\"\"\"\n",
    "        if len(data[self.target_attribute].unique()) == 1:\n",
    "            # All tuples have same class, thus return node as leaf node labeled with this class\n",
    "            return Node(label=data[self.target_attribute].unique()[0])\n",
    "\n",
    "        if not attribute_list:\n",
    "            # List is empty, return leaf node with majority class\n",
    "            majority_class = (\n",
    "                data[self.target_attribute]\n",
    "                .value_counts()\n",
    "                .sort_values(ascending=False)\n",
    "                .index[0]\n",
    "            )\n",
    "            return Node(label=majority_class)\n",
    "\n",
    "        # Determine splitting attribute\n",
    "        splitting_attribute, labels = self._find_best_splitting_attribute(\n",
    "            data, attribute_list\n",
    "        )\n",
    "\n",
    "        # Typically, we have to determine if the splitting attribute is discrete valued,\n",
    "        # but we restrict ourselves here only to discrete-valued data.\n",
    "        # Yet, we need to check if the attribute_selection_method allows multiway splits.\n",
    "        # For instance, Gini index only allows binary trees, thus, we can only remove the\n",
    "        # splitting attribute from the attribute list when we do not have Gini index as the\n",
    "        # attribute selection method.\n",
    "        if self.attribute_selection_method.__name__ != \"gini_index\" or (\n",
    "            labels and len(labels) == 1\n",
    "        ):\n",
    "            # Remove the splitting_attribute from attribute_list\n",
    "            attribute_list = [\n",
    "                attr for attr in attribute_list if attr != splitting_attribute\n",
    "            ]\n",
    "\n",
    "        # Create a node with an empty list as branches\n",
    "        node = Node(splitting_attribute, [])\n",
    "\n",
    "        if self.attribute_selection_method.__name__ == \"gini_index\":\n",
    "            attribute_values = labels\n",
    "        else:\n",
    "            attribute_values = [[value] for value in data[splitting_attribute].unique()]\n",
    "\n",
    "        # For each unique value of this splitting_attribute\n",
    "        for value in attribute_values:\n",
    "            # Partition the tuples and grow subtrees for each partition\n",
    "            partition: pd.DataFrame = data[data[splitting_attribute].isin(value)]\n",
    "            if partition.empty:\n",
    "                # Attach a leaf labeled with the majority class\n",
    "                node.branches.append(Node(value))\n",
    "            else:\n",
    "                # Append the node returned by _build_tree.\n",
    "                # Note that we need to copy the list of attributes otherwise we would perform the following\n",
    "                # operations on the very same attribute list. This can be done by slicing, but\n",
    "                # also by using the built in function copy().\n",
    "                node.branches.append(\n",
    "                    Branch(\n",
    "                        label=value,\n",
    "                        node=self._build_tree(\n",
    "                            data=partition, attribute_list=attribute_list[:]\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "        return node\n",
    "\n",
    "    def _find_best_splitting_attribute(\n",
    "        self, data: pd.DataFrame, attribute_list: List[str]\n",
    "    ) -> tuple[str, Any]:\n",
    "        \"\"\"'Private' method to find the best splitting attribute in a list of all available attributes.\"\"\"\n",
    "        # For each attribute in the given attribute_list calculate a scalar value that\n",
    "        # is later then used to determine the best splitting attribute.\n",
    "        # Here, we build a list of tuples. One such tuple contains the attribute name as\n",
    "        # well the calculated scalar value.\n",
    "        # Note that in the case of Gini index as the attribute selection method, a list\n",
    "        # of attribute values and a scalar value such as\n",
    "        # [[['high'], ['medium', 'low']], 0.4428571428571429] is returnd.\n",
    "        all_split_information = [\n",
    "            (\n",
    "                attribute,\n",
    "                self.attribute_selection_method(\n",
    "                    dataset=data,\n",
    "                    target_attribute=self.target_attribute,\n",
    "                    partition_attribute=attribute,\n",
    "                ),\n",
    "            )\n",
    "            for attribute in attribute_list\n",
    "        ]\n",
    "        # Above list comprehension is the same as:\n",
    "        # all_split_information = []\n",
    "        # for attribute in attribute_list:\n",
    "        #     all_split_information.append(\n",
    "        #         (\n",
    "        #             attribute,\n",
    "        #             self.attribute_selection_method(\n",
    "        #                 dataset=data,\n",
    "        #                 target_attribute=self.target_attribute,\n",
    "        #                 partition_attribute=attribute,\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        # Test if our attribute_selection_method is the Gini index,\n",
    "        # otherwise it must be one of the other measures.\n",
    "        if self.attribute_selection_method.__name__ == \"gini_index\":\n",
    "            # Sort this list of tuples based on the scalar value\n",
    "            sorted_information = sorted(all_split_information, key=lambda x: x[1][-1])\n",
    "            # When using Gini index, we want to maximize the information needed and thus need to select\n",
    "            # the minimum value. This is the first element and it may look like\n",
    "            # ('income', ([['high'], ['medium', 'low']], 0.375)).\n",
    "            # We therefore, want to return the attribute name and the labels.\n",
    "            return sorted_information[0][0], sorted_information[0][1][0]\n",
    "        # \"Else\": Another measure has been used.\n",
    "        # It is not wrong to explicitly write else here. Yet it is not really needed in this particular case.\n",
    "        # Sort this list of tuples based on the scalar value\n",
    "        sorted_information = sorted(all_split_information, key=lambda x: x[1])\n",
    "        # When using information gain or gain ratio we want to minimize the information needed\n",
    "        # to classify a tuple/row, meaning we have to select the element in sorted_information with\n",
    "        # the highest value. In our variable sorted_information it is the last Python tuple element ([-1]).\n",
    "        return sorted_information[-1]\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Returns predicted values for a given dataset.\"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\n",
    "                \"DecisionTree not trained on data. Call function fit() first.\"\n",
    "            )\n",
    "        return [self._dfs(self.tree, row) for _, row in dataset.iterrows()]\n",
    "\n",
    "    def _dfs(self, node: Node, data_row: pd.Series):\n",
    "        \"\"\"Private method to recursively walk down our decision tree to obtain a signle class label.\"\"\"\n",
    "        # If a Branch contains an empty list or is None, return its node label.\n",
    "        if not node.branches:\n",
    "            return node.label\n",
    "\n",
    "        # Obtain the corresponding value of our tuple/sample of the column\n",
    "        # that is specified in our node label.\n",
    "        value = data_row[node.label]\n",
    "\n",
    "        # Iterate over each branch of the current node.\n",
    "        for branch in node.branches:\n",
    "            if value in branch.label:\n",
    "                # If the current branch label is equal to the dataset's corresponding\n",
    "                # column value then go level down in the tree.\n",
    "                return self._dfs(branch.node, data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Train Your Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(attribute_selection_method=information_gain)\n",
    "dt.fit(dataset=train_buys_computer, target_attribute=target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. Obtain Predictions with Your Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your decision tree. In the following cell, we will use this one time only the training dataset to make sure our decision tree works as intended. Note, however, **you do not use your training data to test your model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all columns except the last one to obtain predictions.\n",
    "# Last column contains the true class labels.\n",
    "# Note that typically you do not use your training data to test your model!\n",
    "# Here we only use it to make sure our implementation works as intended.\n",
    "predictions = dt.predict(train_buys_computer.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple look at the true and predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for true, predict in zip(train_buys_computer.iloc[:, -1], predictions):\n",
    "    print(\"True\", true, \"prediction\", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5. Evaluate Your Decision Tree\n",
    "- calculate confusion matrix\n",
    "- calculate other metrics (sensitifity, specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6. Use Another Dataset to Test your Decision Tree Implementation\n",
    "The following dataset may help a tennis player to determine wtether to go play tennis or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.play_tennis import train_play_tennis\n",
    "\n",
    "\n",
    "train_play_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# train your decision tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# train your decision tree here\n",
    "dt_tennis = DecisionTree(attribute_selection_method=information_gain)\n",
    "dt_tennis.fit(dataset=train_play_tennis, target_attribute=\"Play Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your newly trained decision tree with the following test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.play_tennis import test_play_tennis\n",
    "\n",
    "\n",
    "test_play_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions with your decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# get predictions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# get predictions here\n",
    "predictions = dt_tennis.predict(test_play_tennis.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# evaluate your decision tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# evaluate your decision tree here\n",
    "for true, predict in zip(test_play_tennis.iloc[:, -1], predictions):\n",
    "    print(\"True\", true, \"prediction\", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7. Another Attribute Selection Method\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 4:**\n",
    "    \n",
    "Implement Another Attribute Selection Method and Incorporate it in your Decision Tree Implementation. \n",
    "For instance, implement Gain Ratio or Gini Index. Keep in mind that some splitting criteria methods minimize whereas others seek to maximize some value. You may need to update your decision tree accordingly.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.7.1. Gain Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def split_info(dataset: pd.DataFrame, partition_attribute: str) -> float:\n",
    "    \"\"\"Calculates and returns SplitInfo given a dataset and a partitioning attribute.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def gain_ratio(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculates gain ratio given a dataset, a target attribute, and a partitioning attribute.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def split_info(dataset: pd.DataFrame, partition_attribute: str) -> float:\n",
    "    \"\"\"Calculates and returns SplitInfo given a dataset and a partitioning attribute.\"\"\"\n",
    "    weights = dataset[partition_attribute].value_counts() / dataset.shape[0]\n",
    "    return sum([weight * log(weight, 2) for _, weight in weights.items()]) * -1\n",
    "\n",
    "\n",
    "def gain_ratio(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> float:\n",
    "    \"\"\"Calculates gain ratio given a dataset, a target attribute, and a partitioning attribute.\"\"\"\n",
    "    gain = information_gain(\n",
    "        dataset=dataset,\n",
    "        target_attribute=target_attribute,\n",
    "        partition_attribute=partition_attribute,\n",
    "    )\n",
    "    split_info_ = split_info(\n",
    "        dataset=dataset,\n",
    "        partition_attribute=partition_attribute,\n",
    "    )\n",
    "    return gain / split_info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_information = split_info(\n",
    "    dataset=train_buys_computer, partition_attribute=\"income\"\n",
    ")\n",
    "print(split_information)\n",
    "assert split_information == 1.5566567074628228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain = information_gain(\n",
    "    dataset=train_buys_computer,\n",
    "    target_attribute=target_attribute,\n",
    "    partition_attribute=\"income\",\n",
    ")\n",
    "print(info_gain)\n",
    "assert info_gain == 0.029222565658954647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_ratio_value = gain_ratio(\n",
    "    dataset=train_buys_computer,\n",
    "    target_attribute=target_attribute,\n",
    "    partition_attribute=\"income\",\n",
    ")\n",
    "print(gain_ratio_value)\n",
    "assert gain_ratio_value == 0.01877264622241867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your decision tree implementation with gain ratio as the attribute selection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gain_ratio = DecisionTree(attribute_selection_method=gain_ratio)\n",
    "dt_gain_ratio.fit(dataset=train_buys_computer, target_attribute=target_attribute)\n",
    "\n",
    "print(\"Decision tree:\", dt_gain_ratio.tree)\n",
    "\n",
    "predictions = dt.predict(train_buys_computer.iloc[:, :-1])\n",
    "\n",
    "print(\"Predictions:\")\n",
    "for true, predict in zip(train_buys_computer.iloc[:, -1], predictions):\n",
    "    print(\"True\", true, \"prediction\", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.7.2. Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def gini(dataset: pd.DataFrame, target_attribute: str) -> float:\n",
    "    raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "\n",
    "def gini_index(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> List:\n",
    "    raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def gini(dataset: pd.DataFrame, target_attribute: str) -> float:\n",
    "    \"\"\"Calculate the purity of the dataset based on its target_attribute.\"\"\"\n",
    "    weights = dataset[target_attribute].value_counts() / dataset.shape[0]\n",
    "    return 1 - sum([weight**2 for _, weight in weights.items()])\n",
    "\n",
    "\n",
    "def partition_dataset(\n",
    "    dataset: pd.DataFrame, partition_attribute: str, values: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Partition a dataset based on a partitioning attribute and a corresponding list of values.\n",
    "    Returns the dataset partitioned by these values as well as the inverse.\"\"\"\n",
    "    condition = dataset[partition_attribute].isin(values)\n",
    "    return dataset[condition], dataset[~condition]\n",
    "\n",
    "\n",
    "def gini_index(\n",
    "    dataset: pd.DataFrame, target_attribute: str, partition_attribute: str\n",
    ") -> List:\n",
    "    \"\"\"Calculating Gini index of a given dataset, target attribute, and its partitioning attribute.\"\"\"\n",
    "    # Get number of tuples/rows\n",
    "    number_tuples = dataset.shape[0]\n",
    "    # Get unique values of the partitioning attribute\n",
    "    unique_values = dataset[partition_attribute].unique()\n",
    "\n",
    "    # If only one unique value exists, we cannot compute the Gini index.\n",
    "    if len(unique_values) == 1:\n",
    "        # Return this single unique value\n",
    "        return [[unique_values], 1]\n",
    "\n",
    "    # Determine unique value combination to build a binary tree.\n",
    "    # Suppose attribute A has v possible values, then we have to take a look at\n",
    "    # 2^v - 2 combinations (leaving out the empty set and the power set).\n",
    "    subset_combinations = [\n",
    "        list(subset)\n",
    "        for l in range(1, len(unique_values))\n",
    "        for subset in itertools.combinations(unique_values, l)\n",
    "    ]\n",
    "    # We later want to build a binary tree. For this to work, we need a\n",
    "    # Python tuple of label for each branch in our tree.\n",
    "    binary_subset_splits = [\n",
    "        [a, b] for a, b in zip(subset_combinations, subset_combinations[::-1])\n",
    "    ]\n",
    "    # Remove duplicates by only taking the half of all elements\n",
    "    binary_subset_splits = binary_subset_splits[: int(len(binary_subset_splits) / 2)]\n",
    "\n",
    "    # Calculate the Gini index for each branch label combination\n",
    "    gini_indices = []\n",
    "    # For each label combination. We only need one element of each tuple\n",
    "    for label_values_left, label_values_right in binary_subset_splits:\n",
    "        # Partition dataset according to the label values and the partitioning attribute\n",
    "        dataset_1, dataset_2 = partition_dataset(\n",
    "            dataset=dataset,\n",
    "            partition_attribute=partition_attribute,\n",
    "            values=label_values_left,\n",
    "        )\n",
    "        # Calculate the Gini index for the partitioned datasets and add a Python tuple\n",
    "        # consisting of the current label combinations and the calculated Gini index\n",
    "        # to the gini_indices list.\n",
    "        gini_indices.append(\n",
    "            (\n",
    "                [label_values_left, label_values_right],\n",
    "                dataset_1.shape[0] / number_tuples * gini(dataset_1, target_attribute)\n",
    "                + dataset_2.shape[0]\n",
    "                / number_tuples\n",
    "                * gini(dataset_2, target_attribute),\n",
    "            )\n",
    "        )\n",
    "    # Sort the Gini indices by their scalar value and return the element with the smallest value.\n",
    "    sorted_indices = sorted(gini_indices, key=lambda x: x[1])\n",
    "    return sorted_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys_computer_gini_index = gini(\n",
    "    dataset=train_buys_computer, target_attribute=target_attribute\n",
    ")\n",
    "assert buys_computer_gini_index == 0.4591836734693877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_attribute = \"income\"\n",
    "all_split_information, index = gini_index(\n",
    "    train_buys_computer, target_attribute, partition_attribute\n",
    ")\n",
    "print(all_split_information, index)\n",
    "\n",
    "assert all_split_information == [[\"high\"], [\"medium\", \"low\"]]\n",
    "assert index == 0.4428571428571429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gini_index = DecisionTree(attribute_selection_method=gini_index)\n",
    "dt_gini_index.fit(dataset=train_buys_computer, target_attribute=target_attribute)\n",
    "\n",
    "print(dt_gini_index.tree)\n",
    "\n",
    "predictions = dt.predict(train_buys_computer.iloc[:, :-1])\n",
    "\n",
    "for true, predict in zip(train_buys_computer.iloc[:, -1], predictions):\n",
    "    print(\"True\", true, \"prediction\", predict)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
