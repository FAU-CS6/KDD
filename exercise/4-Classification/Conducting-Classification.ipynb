{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "# Exercise 3: Conducting Classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Imagine the following scenario:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "*You are an employee in the fictitious company Adventure Works GmbH. Your task is to automate a critical decision regarding inventory thresholds within the company:*\n",
    "\n",
    "*Currently, each product is manually assigned a stock level at which it should be reordered. This threshold should be automatically determined using classification techniques.*\n",
    "\n",
    "*To perform the classification, you have access to the relevant DataFrame product_df.*\n",
    "\n",
    "*Additionally, your supervisor provided you with a selection of essential libraries:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Import helpful libraries\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "dataset_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Build path to database\n",
    "database_path = os.path.join(dataset_folder, \"adventure-works.db\")\n",
    "\n",
    "# Get the database\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/FAU-CS6/KDD-Databases/raw/main/AdventureWorks/adventure-works.db\",\n",
    "    database_path,\n",
    ")\n",
    "\n",
    "# Open connection to the adventure-works.db\n",
    "connection = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame\n",
    "product_df = pd.read_sql_query(\n",
    "    \"SELECT ProductID, Name, ProductNumber, Size, SizeUnitMeasureCode, Weight, WeightUnitMeasureCode, MakeFlag, StandardCost, ListPrice,  DaysToManufacture, ReorderPoint, Color FROM Product\",\n",
    "    connection,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 1:**\n",
    "    \n",
    "Train a classifier on `product_df` to reliably determine the reorder point for a product. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "As always, the first step is to get to know the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "While `ReorderPoint` is easy to recognize as the target variable of our classification, a simple `head()` points out an other important detail:\n",
    "\n",
    "Some of the columns contain missing values (e.g. `None` or `NaN`). \n",
    "\n",
    "As this might lead to problems later on, we should first investigate further on this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the number of missing values per column\n",
    "print(\"Missing values per column:\\n\" + str(product_df.isna().sum()) + \"\\n\")\n",
    "\n",
    "# Get the total number of values in the DataFrame\n",
    "print(\"Total count of tuples in the DataFrame:\\n\" + str(product_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "It seems that the attributes `Size`, `SizeUnitMeasureCode`, `Weight`, `WeightUnitMeasureCode`, and `Color` do not contain any values in about half of all tuples.\n",
    "\n",
    "This can mean, for example, that a product really has no relevant color, but it can also mean that the color was simply not entered. \n",
    "\n",
    "Depending on which is the case, this can possibly falsify the results of the classification. It would be best to ask the data producers what is the case in for each missing value.\n",
    "\n",
    "However, we do not have this option in this exercise. We are therefore left with some less optimal solutions:\n",
    "\n",
    "1. **Ignore the missing data:**\n",
    "\n",
    "    Ignoring the missing data is a problem at the latest when we apply sklearn's classification methods to the DataFrame. This would lead to an error with NaN Values. Ignoring is therefore not a valid solution.\n",
    "\n",
    "2. **Infer the missing data:**\n",
    "\n",
    "    A frequently used variant for missing data is to simply derive the missing data from the existing data. \n",
    "\n",
    "    However, as long as we do not train our own classification for this, we could only fall back on very generic filling methods such as mode, which would further distort the result of our classification, as the most frequent values of each attribute would suddenly be set for even more tuples and most of them would probably not even belong to this value “in real life”.\n",
    "\n",
    "3. **Mark the missing data:**\n",
    "\n",
    "    Even if deriving the data is a bad idea in our case, there is a second possibility to get away from `NaN`/`None`: \n",
    "\n",
    "    Introduce an extra value for `Unknown`. This has a certain advantage if missing data also has a certain meaning (e.g. if the color is always omitted when it is irrelevant). \n",
    "\n",
    "    However, the value significantly distorts our classification if it is randomly forgotten data. \n",
    "\n",
    "4. **Delete the tuples with missing values:**\n",
    "\n",
    "    Tuples with missing data are often simply deleted and therefore ignored during training. \n",
    "\n",
    "    However, this variant has the disadvantage that there is suddenly a significantly smaller number of tuples on which the classifier can be trained.\n",
    "\n",
    "    In addition, the problem arises at the latest when classes are predicted for tuples with the help of the classifier, which themselves have missing values at these points (because, for example, it might be fully intentional not to have specified a color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame\n",
    "product_df_copy = product_df.copy()\n",
    "\n",
    "# Drop all rows with missing values\n",
    "product_df_copy = product_df_copy.dropna()\n",
    "\n",
    "# Get the shape of the new DataFrame\n",
    "print(\n",
    "    \"Shape of the new DataFrame after dropping missing values:\\n\"\n",
    "    + str(product_df_copy.shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "5. **Delete the attributes with missing values:**\n",
    "\n",
    "    Deleting the attributes with missing values is probably the safest way to avoid the uncertainties of missing values without contacting the data producers. \n",
    "\n",
    "    The disadvantage is that fewer attributes are available for training and the classifier will therefore potentially perform less well.\n",
    "\n",
    "    In our case, however, it is probably the best way to get around the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Drop all columns with missing values\n",
    "product_df = product_df.dropna(axis=1)\n",
    "\n",
    "# Print the DataFrame\n",
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "With this problem solved, we can move on to the actual classification.\n",
    "\n",
    "It is important to note that `sklearn` cannot directly work with categorical attributes and that we must first encode them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Encode the categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in product_df.columns:\n",
    "    if product_df[column].dtype == type(object):  # If a column is categorical\n",
    "        le = LabelEncoder()\n",
    "        # Fit and transform the column\n",
    "        product_df[column] = le.fit_transform(product_df[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Print the DataFrame\n",
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The next step is to separate the target variable from the remaining attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Separate the features and the target variable (ReorderPoint)\n",
    "X = product_df.drop(\"ReorderPoint\", axis=1)\n",
    "y = product_df[\"ReorderPoint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We also have to split the data we have into training data and test data in order to be able to check the quality of our classifier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (in this case, 70% training and 30% testing which is a common split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Based on this data, `sklearn`s classifiers can now be trained. Our \"supervisor” (see scenario) has provided us with both a Decision Tree Classifier and a Naive Bayes Classifier, so it is probably best to try both and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier (with entropy (= Information Gain) as the criterion)\n",
    "tree_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier (with gini as the criterion)\n",
    "tree_model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Naive Bayes Classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "All in all, the Decision Tree Classifier seems to perform better (regardless of the criterion), as it leads to a significantly higher f1-score. \n",
    "\n",
    "The Naive Bayes leads to a slightly better recall, but has significantly worse precision.\n",
    "\n",
    "It is also interesting to note that values that are supposed to contain a `600` are often not predicted correctly. However, this may well be due to the small number of values with this value in the training data set.\n",
    "\n",
    "All in all, the classifier is very satisfactory as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "Next, a slightly modified DataFrame is given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Product into a DataFrame\n",
    "new_product_df = pd.read_sql_query(\n",
    "    \"SELECT ProductID, Name, ProductNumber, Size, SizeUnitMeasureCode, Weight, WeightUnitMeasureCode, MakeFlag, StandardCost, ListPrice,  DaysToManufacture, SafetyStockLevel, ReorderPoint, Color FROM Product\",\n",
    "    connection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "specification"
    ]
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 2:**\n",
    "    \n",
    "Carry out a classification with regard to `ReorderPoint` on `new_product_df`.  \n",
    "\n",
    "What do you notice about the result, why does this change occur and why should you be careful with classifiers with this result?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 01/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 02/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 03/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 04/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 05/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 06/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 07/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 08/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 09/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a good classifier to determine the reorder point (Code placeholder 10/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "As the DataFrame contains new columns, we have to check for missing values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the number of missing values per column\n",
    "print(\"Missing values per column:\\n\" + str(new_product_df.isna().sum()) + \"\\n\")\n",
    "\n",
    "# Get the total number of values in the DataFrame\n",
    "print(\"Total count of tuples in the DataFrame:\\n\" + str(new_product_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "However, the new SafetyStockLevel column does not appear to contain any missing values, so we can continue with our existing preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Drop all columns with missing values\n",
    "new_product_df = new_product_df.dropna(axis=1)\n",
    "\n",
    "# Print the DataFrame\n",
    "new_product_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We do not need to adjust the actual classification either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Encode the categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in new_product_df.columns:\n",
    "    if new_product_df[column].dtype == type(object):  # If a column is categorical\n",
    "        le = LabelEncoder()\n",
    "        # Fit and transform the column\n",
    "        new_product_df[column] = le.fit_transform(new_product_df[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Print the DataFrame\n",
    "product_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Separate the features and the target variable (ReorderPoint)\n",
    "X = new_product_df.drop(\"ReorderPoint\", axis=1)\n",
    "y = new_product_df[\"ReorderPoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (in this case, 70% training and 30% testing which is a common split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier (with entropy (= Information Gain) as the criterion)\n",
    "tree_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier (with gini as the criterion)\n",
    "tree_model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train a Naive Bayes Classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "With the added attribute, all classifiers suddenly have an f1-score of 100%.\n",
    "\n",
    "However, this can be explained relatively easily if you take a look at the correlation of the all attributes with `ReorderPoint`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "new_product_df.corr()[\"ReorderPoint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "It can be clearly seen that `SafetyStockLevel` and `ReorderPoint` are fully correlated.\n",
    "\n",
    "This of course allows the respective class of `ReorderPoint` to be easily predicted from `SafetyStockLevel`. \n",
    "\n",
    "It can be argued that this somewhat negates the point of a classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "If `ReorderPoint` is defined manually before the classification, the same might also apply to SafetyStockLevel.\n",
    "\n",
    "If that is the case, it would probably make sense to determine both automatically via the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "It can also be considered whether the database should not even be normalized in the case of such a clear correlation. \n",
    "\n",
    "However, this goes beyond the questions on this task sheet."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
