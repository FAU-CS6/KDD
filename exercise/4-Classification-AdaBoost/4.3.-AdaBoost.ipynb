{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification\n",
    "\n",
    "This JupyterNotebook is part of an exercise series titled *Classification* based on the lecture of the same title.\n",
    "\n",
    "This exercise series is divided into three parts. There will be one exercise session per part (= one part per week):\n",
    "\n",
    "- **4.1.** Decision Tree (*notebook of the week before last*)\n",
    "- **4.2.** Naive Bayes (*last weeks notebook*)\n",
    "- **4.3.** AdaBoost (*this notebook*)\n",
    "    - **4.3.1.** [Dataset](#4.3.1.-Dataset)\n",
    "    - **4.3.2.** [Instantiate and Train a AdaBoost Model](#4.3.2.-Instantiate-and-Train-a-AdaBoost-Model)\n",
    "    - **4.3.3.** [Obtain Predictions with Your AdaBoost Model](#4.3.3.-Obtain-Predictions-with-Your-AdaBoost-Model)\n",
    "    - **4.3.4.** [Evaluate Your AdaBoost Model](#4.3.4.-Evaluate-Your-AdaBoost-Model)\n",
    "        - **4.3.4.1.** [Import `scikit-learn`'s Metrics Module](#4.3.4.1.-Import-scikit-learn's-Metrics-Module)\n",
    "        - **4.3.4.2.** [Calculate the Evaluation Metrics](#4.3.4.2.-Calculate-the-Evaluation-Metrics)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Important:**\n",
    "    \n",
    "Work on the respective part yourself **BEFORE** each exercise session. The exercise session is **NOT** intended to take a first look at the exercise sheet, but to solve problems students had while preparing the exercise sheet beforehand.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**\n",
    "\n",
    "Feel free to import more libraries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Any\n",
    "from math import log\n",
    "import random\n",
    "\n",
    "from classifier import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. AdaBoost\n",
    "Your task in this exercise is to implement AdaBoost from scratch. It will use a slightly modified version of the previously implemented decision tree. This decision tree will randomly select an attribute as the splitting attribute and only a tree of depth three is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Dataset \n",
    "We will use the following dataset in this JupyterNotebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"car_train.csv\")\n",
    "\n",
    "# view dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a slightly modified version of the [car evaluation dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from the UCI Machine Learning Repository. Originally, this dataset has four class values. For the sake of this example we modified it to binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"condition\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Task 1:**\n",
    "    \n",
    "Implement AdaBoost\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task is to implement some functions in the following `AdaBoost` object: `fit`, `_missclassification_error`, `_calculate_new_weights`, `_normalize_weights`, and `_classifiy_single_tuple`.**\n",
    "\n",
    "- `fit` already comes with some lines of codes. Your task here is to implement the missing crucial functionality to train multiple weak classifiers.\n",
    "- `_missclassification_error` should return 1 if the true value and predicted value are different and return 0 if they are the same.\n",
    "- `_calculate_new_weights` will calculate weights for the next iteration. \n",
    "- `_normalize_weights` \"normalizes\" weights by dividing each weight element by the sum of all weights.\n",
    "- `_classifiy_single_tuple` will, as the name suggest, return a class label for a single tuple. It is used in the function `predict`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:**\n",
    "\n",
    "Sampling a dataset with replacement can be done with the function `sample` from pandas. Make sure to set the parameter `replace=True`, otherwise rows will only be sampled once. Additionally, we want to sample rows.\n",
    "\n",
    "You can take a look at the documentation of `sample` [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_classifier: int = 5, seed: int = 42) -> None:\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.n_classifier: int = n_classifier\n",
    "\n",
    "        # Function fit will later populate these variables\n",
    "        self.target_attribute: str = None  # target attribute of training dataset\n",
    "        self.classes: List[\n",
    "            Any\n",
    "        ] = []  # list of class labels, used later in function _classifiy_single_tuple()\n",
    "        self.weights: List[List[float]] = []  # weights of each trained classifier\n",
    "        self.betas: List[float] = []  # beta values of each trained classifier\n",
    "        self.classifiers: List[DecisionTree] = []  # list of trained classifiers\n",
    "\n",
    "    def fit(self, dataset: pd.DataFrame, target_attribute: str) -> None:\n",
    "        \"\"\"Fit AdaBoost by training multiple weak learners to the given dataset.\"\"\"\n",
    "        # Assign target_attribute to object variable\n",
    "        self.target_attribute = target_attribute\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights(dataset=dataset)\n",
    "\n",
    "        # Get and store number of classes to object variable\n",
    "        self.classes = dataset[target_attribute].unique().tolist()\n",
    "\n",
    "        # Get number of tuples in dataset\n",
    "        n_tuples = dataset.shape[0]\n",
    "\n",
    "        while True:\n",
    "            # Stop while-loop when there are enough classifiers\n",
    "            if len(self.classifiers) == self.n_classifier:\n",
    "                break\n",
    "\n",
    "            raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    def _initialize_weights(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Initialize weights if they have not been initialized before.\n",
    "        Formula: weights = 1 / number of tuples in data\"\"\"\n",
    "        if not self.weights:\n",
    "            n_tuples = dataset.shape[0]\n",
    "            self.weights.append([1 / n_tuples for _ in range(n_tuples)])\n",
    "\n",
    "    def _weak_learn(self, dataset: pd.DataFrame) -> Any:\n",
    "        \"\"\"Fit a weak learner and return this classifier.\"\"\"\n",
    "        # Instantiate weak classifier\n",
    "        classifier = DecisionTree()\n",
    "\n",
    "        # Train weak classifier\n",
    "        classifier.fit(dataset=dataset, target_attribute=self.target_attribute)\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _missclassification_error(true: List[float], predicted: List[float]) -> int:\n",
    "        \"\"\"Calculate the missclassification error.\n",
    "        Returns 1 if missclassified (true != predicted), 0 if correct (true == predicted).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_new_weights(\n",
    "        weights: List[float], beta: float, error: List[float]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Update weights by multiplying weights with beta to the power of 1 - error.\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_weights(weights: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize weights. Formula: weights = weights / sum(weights)\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Return prediction for a given dataset.\"\"\"\n",
    "        return [\n",
    "            self._classifiy_single_tuple(tuple.to_frame().T)\n",
    "            for _, tuple in dataset.iterrows()\n",
    "        ]\n",
    "\n",
    "    def _classifiy_single_tuple(self, tuple: pd.DataFrame) -> Any:\n",
    "        \"\"\"Classifies a single tuple.\"\"\"\n",
    "        raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_classifier: int = 5, seed: int = 42) -> None:\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.n_classifier: int = n_classifier\n",
    "\n",
    "        # Function fit will later populate these variables\n",
    "        self.target_attribute: str = None  # target attribute of training dataset\n",
    "        self.classes: List[\n",
    "            Any\n",
    "        ] = []  # list of class labels, used later in function predict()\n",
    "        self.weights: List[List[float]] = []  # weights of each trained classifier\n",
    "        self.betas: List[float] = []  # beta values of each trained classifier\n",
    "        self.classifiers: List[DecisionTree] = []  # list of trained classifiers\n",
    "\n",
    "    def fit(self, dataset: pd.DataFrame, target_attribute: str) -> None:\n",
    "        \"\"\"Fit AdaBoost by training multiple weak learners to the given dataset.\"\"\"\n",
    "        # Assign target_attribute to object variable\n",
    "        self.target_attribute = target_attribute\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights(dataset=dataset)\n",
    "\n",
    "        # Get and store number of classes to object variable\n",
    "        self.classes = dataset[target_attribute].unique().tolist()\n",
    "\n",
    "        # Get number of tuples in dataset\n",
    "        n_tuples = dataset.shape[0]\n",
    "\n",
    "        while True:\n",
    "            # Stop while-loop when there are enough classifiers\n",
    "            if len(self.classifiers) == self.n_classifier:\n",
    "                break\n",
    "\n",
    "            # Normalize weights\n",
    "            weights_normalized = self._normalize_weights(weights=self.weights[-1])\n",
    "\n",
    "            # Sample dataset with replacement based on the distribution of variable weights_normalized\n",
    "            current_dataset = dataset.sample(\n",
    "                n=n_tuples,\n",
    "                weights=weights_normalized,\n",
    "                replace=True,\n",
    "                random_state=42,\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            # Call weak_learn to obtain the current trained classifier\n",
    "            current_classifier = self._weak_learn(dataset=current_dataset)\n",
    "\n",
    "            # Obtain prediction from current classifier\n",
    "            current_predictions = current_classifier.predict(dataset=current_dataset)\n",
    "\n",
    "            # Determine missclassification error on every element\n",
    "            missclassification_errors = [\n",
    "                self._missclassification_error(true_value, predicted)\n",
    "                for true_value, predicted in zip(\n",
    "                    current_dataset[target_attribute],\n",
    "                    current_predictions,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Calculate overall error\n",
    "            current_error = sum(\n",
    "                [\n",
    "                    w_i * err_i\n",
    "                    for w_i, err_i in zip(weights_normalized, missclassification_errors)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # If error is greater than 0.5 abort loop and start again\n",
    "            if current_error > 0.5:\n",
    "                continue\n",
    "\n",
    "            # Calculate beta\n",
    "            current_beta = current_error / (1 - current_error)\n",
    "\n",
    "            # Append current beta to the list of betas\n",
    "            self.betas.append(current_beta)\n",
    "\n",
    "            # Calculate and normalize weights for next iteration\n",
    "            new_weights = self._calculate_new_weights(\n",
    "                weights=self.weights[-1],\n",
    "                beta=current_beta,\n",
    "                error=missclassification_errors,\n",
    "            )\n",
    "\n",
    "            # Append current weights to the list of weights\n",
    "            self.weights.append(new_weights)\n",
    "\n",
    "            # Append current weak classifier to the list of classifiers\n",
    "            self.classifiers.append(current_classifier)\n",
    "\n",
    "    def _initialize_weights(self, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"Initialize weights if they have not been initialized before.\n",
    "        Formula: weights = 1 / number of tuples in data\"\"\"\n",
    "        if not self.weights:\n",
    "            n_tuples = dataset.shape[0]\n",
    "            self.weights.append([1 / n_tuples for _ in range(n_tuples)])\n",
    "\n",
    "    def _weak_learn(self, dataset: pd.DataFrame) -> Any:\n",
    "        \"\"\"Fit a weak learner and return this classifier.\"\"\"\n",
    "        # Instantiate weak classifier\n",
    "        classifier = DecisionTree()\n",
    "\n",
    "        # Train weak classifier\n",
    "        classifier.fit(dataset=dataset, target_attribute=self.target_attribute)\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _missclassification_error(true: List[float], predicted: List[float]) -> int:\n",
    "        \"\"\"Calculate the missclassification error.\n",
    "        Returns 1 if missclassified (true != predicted), 0 if correct (true == predicted).\n",
    "        \"\"\"\n",
    "        missclassified = 1 - int(true == predicted)\n",
    "        return missclassified\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_new_weights(\n",
    "        weights: List[float], beta: float, error: List[float]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Update weights by multiplying weights with beta to the power of 1 - error.\"\"\"\n",
    "        return [w_i * beta ** (1 - err_i) for w_i, err_i in zip(weights, error)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_weights(weights: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize weights. Formula: weights = weights / sum(weights)\"\"\"\n",
    "        weight_sum = sum(weights)\n",
    "        if weight_sum == 0:\n",
    "            return [0 for _ in weights]\n",
    "        return [w_i / weight_sum for w_i in weights]\n",
    "\n",
    "    def predict(self, dataset: pd.DataFrame) -> List[Any]:\n",
    "        \"\"\"Return prediction for a given dataset.\"\"\"\n",
    "        return [\n",
    "            self._classifiy_single_tuple(tuple.to_frame().T)\n",
    "            for _, tuple in dataset.iterrows()\n",
    "        ]\n",
    "\n",
    "    def _classifiy_single_tuple(self, tuple: pd.DataFrame) -> Any:\n",
    "        \"\"\"Classifies a single tuple.\"\"\"\n",
    "        # Initialize class weights:\n",
    "        class_weights = {current_class: 0 for current_class in self.classes}\n",
    "\n",
    "        # Calculate weight of each classifier's vote\n",
    "        classifier_weight = [\n",
    "            log(1 / beta_i, 2) if beta_i > 0 else 0 for beta_i in self.betas\n",
    "        ]\n",
    "\n",
    "        # Obtain predictions from all classifiers\n",
    "        predicitons_per_classifier = [\n",
    "            current_classifier.predict(tuple)[0]\n",
    "            for current_classifier in self.classifiers\n",
    "        ]\n",
    "\n",
    "        # Update class weights\n",
    "        for current_weight, current_prediction in zip(\n",
    "            classifier_weight, predicitons_per_classifier\n",
    "        ):\n",
    "            class_weights[current_prediction] += current_weight\n",
    "\n",
    "        # Return class with the highest weight\n",
    "        return max(class_weights, key=class_weights.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Instantiate and Train a AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute = \"condition\"\n",
    "adaboost = AdaBoost()\n",
    "adaboost.fit(dataset=dataset, target_attribute=target_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Obtain Predictions with Your AdaBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this model, we import the corresponding test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"car_test.csv\")\n",
    "\n",
    "# view dataset\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = adaboost.predict(test_dataset.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the true values for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = test_dataset.iloc[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. Evaluate Your AdaBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lecture you learned how to evaluate models. In the following cells we evaluate our previously trained AdaBoost model.\n",
    "\n",
    "`scikit-learn` provides multiple metrics to compute. Depending on which problem you have at hand (clustering, classification, or regression) different metrics are available. `scikit-learn` has a good overview of its functions on this [page](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "In this exercise we are solely interested in (some) classification metrics, namely:\n",
    "- Accuracy, function `accuracy_score`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "- Sensitivity/Recall, function `recall_score`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)\n",
    "- Precision, function `precision_score`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "- Specificity. This metric unfortunately is not available. However, it is simple to compute.\n",
    "- Confusion matrix, class `ConfusionMatrixDisplay` with function `from_predictions`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions)\n",
    "- Receiver Operating Characteristic (ROC), function `roc_curve`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)\n",
    "- Area Under the Curve (AUC), function `auc`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc)\n",
    "- F1, function `f1_score`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "- Classification report, function `classification_report`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.1. Import `scikit-learn`'s Metrics Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4.2. Calculate the Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 2: Calculate Accuracy**\n",
    "    \n",
    "Apply function `metrics.accuracy_score` to calculate the model's accuracy. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score).\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.accuracy_score here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_true=true_values, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 3: Calculate Sensitivity/Recall**\n",
    "\n",
    "Apply function `metrics.recall_score` to calculate the model's sensitivity. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score).\n",
    "\n",
    "</div>    \n",
    "    \n",
    "**Hint:** Make sure to set parameter `pos_label='acc'`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.recall_score here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "metrics.recall_score(y_true=true_values, y_pred=predictions, pos_label=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "        \n",
    "**Task 4: Implement and Calculate Specificity**\n",
    "\n",
    "Implement a function that calculates the specificity. Apply this function afterwards.    \n",
    "    \n",
    "</div>    \n",
    "\n",
    "Unfortunately, `scikit-learn` does not provide a function to simply compute the specificity. Yet, we know how to compute it: $\\frac{\\text{True Negatives}}{\\text{Negatives}}$.\n",
    "\n",
    "Lucky for us, `scikit-learn` has the function `confusion_matrix` ([link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)) which computes all necessary metrics we need: true negatives, false positives, false negatives, and true positives in exactly this order.\n",
    "\n",
    "**Hint:** Make sure to correctly label the metrics of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def specificity(y_true: List[Any], y_pred: List[Any]) -> float:\n",
    "    raise NotImplementedError(\"Implement this function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def specificity(y_true: List[Any], y_pred: List[Any]) -> float:\n",
    "    tp, fn, fp, tn = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n",
    "    return tn / (fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity(y_true=true_values, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "#### Task: Plot Confusion Matrix\n",
    "\n",
    "`scikit-learn`'s class `ConfusionMatrixDisplay` provides plotting capabilities, for instance with function `from_predictions`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions)\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.ConfusionMatrixDisplay.from_predictions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "metrics.ConfusionMatrixDisplay.from_predictions(y_true=true_values, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 5: Calculate ROC Curve Measure**\n",
    "\n",
    "Apply `metrics.roc_curve`. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve).\n",
    "\n",
    "</div>\n",
    "\n",
    "**Hint:** Convert textual description of `true_values` and `predictions` into their numerical equivalent. Set `'acc'=1` and `'unacc'=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.roc_curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(\n",
    "    y_true=[1 if elem == \"acc\" else 0 for elem in true_values],\n",
    "    y_score=[1 if elem == \"acc\" else 0 for elem in predictions],\n",
    "    pos_label=1,\n",
    ")\n",
    "\n",
    "print(\"False Positive Rate\", fpr)\n",
    "print(\"True Positive Rate\", tpr)\n",
    "print(\"Thresholds\", thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, from the values we do not \"see\" anything. We will use these in AUC and later to plot the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 6: Calculate Area Under the Curve (AUC)**\n",
    "\n",
    "Apply function `metrics.auc` to calculate the model's AUC value. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc).\n",
    "\n",
    "</div>    \n",
    "\n",
    "**Hint:** Take a look at the documentation. You need to call `roc_curve` first. More specifically, you need the False Positive Rate and True Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.auc here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "**Task 7: Plot ROC Curve**\n",
    "\n",
    "Apply function `metrics.RocCurveDisplay` to plot the model's ROC curve. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html).\n",
    "\n",
    "</div>    \n",
    "\n",
    "**Hint:** You need the values returned by applying `metrics.roc_curve` and `metrics.auc`. Make sure to use matplotlibs functionality to show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.RocCurveDisplay here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(\n",
    "    y_true=[1 if elem == \"acc\" else 0 for elem in true_values],\n",
    "    y_score=[1 if elem == \"acc\" else 0 for elem in predictions],\n",
    ")\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(\n",
    "    fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"example estimator\"\n",
    ")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "#### Task: Calculate F1\n",
    "\n",
    "Apply function `metrics.f1_score` to calculate the model's F1 value. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "\n",
    "</div>    \n",
    "\n",
    "**Hint:** We have a binary classification module. Additionally, make sure to set the parameter `pos_label` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.f1_score here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "metrics.f1_score(\n",
    "    y_true=true_values,\n",
    "    y_pred=predictions,\n",
    "    average=\"binary\",  # it is the default, but included here to show that we have a binary problem.\n",
    "    pos_label=\"acc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "#### Task: Print a Classification Report\n",
    "Apply function `metris.classification_report` that calculates and prints some important metrics in one single function call. [Link to documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report).\n",
    "    \n",
    "</div>    \n",
    "\n",
    "**Hint:** For pretty print make sure to use the function `print`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# apply metrics.classification_report here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true=true_values, y_pred=predictions))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a860ea30b01b0bbbc878480a606182d7c4139ace473edbf042fd8e5bb69bbf11"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
